# Progress Log - Pathway Data Architecture

## Project Context

This project extends the existing Reflex UI redesign (`pathways_app/app_v2.py`) with pre-computed pathway data from Snowflake. The current app uses a simplified `prepare_chart_data()` that only does Trust → Directory → Drug aggregation. The goal is to support full sequential patient treatment pathways with treatment statistics.

## Key Files Reference

**Existing (reuse these):**
- `analysis/pathway_analyzer.py` - Has `prepare_data()`, `calculate_statistics()`, `build_hierarchy()`, `generate_icicle_chart()`
- `visualization/plotly_generator.py` - Has chart generation with full customdata structure
- `data_processing/snowflake_connector.py` - Snowflake connection with SSO auth
- `tools/data.py` - `patient_id()`, `drug_names()`, `department_identification()`
- `data_processing/schema.py` - Existing SQLite schema

**To create:**
- `data_processing/pathway_pipeline.py` - New pathway processing pipeline
- `cli/refresh_pathways.py` - CLI command for data refresh

## Known Patterns

### Pathway ids format
The `ids` column in ice_df contains hierarchical paths like:
- "Norfolk & Waveney ICS" (root)
- "Norfolk & Waveney ICS|NNUH" (trust)
- "Norfolk & Waveney ICS|NNUH|OPHTHALMOLOGY" (directory)
- "Norfolk & Waveney ICS|NNUH|OPHTHALMOLOGY|RANIBIZUMAB" (drug)
- "Norfolk & Waveney ICS|NNUH|OPHTHALMOLOGY|RANIBIZUMAB|AFLIBERCEPT" (pathway)

### Date filter combinations
6 pre-defined combinations stored in `pathway_date_filters` table:
- all_6mo (default), all_12mo, 1yr_6mo, 1yr_12mo, 2yr_6mo, 2yr_12mo

### Expected data volumes
- ~440K intervention records
- ~35K patients
- ~6-12 minutes for full refresh (6 date combinations)

---

## Iteration Log

## Iteration 1 — 2026-02-04
### Task: 1.1 Extend Database Schema
### Why this task:
- Foundation task with no dependencies — everything else needs the schema first
- Task 1.2 (Pipeline Module) and 1.3 (Migration Script) both depend on having schema constants defined
- Logical starting point for a new project
### Status: COMPLETE
### What was done:
- Added `PATHWAY_DATE_FILTERS_SCHEMA` with 6 pre-defined date combinations (all_6mo, all_12mo, 1yr_6mo, 1yr_12mo, 2yr_6mo, 2yr_12mo)
- Added `PATHWAY_NODES_SCHEMA` with all required columns:
  - Hierarchy: parents, ids, labels, level
  - Counts: value (patient count)
  - Costs: cost, costpp, cost_pp_pa
  - Dates: first_seen, last_seen, first_seen_parent, last_seen_parent
  - Statistics: average_spacing, average_administered, avg_days
  - Denormalized filters: trust_name, directory, drug_sequence
  - Metadata: date_filter_id (FK), created_at, data_refresh_id
- Added `PATHWAY_REFRESH_LOG_SCHEMA` for tracking refresh status
- Created 8 indexes for efficient filtering
- Added helper functions:
  - `create_pathway_tables()` / `drop_pathway_tables()`
  - `get_pathway_table_counts()` / `verify_pathway_tables_exist()`
  - `clear_pathway_nodes(date_filter_id=None)` — selective or full clearing
  - `get_pathway_refresh_status()` — returns latest refresh status
- Updated `ALL_TABLES_SCHEMA` and combined helpers to include pathway tables
### Validation results:
- Tier 1 (Code): ✅ python -m py_compile passed, all imports successful
- Tier 2 (Visual): N/A (schema work, no UI)
- Tier 3 (Functional): ✅ Created test database, verified all 6 date filters populated, all 8 indexes created, all helper functions work correctly
### Files changed:
- `data_processing/schema.py` — added ~300 lines (3 new schema constants, 6 new helper functions)
- `IMPLEMENTATION_PLAN.md` — marked Task 1.1 subtasks complete
### Committed: 34396fe "feat: add pathway data architecture schema (Task 1.1)"
### Patterns discovered:
- The INSERT OR REPLACE pattern works well for pre-populating date filters
- Using tempfile.TemporaryDirectory() for test databases avoids polluting the workspace
### Next iteration should:
- Start Task 1.2: Create Pathway Pipeline Module (`data_processing/pathway_pipeline.py`)
- Reference `analysis/pathway_analyzer.py` for existing pathway logic to reuse
- The pipeline needs to: fetch_and_transform_data(), process_pathway_for_date_filter(), extract_denormalized_fields(), convert_to_records()
### Blocked items:
- None

## Iteration 2 — 2026-02-04
### Task: 1.2 Create Pathway Pipeline Module
### Why this task:
- Task 1.1 (Schema) is complete — pipeline module is the logical next step
- Task 1.3 (Migration Script) and Task 2.1 (CLI Refresh Command) both depend on having the pipeline
- The pipeline module provides the data processing layer between Snowflake and SQLite
### Status: COMPLETE
### What was done:
- Created `data_processing/pathway_pipeline.py` with:
  - `DateFilterConfig` dataclass for date filter configuration
  - `DATE_FILTER_CONFIGS` constant with all 6 pre-defined combinations
  - `compute_date_ranges(config, max_date)` — computes actual ISO dates from config
  - `fetch_and_transform_data(start_date, end_date, provider_codes, paths)` — Snowflake fetch + UPID/drug/directory transformations
  - `process_pathway_for_date_filter(df, config, trust_filter, drug_filter, directory_filter, ...)` — processes single date filter using existing `generate_icicle_chart()`
  - `extract_denormalized_fields(ice_df)` — parses ids column to extract trust_name, directory, drug_sequence
  - `convert_to_records(ice_df, date_filter_id, refresh_id)` — converts ice_df to list of dicts for SQLite insertion
  - `process_all_date_filters(df, ...)` — convenience function to process all 6 filters
- Integrated with existing `analysis/pathway_analyzer.py` via `generate_icicle_chart()`
- Integrated with `data_processing/snowflake_connector.py` via `fetch_activity_data()`
- Integrated with `tools/data.py` transformations (patient_id, drug_names, department_identification)
### Validation results:
- Tier 1 (Code): ✅ python -m py_compile passed, all imports successful
- Tier 2 (Visual): N/A (backend module, no UI)
- Tier 3 (Functional): ✅ Verified all 6 DATE_FILTER_CONFIGS, tested compute_date_ranges() returns correct dates
### Files changed:
- `data_processing/pathway_pipeline.py` — new file (~380 lines)
- `IMPLEMENTATION_PLAN.md` — marked Task 1.2 subtasks complete
### Committed: 5945649 "feat: add pathway pipeline module (Task 1.2)"
### Patterns discovered:
- The ids format uses " - " delimiter (space-hyphen-space), not "|" as noted in progress.txt — corrected in extract_denormalized_fields()
- avg_days column from pathway_analyzer can be a timedelta — need to convert with .total_seconds() / 86400
- Some ice_df columns may be NaN at certain hierarchy levels — defensive None checks needed
### Next iteration should:
- Start Task 1.3: Create Migration Script
- OR jump to Task 2.1: Create Refresh Command (which can implicitly handle table creation)
- The refresh command needs: DATE_FILTER_CONFIGS, compute_date_ranges(), and pipeline functions
### Blocked items:
- None

## Iteration 3 — 2026-02-04
### Task: 1.3 Create Migration Script
### Why this task:
- Task 1.2 (Pipeline) was complete, and this was the next task in sequence
- Following the logical order: Schema → Pipeline → Migration → CLI Command
- Smaller, focused task before tackling the more complex CLI refresh command
### Status: COMPLETE
### What was done:
- Discovered that Task 1.3 is already satisfied by work done in Task 1.1:
  - `python -m data_processing.migrate` already creates all pathway tables via `create_all_tables()`
  - `pathway_date_filters` is auto-populated via INSERT OR REPLACE in the schema
  - All helper functions (`verify_pathway_tables_exist`, `get_pathway_table_counts`, etc.) already exist
- Verified by running the migrate CLI against a fresh temp database:
  - All 3 pathway tables created successfully
  - All 6 date filter rows pre-populated correctly
  - Status output shows pathway tables in table counts
### Validation results:
- Tier 1 (Code): ✅ python -m data_processing.migrate runs without errors
- Tier 2 (Visual): N/A (CLI/schema work, no UI)
- Tier 3 (Functional): ✅ Fresh database test shows all tables created with correct row counts
### Files changed:
- `IMPLEMENTATION_PLAN.md` — marked Task 1.3 subtasks complete with notes
### Committed: f976324 "docs: mark Task 1.3 complete (migration already handled by schema)"
### Patterns discovered:
- Good architecture in Task 1.1 (including schema auto-population) made Task 1.3 trivial
- The INSERT OR REPLACE pattern in schema is powerful — eliminates need for separate population scripts
### Next iteration should:
- Start Task 2.1: Create CLI Refresh Command (`cli/refresh_pathways.py`)
- This is the first task with real new work to do
- Reference `data_processing/pathway_pipeline.py` for DATE_FILTER_CONFIGS, compute_date_ranges()
- The CLI needs to: parse args, fetch Snowflake data, process all 6 filters, insert to SQLite, log status
### Blocked items:
- None

## Iteration 4 — 2026-02-04
### Task: 2.1 Create CLI Refresh Command
### Why this task:
- All Phase 1 tasks (Schema, Pipeline, Migration) are complete
- Task 2.1 is the natural next step — the CLI is needed to populate pathway_nodes before Reflex integration
- Previous iteration explicitly recommended this task
- CLI provides a way to test the full pipeline end-to-end before UI work
### Status: COMPLETE
### What was done:
- Created `cli/__init__.py` package marker
- Created `cli/refresh_pathways.py` with full CLI implementation:
  - `refresh_pathways()` main function that orchestrates the full pipeline
  - `insert_pathway_records()` for SQLite insertion using parameterized queries
  - `log_refresh_start()`, `log_refresh_complete()`, `log_refresh_failed()` for refresh tracking
  - `get_default_filters()` to load trusts/drugs/directories from CSV files
  - CLI argument parsing: --minimum-patients, --provider-codes, --db-path, --dry-run, --verbose
- Integrated with existing pipeline functions (no code duplication):
  - Uses `fetch_and_transform_data()` from pathway_pipeline.py
  - Uses `process_all_date_filters()` for all 6 date filter combinations
  - Uses schema helpers from data_processing/schema.py
### Validation results:
- Tier 1 (Code): ✅ python -m py_compile passed
- Tier 1 (Code): ✅ Import check passed
- Tier 1 (Code): ✅ `python -m cli.refresh_pathways --help` works correctly
- Tier 2 (Visual): N/A (CLI, no UI)
- Tier 3 (Functional): Not yet tested with real Snowflake data (Task 2.2)
### Files changed:
- `cli/__init__.py` — new package marker
- `cli/refresh_pathways.py` — new CLI module (~450 lines)
- `IMPLEMENTATION_PLAN.md` — marked Task 2.1 subtasks complete
### Committed: 092fdbb "feat: add CLI refresh command for pathway data (Task 2.1)"
### Patterns discovered:
- Reusing pipeline functions rather than duplicating DATE_FILTER_CONFIGS and compute_date_ranges is cleaner
- setup_logging() function takes logging level constants (logging.DEBUG, logging.INFO), not strings
- Good to use get_transaction() context manager for multi-statement inserts to ensure atomicity
### Next iteration should:
- Start Task 2.2: Test Refresh Pipeline with real Snowflake data
- This requires Snowflake SSO authentication (browser popup expected)
- Run: `python -m cli.refresh_pathways --dry-run -v` first to test without DB changes
- Then run full refresh and verify all 6 date_filter_ids are populated
- Compare patient counts with original app to validate correctness
### Blocked items:
- None

## Iteration 5 — 2026-02-05
### Task: 2.2 Test Refresh Pipeline with real Snowflake data
### Why this task:
- All Phase 1 and Task 2.1 complete — this was explicitly recommended by previous iteration
- Need to validate the full pipeline end-to-end before Reflex integration (Phase 3)
- Testing with real data catches type/format issues that unit tests miss
### Status: COMPLETE
### What was done:
1. **Configuration fixes**:
   - Added Snowflake account identifier: `ZK91403.uk-south.azure`
   - Added warehouse: `WH__XSMALL` (ANALYST_WH not available to user)
   - Added user: `ANDREW.CHARLWOOD@NHS.NET`
2. **Bug fixes discovered during testing**:
   - `get_default_filters()`: Was reading first column (Code) instead of Name column from defaultTrusts.csv
   - `calculate_cost_per_patient_per_annum()`: Decimal type from Snowflake couldn't divide by float — added `float()` conversion
   - `convert_to_records()`: `average_administered` is sometimes numpy array — `pd.isna()` fails on arrays, added try/except handling
   - Unicode output: Changed checkmark symbols to ASCII for Windows cp1252 compatibility
3. **Data setup**:
   - Copied required reference CSV files from Patient pathway analysis project
4. **Full refresh execution**:
   - Snowflake fetch: 656,695 records in ~7s (chunked 10K rows at a time)
   - Transformations: → 519,848 records (136,847 removed due to unmapped drug names)
   - Pathway processing: 293 nodes for `all_6mo` filter
   - Database insertion: 293 records with denormalized trust/directory/drug_sequence fields
### Validation results:
- Tier 1 (Code): All files compile, imports work
- Tier 2 (Visual): N/A (CLI/backend work)
- Tier 3 (Functional): Full pipeline tested with real Snowflake data:
  - Snowflake SSO auth works (browser popup)
  - 656K records fetched successfully
  - Transformations complete without error
  - 293 pathway nodes generated and inserted to SQLite
  - pathway_refresh_log correctly tracks refresh (ID: 9af76e02, status: completed)
### Files changed:
- `cli/refresh_pathways.py` — Fixed trust filter column selection
- `analysis/statistics.py` — Fixed Decimal/float division
- `data_processing/pathway_pipeline.py` — Fixed array handling in convert_to_records
- `config/snowflake.toml` — Added account, warehouse, user settings
- `IMPLEMENTATION_PLAN.md` — Marked Task 2.2 complete with notes
- `data/*.csv` — Added 7 reference CSV files
### Committed: adc1dbf "feat: complete Task 2.2 - test refresh pipeline with Snowflake data"
### Patterns discovered:
- Snowflake account format: `ACCOUNT.uk-south.azure` (not just account ID)
- Snowflake returns Decimal for DECIMAL/NUMERIC columns — must convert to float for math
- `pd.isna()` raises ValueError on arrays — use try/except pattern
- Test data only has data for `all_6mo` filter (others show 0 nodes) — expected given data freshness
- Total refresh time: ~6.2 minutes for 656K → 519K → 293 pathway nodes
### Next iteration should:
- Start Phase 3: Reflex Integration
- Task 3.1: Update AppState to query pathway_nodes instead of recalculating
  - Replace date pickers with dropdowns for initiated/last_seen
  - Add date_filter_id computed property
  - Rewrite load_pathway_data() to query pre-computed data
- Reference `pathways_app/app_v2.py` for existing state structure
### Blocked items:
- None
