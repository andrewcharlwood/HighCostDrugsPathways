{"type":"system","subtype":"init","cwd":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign","session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","tools":["Task","TaskOutput","Bash","Glob","Grep","ExitPlanMode","Read","Edit","Write","NotebookEdit","WebFetch","TodoWrite","WebSearch","TaskStop","AskUserQuestion","Skill","EnterPlanMode","ToolSearch","mcp__snowflake-mcp__test_connection","mcp__snowflake-mcp__list_databases","mcp__snowflake-mcp__list_schemas","mcp__snowflake-mcp__list_tables","mcp__snowflake-mcp__list_views","mcp__snowflake-mcp__describe_table","mcp__snowflake-mcp__read_data","mcp__snowflake-mcp__get_system_health","mcp__snowflake-mcp__describe_query","mcp__snowflake-mcp__execute_async","mcp__snowflake-mcp__get_query_status","mcp__snowflake-mcp__get_async_results","mcp__snowflake-mcp__list_async_queries","mcp__snowflake-mcp__read_data_paginated","mcp__snowflake-mcp__read_data_pandas"],"mcp_servers":[{"name":"Context7","status":"failed"},{"name":"snowflake-mcp","status":"connected"}],"model":"claude-opus-4-6","permissionMode":"bypassPermissions","slash_commands":["keybindings-help","debug","frontend-design","sc:analyze","sc:build","sc:cleanup","sc:design","sc:document","sc:estimate","sc:explain","sc:git","sc:implement","sc:improve","sc:index","sc:load","sc:spawn","sc:task","sc:test","sc:troubleshoot","sc:workflow","compact","context","cost","init","pr-comments","release-notes","review","security-review","insights"],"apiKeySource":"none","claude_code_version":"2.1.32","output_style":"default","agents":["Bash","general-purpose","statusline-setup","Explore","Plan","code-reviewer","documentation-specialist","performance-optimizer","medicines-intelligence-analyst","project-analyst","team-configurator","tech-lead-orchestrator","data-scientist","postgres-expert","sql-pro","django-api-developer","django-backend-expert","django-orm-expert","laravel-backend-expert","laravel-eloquent-expert","actix-expert","android-expert","angular-expert","angularjs-expert","ansible-expert","aspnet-core-expert","astro-expert","auth0-expert","ava-expert","braintree-expert","bullmq-expert","bun-expert","c-expert","cassandra-expert","celery-expert","circleci-expert","clojure-expert","cockroachdb-expert","cpp-expert","csharp-expert","css-expert","cypress-expert","dart-expert","deno-expert","django-expert","docker-expert","dynamodb-expert","elasticsearch-expert","electron-expert","elixir-expert","elk-expert","erlang-expert","expo-expert","express-expert","fastapi-expert","fastify-expert","fiber-expert","flask-expert","flutter-expert","flyway-expert","gin-expert","github-actions-expert","gitlab-ci-expert","go-expert","grafana-expert","graphql-expert","grpc-expert","haskell-expert","html-expert","ios-expert","jasmine-expert","java-expert","javascript-expert","jenkins-expert","jest-expert","jquery-expert","jwt-expert","kafka-expert","keycloak-expert","knex-expert","kotlin-expert","kubernetes-expert","langchain-expert","laravel-expert","liquibase-expert","loki-expert","lua-expert","mariadb-expert","mocha-expert","mongodb-expert","mongoose-expert","mqtt-expert","mssql-expert","mysql-expert","nats-expert","neo4j-expert","nestjs-expert","nextjs-expert","nodejs-expert","numpy-expert","oauth-oidc-expert","ocaml-expert","openai-api-expert","openapi-expert","opensearch-expert","opentelemetry-expert","owasp-top10-expert","pandas-expert","perl-expert","phoenix-expert","php-expert","playwright-expert","prisma-expert","prometheus-expert","pulumi-expert","puppeteer-expert","python-expert","pytorch-expert","rabbitmq-expert","rails-expert","react-expert","react-native-expert","redis-expert","remix-expert","rest-expert","rollup-expert","ruby-expert","rust-expert","scala-expert","scikit-learn-expert","selenium-expert","sequelize-expert","sidekiq-expert","sns-expert","solidjs-expert","spring-boot-expert","sql-expert","sqlite-expert","sqs-expert","stripe-expert","svelte-expert","swift-expert","swiftui-expert","tailwind-expert","tauri-expert","tensorflow-expert","terraform-expert","testcafe-expert","trpc-expert","typeorm-expert","typescript-expert","vector-db-expert","vitest-expert","vue-expert","webpack-expert","websocket-expert","rails-activerecord-expert","rails-api-developer","react-component-architect","react-nextjs-expert","vue-component-architect","vue-nuxt-expert","api-architect","backend-developer","frontend-developer","tailwind-frontend-expert"],"skills":["keybindings-help","debug","frontend-design"],"plugins":[],"uuid":"a4537d3e-07f4-4452-a1ae-13d33ed69e77"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Kg3naBhMrg2SP4UXU7JM8q","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nI'll start by reading the required files in order to understand the current state of the project."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":42290,"cache_read_input_tokens":27385,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":42290},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"a2546296-b3ff-4d88-bbe1-c78775873ed7"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Kg3naBhMrg2SP4UXU7JM8q","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01ApoKpNASj2XiWMzWbbqNjF","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\progress.txt"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":42290,"cache_read_input_tokens":27385,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":42290},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"1f47b735-1596-42b9-90b2-77b1497fdd86"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01ApoKpNASj2XiWMzWbbqNjF","type":"tool_result","content":"     1ÔåÆ# Progress Log - Drug-Aware Indication Matching\n     2ÔåÆ\n     3ÔåÆ## Project Context\n     4ÔåÆ\n     5ÔåÆThis project extends the indication-based pathway charts (Phase 1-5 complete) with drug-aware matching.\n     6ÔåÆ\n     7ÔåÆ**Previous state**: Patients get ONE indication based on their most recent GP diagnosis match (SNOMED cluster codes). This ignores which drugs the patient is taking.\n     8ÔåÆ\n     9ÔåÆ**New goal**: Match each drug to an indication by cross-referencing the patient's GP diagnoses AND the drug's Search_Term mapping from DimSearchTerm.csv.\n    10ÔåÆ\n    11ÔåÆ## Key Data/Patterns\n    12ÔåÆ\n    13ÔåÆ### DimSearchTerm.csv\n    14ÔåÆ- Located at `data/DimSearchTerm.csv`\n    15ÔåÆ- Columns: Search_Term, CleanedDrugName (pipe-separated), PrimaryDirectorate\n    16ÔåÆ- ~165 rows mapping clinical conditions to drug name fragments\n    17ÔåÆ- Drug fragments are substrings that match standardized drug names from HCD data\n    18ÔåÆ- Some entries have generic fragments: INHALED, CONTINUOUS, STANDARD-DOSE, PEGYLATED\n    19ÔåÆ\n    20ÔåÆ### Current get_patient_indication_groups() in diagnosis_lookup.py\n    21ÔåÆ- Uses CLUSTER_MAPPING_SQL as CTE in Snowflake query\n    22ÔåÆ- Returns ONLY the most recent match per patient (QUALIFY ROW_NUMBER() = 1)\n    23ÔåÆ- Needs to return ALL matching Search_Terms per patient (remove QUALIFY)\n    24ÔåÆ- Batches 500 patients per query\n    25ÔåÆ\n    26ÔåÆ### Modified UPID approach\n    27ÔåÆ- Current: UPID = Provider Code[:3] + PersonKey (e.g., \"RMV12345\")\n    28ÔåÆ- New: UPID = original + \"|\" + search_term (e.g., \"RMV12345|rheumatoid arthritis\")\n    29ÔåÆ- The pipe delimiter \"|\" is safe because existing UPIDs are alphanumeric\n    30ÔåÆ- generate_icicle_chart_indication() treats UPID as an opaque identifier ÔÇö modified UPIDs work transparently\n    31ÔåÆ- The \" - \" delimiter in pathway ids is used for hierarchy levels, not within UPIDs\n    32ÔåÆ\n    33ÔåÆ### PseudoNHSNoLinked mapping\n    34ÔåÆ- HCD data has PseudoNHSNoLinked column that matches PatientPseudonym in GP records\n    35ÔåÆ- PersonKey is provider-specific local ID ÔÇö do NOT use for GP matching\n    36ÔåÆ- One PseudoNHSNoLinked can map to multiple UPIDs (multi-provider patients)\n    37ÔåÆ- GP match lookup: PseudoNHSNoLinked ÔåÆ list of matched Search_Terms\n    38ÔåÆ\n    39ÔåÆ### Drug matching logic\n    40ÔåÆ- For each HCD row (UPID + Drug Name):\n    41ÔåÆ  1. Get patient's GP-matched Search_Terms with code_frequency (via PseudoNHSNoLinked)\n    42ÔåÆ  2. Get which Search_Terms list this drug (from DimSearchTerm.csv)\n    43ÔåÆ  3. Intersection = valid indications\n    44ÔåÆ  4. If 1: use it. If multiple: pick highest code_frequency (most GP coding = most likely indication). If 0: fallback to directory.\n    45ÔåÆ- Modified UPID groups drugs under same indication together naturally\n    46ÔåÆ- code_frequency = COUNT(*) of matching SNOMED codes per Search_Term per patient in GP records\n    47ÔåÆ- GP code time range: only count codes from MIN(Intervention Date) onwards (the HCD data window)\n    48ÔåÆ  - Reduces noise from old/irrelevant diagnoses, makes frequency more meaningful\n    49ÔåÆ  - Pass earliest_hcd_date as parameter to get_patient_indication_groups()\n    50ÔåÆ- Tiebreaker rationale: 47 RA codes vs 2 crohn's codes ÔåÆ RA is clearly the active condition\n    51ÔåÆ\n    52ÔåÆ### Known edge cases\n    53ÔåÆ- Some DimSearchTerm drug fragments are generic (INHALED, ORAL, CONTINUOUS)\n    54ÔåÆ  - These could match broadly but are constrained by GP diagnosis requirement\n    55ÔåÆ- A patient visiting multiple providers has multiple UPIDs\n    56ÔåÆ  - Each UPID gets its own drug-indication matching independently\n    57ÔåÆ- Same Search_Term appears twice in DimSearchTerm.csv with different directorates\n    58ÔåÆ  - e.g., \"diabetes\" ÔåÆ DIABETIC MEDICINE and OPHTHALMOLOGY\n    59ÔåÆ  - For indication charts, we use Search_Term not directorate, so this is fine\n    60ÔåÆ\n    61ÔåÆ## Iteration Log\n    62ÔåÆ\n    63ÔåÆ## Iteration 1 ÔÇö 2026-02-05\n    64ÔåÆ### Task: 1.3 ÔÇö Build drug-to-Search_Term lookup from DimSearchTerm.csv\n    65ÔåÆ### Why this task:\n    66ÔåÆ- First iteration, chose Phase 1 foundations. Task 1.2 (CSV loading) is self-contained and testable locally without Snowflake.\n    67ÔåÆ- Task 1.1 (Snowflake query update) can't be verified without a live connection ÔÇö better to do 1.2 first.\n    68ÔåÆ- Both 1.1 and 1.2 are independent, so order doesn't matter for dependencies.\n    69ÔåÆ### Status: COMPLETE\n    70ÔåÆ### What was done:\n    71ÔåÆ- Added `load_drug_indication_mapping()` to `diagnosis_lookup.py`:\n    72ÔåÆ  - Loads `data/DimSearchTerm.csv`, builds two dicts:\n    73ÔåÆ    - `fragment_to_search_terms`: drug fragment (UPPER) ÔåÆ list of Search_Terms\n    74ÔåÆ    - `search_term_to_fragments`: search_term ÔåÆ list of drug fragments (UPPER)\n    75ÔåÆ  - Handles duplicate Search_Terms (e.g., \"diabetes\" rows combined)\n    76ÔåÆ  - Result: 164 Search_Terms, 346 drug fragments\n    77ÔåÆ- Added `get_search_terms_for_drug()` to `diagnosis_lookup.py`:\n    78ÔåÆ  - Returns all Search_Terms whose drug fragments are substrings of the drug name (case-insensitive)\n    79ÔåÆ  - Named differently from plan's `drug_matches_search_term()` ÔÇö returns all matches at once rather than single boolean, more practical for Phase 2\n    80ÔåÆ- Updated `__all__` exports\n    81ÔåÆ### Validation results:\n    82ÔåÆ- Tier 1 (Code): py_compile passed, import check passed\n    83ÔåÆ- Tier 2 (Data): ADALIMUMAB ÔåÆ 7 indications (including axial spondyloarthritis, rheumatoid arthritis), OMALIZUMAB ÔåÆ 4 indications (asthma, allergic asthma, etc.), PEGYLATED LIPOSOMAL DOXORUBICIN ÔåÆ 4 matches via substring, \"ADALIMUMAB 40MG\" matches correctly with dosage info, diabetes fragments combined from 2 CSV rows\n    84ÔåÆ- Tier 3 (Functional): N/A (no UI changes)\n    85ÔåÆ### Files changed:\n    86ÔåÆ- data_processing/diagnosis_lookup.py (added load_drug_indication_mapping, get_search_terms_for_drug)\n    87ÔåÆ- IMPLEMENTATION_PLAN.md (marked 1.2 subtasks [x])\n    88ÔåÆ### Committed: 0779df7 \"feat: add drug-to-indication mapping from DimSearchTerm.csv (Task 1.3)\"\n    89ÔåÆ### Patterns discovered:\n    90ÔåÆ- DimSearchTerm.csv has 164 unique Search_Terms (not 165 as noted) because diabetes appears twice with different directorates but same Search_Term\n    91ÔåÆ- Some drug fragments are very generic: INHALED, CONTINUOUS, ORAL, STANDARD-DOSE, INTRAVENOUS, PEGYLATED, ROUTINE, INDUCTION ÔÇö these will match broadly but are constrained by the GP diagnosis requirement in Phase 2\n    92ÔåÆ- Function signatures for Phase 2: `get_search_terms_for_drug(drug_name, search_term_to_fragments)` returns list[str] ÔÇö use this to get candidate indications per drug\n    93ÔåÆ### Next iteration should:\n    94ÔåÆ- Work on Task 1.2: Merge asthma Search_Terms in CLUSTER_MAPPING_SQL and load_drug_indication_mapping()\n    95ÔåÆ  - Merge \"allergic asthma\", \"asthma\", \"severe persistent allergic asthma\" ÔåÆ \"asthma\"\n    96ÔåÆ  - Keep \"urticaria\" separate\n    97ÔåÆ  - This is self-contained and testable locally\n    98ÔåÆ- OR work on Task 1.1: Update `get_patient_indication_groups()` to return ALL matches with code_frequency\n    99ÔåÆ  - The current query at line ~1352 of diagnosis_lookup.py uses `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1` ÔÇö this must be replaced with GROUP BY + COUNT(*)\n   100ÔåÆ  - Add `earliest_hcd_date` parameter to restrict GP codes to HCD data window\n   101ÔåÆ  - Return columns: PatientPseudonym, Search_Term, code_frequency (not EventDateTime)\n   102ÔåÆ- OR if Snowflake isn't available to test 1.1, skip to Task 2.1 (assign_drug_indications function) which can be built and tested with mock data\n   103ÔåÆ### Blocked items:\n   104ÔåÆ- None\n   105ÔåÆ\n   106ÔåÆ## Iteration 2 ÔÇö 2026-02-05\n   107ÔåÆ### Task: 1.2 ÔÇö Merge related asthma Search_Terms in CLUSTER_MAPPING_SQL\n   108ÔåÆ### Why this task:\n   109ÔåÆ- Previous iteration recommended this as the next task (self-contained, testable locally)\n   110ÔåÆ- Both CLUSTER_MAPPING_SQL and load_drug_indication_mapping() need consistent Search_Term names\n   111ÔåÆ- Must be done before Task 1.1 (Snowflake query) to ensure GP lookups return \"asthma\" not \"allergic asthma\"\n   112ÔåÆ### Status: COMPLETE\n   113ÔåÆ### What was done:\n   114ÔåÆ- Updated CLUSTER_MAPPING_SQL: changed 'allergic asthma' ÔåÆ 'asthma' (AST_COD) and 'severe persistent allergic asthma' ÔåÆ 'asthma' (SEVAST_COD)\n   115ÔåÆ  - Now 3 rows for 'asthma': AST_COD, eFI2_Asthma, SEVAST_COD\n   116ÔåÆ  - urticaria (XSAL_COD) stays separate\n   117ÔåÆ- Added SEARCH_TERM_MERGE_MAP constant: {\"allergic asthma\": \"asthma\", \"severe persistent allergic asthma\": \"asthma\"}\n   118ÔåÆ- Updated load_drug_indication_mapping() to apply SEARCH_TERM_MERGE_MAP when loading CSV\n   119ÔåÆ  - Normalizes Search_Term before accumulating fragments\n   120ÔåÆ  - Drug fragments from all 3 original rows combined under \"asthma\" key\n   121ÔåÆ- Exported SEARCH_TERM_MERGE_MAP in __all__\n   122ÔåÆ### Validation results:\n   123ÔåÆ- Tier 1 (Code): py_compile passed, import check passed\n   124ÔåÆ- Tier 2 (Data):\n   125ÔåÆ  - \"asthma\" fragments: OMALIZUMAB, BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, RESLIZUMAB (complete combined list)\n   126ÔåÆ  - \"allergic asthma\" no longer exists as separate key\n   127ÔåÆ  - \"severe persistent allergic asthma\" no longer exists as separate key\n   128ÔåÆ  - \"urticaria\" ÔåÆ ['OMALIZUMAB'] ÔÇö correctly separate\n   129ÔåÆ  - OMALIZUMAB maps to: ['asthma', 'urticaria'] ÔÇö correct\n   130ÔåÆ  - Total Search_Terms: 162 (was 164, 3 asthma entries ÔåÆ 1)\n   131ÔåÆ  - Total fragments: 346 (unchanged)\n   132ÔåÆ- Tier 3 (Functional): N/A (no UI changes)\n   133ÔåÆ### Files changed:\n   134ÔåÆ- data_processing/diagnosis_lookup.py (CLUSTER_MAPPING_SQL, SEARCH_TERM_MERGE_MAP, load_drug_indication_mapping)\n   135ÔåÆ- IMPLEMENTATION_PLAN.md (marked 1.2 subtasks [x])\n   136ÔåÆ### Committed: b0a8a9d \"feat: merge asthma Search_Term variants in CLUSTER_MAPPING_SQL and drug mapping (Task 1.2)\"\n   137ÔåÆ### Patterns discovered:\n   138ÔåÆ- SEARCH_TERM_MERGE_MAP is reusable: any future module that receives Search_Terms from Snowflake can apply the same normalization\n   139ÔåÆ- The merge approach (normalize at load time) is cleaner than post-hoc deduplication\n   140ÔåÆ### Next iteration should:\n   141ÔåÆ- Work on Task 1.1: Update `get_patient_indication_groups()` to return ALL matches with code_frequency\n   142ÔåÆ  - The current query at ~line 1467 uses `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1`\n   143ÔåÆ  - Replace with GROUP BY + COUNT(*) for code_frequency\n   144ÔåÆ  - Add `earliest_hcd_date` parameter to restrict GP codes to HCD data window\n   145ÔåÆ  - Return columns: PatientPseudonym, Search_Term, code_frequency\n   146ÔåÆ  - Empty DataFrame columns should match new return type\n   147ÔåÆ  - This requires Snowflake connectivity to fully test, but code changes can be verified with py_compile and import checks\n   148ÔåÆ- OR work on Task 2.1: Create assign_drug_indications() ÔÇö can be built and tested with mock data\n   149ÔåÆ  - This is independent of Task 1.1 if you mock the gp_matches_df input\n   150ÔåÆ### Blocked items:\n   151ÔåÆ- None\n   152ÔåÆ\n   153ÔåÆ## Iteration 3 ÔÇö 2026-02-05\n   154ÔåÆ### Task: 1.1 ÔÇö Update get_patient_indication_groups() to return ALL matches with code_frequency\n   155ÔåÆ### Why this task:\n   156ÔåÆ- Previous iteration recommended this as the next task\n   157ÔåÆ- This is the last remaining Phase 1 dependency ÔÇö Tasks 2.1/2.2 need the GP matches data in the right format (multiple rows per patient with code_frequency)\n   158ÔåÆ- Task 1.1 changes the query fundamentals; better to do this before building assign_drug_indications() which consumes the output\n   159ÔåÆ### Status: COMPLETE\n   160ÔåÆ### What was done:\n   161ÔåÆ- Modified `get_patient_indication_groups()` in `diagnosis_lookup.py`:\n   162ÔåÆ  1. **Removed** `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1`\n   163ÔåÆ  2. **Replaced** with `GROUP BY pc.\"PatientPseudonym\", aic.Search_Term` + `COUNT(*) AS \"code_frequency\"`\n   164ÔåÆ  3. **Added** `earliest_hcd_date: Optional[str] = None` parameter\n   165ÔåÆ  4. **Added** optional `AND pc.\"EventDateTime\" >= %s` when earliest_hcd_date is provided\n   166ÔåÆ  5. **Updated** return columns from `(PatientPseudonym, Search_Term, EventDateTime)` to `(PatientPseudonym, Search_Term, code_frequency)`\n   167ÔåÆ  6. **Updated** all empty DataFrame returns to use new column names\n   168ÔåÆ  7. **Updated** logging to show multiple-rows-per-patient stats (avg indications per patient)\n   169ÔåÆ  8. **Updated** docstring to describe new behavior and parameters\n   170ÔåÆ- Backward compatible: `earliest_hcd_date` defaults to `None`, existing callers still work\n   171ÔåÆ- Note: caller in `refresh_pathways.py` (line 424-428) does `dict(zip(...))` which will only keep last match per patient with new multi-row format ÔÇö this will be updated in Task 3.1\n   172ÔåÆ### Validation results:\n   173ÔåÆ- Tier 1 (Code): py_compile PASSED, import check PASSED, function signature verified\n   174ÔåÆ- Tier 2 (Data): Empty DataFrame returns correct columns ['PatientPseudonym', 'Search_Term', 'code_frequency']; live Snowflake test deferred to Phase 3/4\n   175ÔåÆ- Tier 3 (Functional): N/A (no UI changes)\n   176ÔåÆ### Files changed:\n   177ÔåÆ- data_processing/diagnosis_lookup.py (modified get_patient_indication_groups function)\n   178ÔåÆ- IMPLEMENTATION_PLAN.md (marked 1.1 subtasks [x])\n   179ÔåÆ### Committed: c93417f \"feat: return ALL GP matches with code_frequency in get_patient_indication_groups (Task 1.1)\"\n   180ÔåÆ### Patterns discovered:\n   181ÔåÆ- The `earliest_hcd_date` parameter is passed as a string in ISO format (YYYY-MM-DD) via Snowflake %s placeholder ÔÇö Snowflake handles string-to-timestamp comparison implicitly\n   182ÔåÆ- The GROUP BY approach naturally deduplicates SNOMED codes within the same Search_Term ÔÇö a patient with the same SNOMED code recorded 5 times gets code_frequency=5 (reflecting clinical activity intensity)\n   183ÔåÆ- params list is built dynamically: `batch_pseudonyms + [earliest_hcd_date]` only when date filter is active\n   184ÔåÆ### Next iteration should:\n   185ÔåÆ- Work on Task 2.1: Create `assign_drug_indications()` function\n   186ÔåÆ  - This is now unblocked since 1.1 is complete (return format is known)\n   187ÔåÆ  - Input: HCD df, gp_matches_df (PatientPseudonym, Search_Term, code_frequency), drug_mapping from load_drug_indication_mapping()\n   188ÔåÆ  - Output: (modified_df with UPID|search_term, indication_df mapping modified_UPID ÔåÆ Search_Term)\n   189ÔåÆ  - Can be built and tested with mock data (no Snowflake needed)\n   190ÔåÆ  - Key logic: for each UPID+Drug pair, intersect drug's Search_Terms with patient's GP matches, pick highest code_frequency as tiebreaker\n   191ÔåÆ  - The function needs PseudoNHSNoLinked to look up GP matches, so the df must have that column\n   192ÔåÆ  - Task 2.2 (tiebreaker logic) can be done within 2.1 or as a follow-up\n   193ÔåÆ- The final Phase 1 subtask (1.1 verify with live Snowflake) will be tested during Phase 3/4 integration\n   194ÔåÆ### Blocked items:\n   195ÔåÆ- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4\n   196ÔåÆ\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"83a40667-0748-4134-9628-f533332664e4","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\progress.txt","content":"# Progress Log - Drug-Aware Indication Matching\n\n## Project Context\n\nThis project extends the indication-based pathway charts (Phase 1-5 complete) with drug-aware matching.\n\n**Previous state**: Patients get ONE indication based on their most recent GP diagnosis match (SNOMED cluster codes). This ignores which drugs the patient is taking.\n\n**New goal**: Match each drug to an indication by cross-referencing the patient's GP diagnoses AND the drug's Search_Term mapping from DimSearchTerm.csv.\n\n## Key Data/Patterns\n\n### DimSearchTerm.csv\n- Located at `data/DimSearchTerm.csv`\n- Columns: Search_Term, CleanedDrugName (pipe-separated), PrimaryDirectorate\n- ~165 rows mapping clinical conditions to drug name fragments\n- Drug fragments are substrings that match standardized drug names from HCD data\n- Some entries have generic fragments: INHALED, CONTINUOUS, STANDARD-DOSE, PEGYLATED\n\n### Current get_patient_indication_groups() in diagnosis_lookup.py\n- Uses CLUSTER_MAPPING_SQL as CTE in Snowflake query\n- Returns ONLY the most recent match per patient (QUALIFY ROW_NUMBER() = 1)\n- Needs to return ALL matching Search_Terms per patient (remove QUALIFY)\n- Batches 500 patients per query\n\n### Modified UPID approach\n- Current: UPID = Provider Code[:3] + PersonKey (e.g., \"RMV12345\")\n- New: UPID = original + \"|\" + search_term (e.g., \"RMV12345|rheumatoid arthritis\")\n- The pipe delimiter \"|\" is safe because existing UPIDs are alphanumeric\n- generate_icicle_chart_indication() treats UPID as an opaque identifier ÔÇö modified UPIDs work transparently\n- The \" - \" delimiter in pathway ids is used for hierarchy levels, not within UPIDs\n\n### PseudoNHSNoLinked mapping\n- HCD data has PseudoNHSNoLinked column that matches PatientPseudonym in GP records\n- PersonKey is provider-specific local ID ÔÇö do NOT use for GP matching\n- One PseudoNHSNoLinked can map to multiple UPIDs (multi-provider patients)\n- GP match lookup: PseudoNHSNoLinked ÔåÆ list of matched Search_Terms\n\n### Drug matching logic\n- For each HCD row (UPID + Drug Name):\n  1. Get patient's GP-matched Search_Terms with code_frequency (via PseudoNHSNoLinked)\n  2. Get which Search_Terms list this drug (from DimSearchTerm.csv)\n  3. Intersection = valid indications\n  4. If 1: use it. If multiple: pick highest code_frequency (most GP coding = most likely indication). If 0: fallback to directory.\n- Modified UPID groups drugs under same indication together naturally\n- code_frequency = COUNT(*) of matching SNOMED codes per Search_Term per patient in GP records\n- GP code time range: only count codes from MIN(Intervention Date) onwards (the HCD data window)\n  - Reduces noise from old/irrelevant diagnoses, makes frequency more meaningful\n  - Pass earliest_hcd_date as parameter to get_patient_indication_groups()\n- Tiebreaker rationale: 47 RA codes vs 2 crohn's codes ÔåÆ RA is clearly the active condition\n\n### Known edge cases\n- Some DimSearchTerm drug fragments are generic (INHALED, ORAL, CONTINUOUS)\n  - These could match broadly but are constrained by GP diagnosis requirement\n- A patient visiting multiple providers has multiple UPIDs\n  - Each UPID gets its own drug-indication matching independently\n- Same Search_Term appears twice in DimSearchTerm.csv with different directorates\n  - e.g., \"diabetes\" ÔåÆ DIABETIC MEDICINE and OPHTHALMOLOGY\n  - For indication charts, we use Search_Term not directorate, so this is fine\n\n## Iteration Log\n\n## Iteration 1 ÔÇö 2026-02-05\n### Task: 1.3 ÔÇö Build drug-to-Search_Term lookup from DimSearchTerm.csv\n### Why this task:\n- First iteration, chose Phase 1 foundations. Task 1.2 (CSV loading) is self-contained and testable locally without Snowflake.\n- Task 1.1 (Snowflake query update) can't be verified without a live connection ÔÇö better to do 1.2 first.\n- Both 1.1 and 1.2 are independent, so order doesn't matter for dependencies.\n### Status: COMPLETE\n### What was done:\n- Added `load_drug_indication_mapping()` to `diagnosis_lookup.py`:\n  - Loads `data/DimSearchTerm.csv`, builds two dicts:\n    - `fragment_to_search_terms`: drug fragment (UPPER) ÔåÆ list of Search_Terms\n    - `search_term_to_fragments`: search_term ÔåÆ list of drug fragments (UPPER)\n  - Handles duplicate Search_Terms (e.g., \"diabetes\" rows combined)\n  - Result: 164 Search_Terms, 346 drug fragments\n- Added `get_search_terms_for_drug()` to `diagnosis_lookup.py`:\n  - Returns all Search_Terms whose drug fragments are substrings of the drug name (case-insensitive)\n  - Named differently from plan's `drug_matches_search_term()` ÔÇö returns all matches at once rather than single boolean, more practical for Phase 2\n- Updated `__all__` exports\n### Validation results:\n- Tier 1 (Code): py_compile passed, import check passed\n- Tier 2 (Data): ADALIMUMAB ÔåÆ 7 indications (including axial spondyloarthritis, rheumatoid arthritis), OMALIZUMAB ÔåÆ 4 indications (asthma, allergic asthma, etc.), PEGYLATED LIPOSOMAL DOXORUBICIN ÔåÆ 4 matches via substring, \"ADALIMUMAB 40MG\" matches correctly with dosage info, diabetes fragments combined from 2 CSV rows\n- Tier 3 (Functional): N/A (no UI changes)\n### Files changed:\n- data_processing/diagnosis_lookup.py (added load_drug_indication_mapping, get_search_terms_for_drug)\n- IMPLEMENTATION_PLAN.md (marked 1.2 subtasks [x])\n### Committed: 0779df7 \"feat: add drug-to-indication mapping from DimSearchTerm.csv (Task 1.3)\"\n### Patterns discovered:\n- DimSearchTerm.csv has 164 unique Search_Terms (not 165 as noted) because diabetes appears twice with different directorates but same Search_Term\n- Some drug fragments are very generic: INHALED, CONTINUOUS, ORAL, STANDARD-DOSE, INTRAVENOUS, PEGYLATED, ROUTINE, INDUCTION ÔÇö these will match broadly but are constrained by the GP diagnosis requirement in Phase 2\n- Function signatures for Phase 2: `get_search_terms_for_drug(drug_name, search_term_to_fragments)` returns list[str] ÔÇö use this to get candidate indications per drug\n### Next iteration should:\n- Work on Task 1.2: Merge asthma Search_Terms in CLUSTER_MAPPING_SQL and load_drug_indication_mapping()\n  - Merge \"allergic asthma\", \"asthma\", \"severe persistent allergic asthma\" ÔåÆ \"asthma\"\n  - Keep \"urticaria\" separate\n  - This is self-contained and testable locally\n- OR work on Task 1.1: Update `get_patient_indication_groups()` to return ALL matches with code_frequency\n  - The current query at line ~1352 of diagnosis_lookup.py uses `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1` ÔÇö this must be replaced with GROUP BY + COUNT(*)\n  - Add `earliest_hcd_date` parameter to restrict GP codes to HCD data window\n  - Return columns: PatientPseudonym, Search_Term, code_frequency (not EventDateTime)\n- OR if Snowflake isn't available to test 1.1, skip to Task 2.1 (assign_drug_indications function) which can be built and tested with mock data\n### Blocked items:\n- None\n\n## Iteration 2 ÔÇö 2026-02-05\n### Task: 1.2 ÔÇö Merge related asthma Search_Terms in CLUSTER_MAPPING_SQL\n### Why this task:\n- Previous iteration recommended this as the next task (self-contained, testable locally)\n- Both CLUSTER_MAPPING_SQL and load_drug_indication_mapping() need consistent Search_Term names\n- Must be done before Task 1.1 (Snowflake query) to ensure GP lookups return \"asthma\" not \"allergic asthma\"\n### Status: COMPLETE\n### What was done:\n- Updated CLUSTER_MAPPING_SQL: changed 'allergic asthma' ÔåÆ 'asthma' (AST_COD) and 'severe persistent allergic asthma' ÔåÆ 'asthma' (SEVAST_COD)\n  - Now 3 rows for 'asthma': AST_COD, eFI2_Asthma, SEVAST_COD\n  - urticaria (XSAL_COD) stays separate\n- Added SEARCH_TERM_MERGE_MAP constant: {\"allergic asthma\": \"asthma\", \"severe persistent allergic asthma\": \"asthma\"}\n- Updated load_drug_indication_mapping() to apply SEARCH_TERM_MERGE_MAP when loading CSV\n  - Normalizes Search_Term before accumulating fragments\n  - Drug fragments from all 3 original rows combined under \"asthma\" key\n- Exported SEARCH_TERM_MERGE_MAP in __all__\n### Validation results:\n- Tier 1 (Code): py_compile passed, import check passed\n- Tier 2 (Data):\n  - \"asthma\" fragments: OMALIZUMAB, BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, RESLIZUMAB (complete combined list)\n  - \"allergic asthma\" no longer exists as separate key\n  - \"severe persistent allergic asthma\" no longer exists as separate key\n  - \"urticaria\" ÔåÆ ['OMALIZUMAB'] ÔÇö correctly separate\n  - OMALIZUMAB maps to: ['asthma', 'urticaria'] ÔÇö correct\n  - Total Search_Terms: 162 (was 164, 3 asthma entries ÔåÆ 1)\n  - Total fragments: 346 (unchanged)\n- Tier 3 (Functional): N/A (no UI changes)\n### Files changed:\n- data_processing/diagnosis_lookup.py (CLUSTER_MAPPING_SQL, SEARCH_TERM_MERGE_MAP, load_drug_indication_mapping)\n- IMPLEMENTATION_PLAN.md (marked 1.2 subtasks [x])\n### Committed: b0a8a9d \"feat: merge asthma Search_Term variants in CLUSTER_MAPPING_SQL and drug mapping (Task 1.2)\"\n### Patterns discovered:\n- SEARCH_TERM_MERGE_MAP is reusable: any future module that receives Search_Terms from Snowflake can apply the same normalization\n- The merge approach (normalize at load time) is cleaner than post-hoc deduplication\n### Next iteration should:\n- Work on Task 1.1: Update `get_patient_indication_groups()` to return ALL matches with code_frequency\n  - The current query at ~line 1467 uses `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1`\n  - Replace with GROUP BY + COUNT(*) for code_frequency\n  - Add `earliest_hcd_date` parameter to restrict GP codes to HCD data window\n  - Return columns: PatientPseudonym, Search_Term, code_frequency\n  - Empty DataFrame columns should match new return type\n  - This requires Snowflake connectivity to fully test, but code changes can be verified with py_compile and import checks\n- OR work on Task 2.1: Create assign_drug_indications() ÔÇö can be built and tested with mock data\n  - This is independent of Task 1.1 if you mock the gp_matches_df input\n### Blocked items:\n- None\n\n## Iteration 3 ÔÇö 2026-02-05\n### Task: 1.1 ÔÇö Update get_patient_indication_groups() to return ALL matches with code_frequency\n### Why this task:\n- Previous iteration recommended this as the next task\n- This is the last remaining Phase 1 dependency ÔÇö Tasks 2.1/2.2 need the GP matches data in the right format (multiple rows per patient with code_frequency)\n- Task 1.1 changes the query fundamentals; better to do this before building assign_drug_indications() which consumes the output\n### Status: COMPLETE\n### What was done:\n- Modified `get_patient_indication_groups()` in `diagnosis_lookup.py`:\n  1. **Removed** `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1`\n  2. **Replaced** with `GROUP BY pc.\"PatientPseudonym\", aic.Search_Term` + `COUNT(*) AS \"code_frequency\"`\n  3. **Added** `earliest_hcd_date: Optional[str] = None` parameter\n  4. **Added** optional `AND pc.\"EventDateTime\" >= %s` when earliest_hcd_date is provided\n  5. **Updated** return columns from `(PatientPseudonym, Search_Term, EventDateTime)` to `(PatientPseudonym, Search_Term, code_frequency)`\n  6. **Updated** all empty DataFrame returns to use new column names\n  7. **Updated** logging to show multiple-rows-per-patient stats (avg indications per patient)\n  8. **Updated** docstring to describe new behavior and parameters\n- Backward compatible: `earliest_hcd_date` defaults to `None`, existing callers still work\n- Note: caller in `refresh_pathways.py` (line 424-428) does `dict(zip(...))` which will only keep last match per patient with new multi-row format ÔÇö this will be updated in Task 3.1\n### Validation results:\n- Tier 1 (Code): py_compile PASSED, import check PASSED, function signature verified\n- Tier 2 (Data): Empty DataFrame returns correct columns ['PatientPseudonym', 'Search_Term', 'code_frequency']; live Snowflake test deferred to Phase 3/4\n- Tier 3 (Functional): N/A (no UI changes)\n### Files changed:\n- data_processing/diagnosis_lookup.py (modified get_patient_indication_groups function)\n- IMPLEMENTATION_PLAN.md (marked 1.1 subtasks [x])\n### Committed: c93417f \"feat: return ALL GP matches with code_frequency in get_patient_indication_groups (Task 1.1)\"\n### Patterns discovered:\n- The `earliest_hcd_date` parameter is passed as a string in ISO format (YYYY-MM-DD) via Snowflake %s placeholder ÔÇö Snowflake handles string-to-timestamp comparison implicitly\n- The GROUP BY approach naturally deduplicates SNOMED codes within the same Search_Term ÔÇö a patient with the same SNOMED code recorded 5 times gets code_frequency=5 (reflecting clinical activity intensity)\n- params list is built dynamically: `batch_pseudonyms + [earliest_hcd_date]` only when date filter is active\n### Next iteration should:\n- Work on Task 2.1: Create `assign_drug_indications()` function\n  - This is now unblocked since 1.1 is complete (return format is known)\n  - Input: HCD df, gp_matches_df (PatientPseudonym, Search_Term, code_frequency), drug_mapping from load_drug_indication_mapping()\n  - Output: (modified_df with UPID|search_term, indication_df mapping modified_UPID ÔåÆ Search_Term)\n  - Can be built and tested with mock data (no Snowflake needed)\n  - Key logic: for each UPID+Drug pair, intersect drug's Search_Terms with patient's GP matches, pick highest code_frequency as tiebreaker\n  - The function needs PseudoNHSNoLinked to look up GP matches, so the df must have that column\n  - Task 2.2 (tiebreaker logic) can be done within 2.1 or as a follow-up\n- The final Phase 1 subtask (1.1 verify with live Snowflake) will be tested during Phase 3/4 integration\n### Blocked items:\n- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4\n","numLines":196,"startLine":1,"totalLines":196}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Kg3naBhMrg2SP4UXU7JM8q","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012JajNLrAdgPZ8g56P1xF6g","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\IMPLEMENTATION_PLAN.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":42290,"cache_read_input_tokens":27385,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":42290},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"58daf048-327c-4bc9-804c-3a450afdb3ce"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012JajNLrAdgPZ8g56P1xF6g","type":"tool_result","content":"     1ÔåÆ# Implementation Plan - Drug-Aware Indication Matching\n     2ÔåÆ\n     3ÔåÆ## Project Overview\n     4ÔåÆ\n     5ÔåÆUpdate the indication-based pathway charts so that patient indications are matched **per drug**, not just per patient. Currently, each patient gets ONE indication (most recent GP diagnosis match). This ignores which drugs the patient is actually taking.\n     6ÔåÆ\n     7ÔåÆ### The Problem\n     8ÔåÆ\n     9ÔåÆA patient on ADALIMUMAB + OMALIZUMAB currently gets assigned a single indication (e.g., \"rheumatoid arthritis\" ÔÇö the most recent GP match). But:\n    10ÔåÆ- ADALIMUMAB is used for rheumatoid arthritis, axial spondyloarthritis, crohn's disease, etc.\n    11ÔåÆ- OMALIZUMAB is used for asthma, allergic asthma, urticaria\n    12ÔåÆ\n    13ÔåÆThese are different clinical pathways and should be treated as separate treatment journeys.\n    14ÔåÆ\n    15ÔåÆ### The Solution\n    16ÔåÆ\n    17ÔåÆMatch each drug to an indication by cross-referencing:\n    18ÔåÆ1. **GP diagnosis** ÔÇö which Search_Terms the patient has matching SNOMED codes for\n    19ÔåÆ2. **Drug mapping** ÔÇö which Search_Terms list each drug (from `DimSearchTerm.csv`)\n    20ÔåÆ\n    21ÔåÆOnly assign a drug to an indication if BOTH conditions are met. If a patient's drugs map to different indications, they become separate pathways (via modified UPID).\n    22ÔåÆ\n    23ÔåÆ### Key Design Decisions\n    24ÔåÆ\n    25ÔåÆ| Aspect | Decision |\n    26ÔåÆ|--------|----------|\n    27ÔåÆ| Drug-indication source | `data/DimSearchTerm.csv` ÔÇö Search_Term ÔåÆ CleanedDrugName mapping |\n    28ÔåÆ| UPID modification | `{original_UPID}\\|{search_term}` for drugs with matched indication |\n    29ÔåÆ| GP diagnosis matching | Return ALL matches per patient (not just most recent) |\n    30ÔåÆ| Drug matching | Substring match: HCD drug name contains DimSearchTerm fragment |\n    31ÔåÆ| Multiple indication matches per drug | Use highest GP code frequency as tiebreaker (COUNT of matching SNOMED codes per Search_Term) |\n    32ÔåÆ| GP code time range | Only codes from MIN(Intervention Date) onwards ÔÇö restricts to HCD data window |\n    33ÔåÆ| No indication match | Fallback to directory (same as current behavior) |\n    34ÔåÆ| Same patient, different indications | Separate pathways via different modified UPIDs |\n    35ÔåÆ\n    36ÔåÆ### Examples\n    37ÔåÆ\n    38ÔåÆ**Patient on ADALIMUMAB + GOLIMUMAB, GP dx: axial spondyloarthritis + asthma**\n    39ÔåÆ- axial spondyloarthritis drug list includes both ADALIMUMAB and GOLIMUMAB\n    40ÔåÆ- ÔåÆ Both drugs grouped under \"axial spondyloarthritis\", single pathway\n    41ÔåÆ- Modified UPID: `RMV12345|axial spondyloarthritis`\n    42ÔåÆ\n    43ÔåÆ**Patient on ADALIMUMAB + OMALIZUMAB, GP dx: axial spondyloarthritis + asthma**\n    44ÔåÆ- axial spondyloarthritis lists ADALIMUMAB but not OMALIZUMAB\n    45ÔåÆ- asthma lists OMALIZUMAB but not ADALIMUMAB\n    46ÔåÆ- ÔåÆ Two separate pathways:\n    47ÔåÆ  - `RMV12345|axial spondyloarthritis` with ADALIMUMAB\n    48ÔåÆ  - `RMV12345|asthma` with OMALIZUMAB\n    49ÔåÆ\n    50ÔåÆ**Patient on ADALIMUMAB, GP dx: rheumatoid arthritis (47 codes) + crohn's disease (2 codes)**\n    51ÔåÆ- Both Search_Terms list ADALIMUMAB AND patient has GP dx for both\n    52ÔåÆ- ÔåÆ Tiebreaker: highest code frequency ÔÇö rheumatoid arthritis has 47 matching SNOMED codes vs 2 for crohn's\n    53ÔåÆ- ÔåÆ Single pathway under rheumatoid arthritis (more clinical activity = more likely the treatment indication)\n    54ÔåÆ\n    55ÔåÆ---\n    56ÔåÆ\n    57ÔåÆ## Phase 1: Update Snowflake Query & Drug Mapping\n    58ÔåÆ\n    59ÔåÆ### 1.1 Update `get_patient_indication_groups()` to return ALL matches with frequency\n    60ÔåÆ- [x] Modify the Snowflake query in `get_patient_indication_groups()` (diagnosis_lookup.py):\n    61ÔåÆ  - Remove `QUALIFY ROW_NUMBER() OVER (PARTITION BY ... ORDER BY EventDateTime DESC) = 1`\n    62ÔåÆ  - Return ALL matching Search_Terms per patient with code frequency:\n    63ÔåÆ    ```sql\n    64ÔåÆ    SELECT pc.\"PatientPseudonym\" AS \"PatientPseudonym\",\n    65ÔåÆ           aic.Search_Term AS \"Search_Term\",\n    66ÔåÆ           COUNT(*) AS \"code_frequency\"\n    67ÔåÆ    FROM PrimaryCareClinicalCoding pc\n    68ÔåÆ    JOIN AllIndicationCodes aic ON pc.\"SNOMEDCode\" = aic.SNOMEDCode\n    69ÔåÆ    WHERE pc.\"PatientPseudonym\" IN (...)\n    70ÔåÆ      AND pc.\"EventDateTime\" >= :earliest_hcd_date\n    71ÔåÆ    GROUP BY pc.\"PatientPseudonym\", aic.Search_Term\n    72ÔåÆ    ```\n    73ÔåÆ  - `code_frequency` = number of matching SNOMED codes per Search_Term per patient\n    74ÔåÆ  - Higher frequency = more clinical activity = stronger signal for tiebreaker\n    75ÔåÆ  - `earliest_hcd_date` = `MIN(Intervention Date)` from the HCD DataFrame ÔÇö restricts GP codes to the HCD data window, reducing noise from old/irrelevant diagnoses\n    76ÔåÆ- [x] Accept `earliest_hcd_date` parameter in `get_patient_indication_groups()` and pass to query\n    77ÔåÆ- [x] Keep batch processing (500 patients per query)\n    78ÔåÆ- [x] Update return type: DataFrame now has multiple rows per patient (PatientPseudonym, Search_Term, code_frequency)\n    79ÔåÆ- [ ] Verify: Query returns more rows than before (patients with multiple matching diagnoses) *(requires live Snowflake ÔÇö will be verified in Phase 3/4)*\n    80ÔåÆ\n    81ÔåÆ### 1.2 Merge related asthma Search_Terms in CLUSTER_MAPPING_SQL\n    82ÔåÆ- [x] In `CLUSTER_MAPPING_SQL` (diagnosis_lookup.py), merge these 3 Search_Terms into one `\"asthma\"` entry:\n    83ÔåÆ  - `allergic asthma` (Cluster: OMALIZUMAB only)\n    84ÔåÆ  - `asthma` (Cluster: BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, OMALIZUMAB, RESLIZUMAB)\n    85ÔåÆ  - `severe persistent allergic asthma` (Cluster: OMALIZUMAB only)\n    86ÔåÆ- [x] Map all 3 Cluster_IDs to `Search_Term = 'asthma'` in the CTE VALUES\n    87ÔåÆ- [x] `urticaria` (OMALIZUMAB, DERMATOLOGY) stays SEPARATE ÔÇö do NOT merge with asthma\n    88ÔåÆ- [x] Also update `load_drug_indication_mapping()` to apply the same merge when loading DimSearchTerm.csv:\n    89ÔåÆ  - Combine drug lists from all 3 entries under a single `\"asthma\"` key\n    90ÔåÆ  - Deduplicate drug fragments (OMALIZUMAB appears in all 3)\n    91ÔåÆ- [x] Verify: GP code lookup returns `\"asthma\"` (not `\"allergic asthma\"` or `\"severe persistent allergic asthma\"`)\n    92ÔåÆ- [x] Verify: Drug mapping for `\"asthma\"` includes full combined drug list: BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, OMALIZUMAB, RESLIZUMAB\n    93ÔåÆ\n    94ÔåÆ### 1.3 Build drug-to-Search_Term lookup from DimSearchTerm.csv\n    95ÔåÆ- [x] Add function `load_drug_indication_mapping()` to `diagnosis_lookup.py`:\n    96ÔåÆ  - Loads `data/DimSearchTerm.csv`\n    97ÔåÆ  - Builds dict: `drug_fragment (uppercase) ÔåÆ list[Search_Term]`\n    98ÔåÆ  - Also builds reverse: `search_term ÔåÆ list[drug_fragments]`\n    99ÔåÆ  - CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\")\n   100ÔåÆ- [x] Add function `get_search_terms_for_drug(drug_name, search_term_to_fragments) -> list[str]`:\n   101ÔåÆ  - Returns all Search_Terms whose drug fragments are substrings of the drug name (case-insensitive)\n   102ÔåÆ  - More practical than per-term boolean check ÔÇö returns all matches at once for Phase 2 use\n   103ÔåÆ- [x] Verify: ADALIMUMAB matches \"axial spondyloarthritis\", OMALIZUMAB matches \"asthma\"\n   104ÔåÆ\n   105ÔåÆ---\n   106ÔåÆ\n   107ÔåÆ## Phase 2: Drug-Aware Indication Matching Logic\n   108ÔåÆ\n   109ÔåÆ### 2.1 Create `assign_drug_indications()` function\n   110ÔåÆ- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n   111ÔåÆ  ```\n   112ÔåÆ  def assign_drug_indications(\n   113ÔåÆ      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n   114ÔåÆ      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n   115ÔåÆ      drug_mapping: dict,             # From load_drug_indication_mapping()\n   116ÔåÆ  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n   117ÔåÆ      Returns: (modified_df, indication_df)\n   118ÔåÆ      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n   119ÔåÆ      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n   120ÔåÆ  ```\n   121ÔåÆ- [ ] Logic per UPID + Drug Name pair:\n   122ÔåÆ  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n   123ÔåÆ  2. Get which Search_Terms include this drug (from drug_mapping)\n   124ÔåÆ  3. Intersection = valid indications for this drug-patient pair\n   125ÔåÆ  4. If 1 match: use it\n   126ÔåÆ  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n   127ÔåÆ  6. If 0 matches: use fallback directory\n   128ÔåÆ- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n   129ÔåÆ- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n   130ÔåÆ- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n   131ÔåÆ\n   132ÔåÆ### 2.2 Handle tiebreaker for multiple indication matches\n   133ÔåÆ- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n   134ÔåÆ  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n   135ÔåÆ  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n   136ÔåÆ  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n   137ÔåÆ  - code_frequency is already returned by the updated query in Task 1.1\n   138ÔåÆ- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n   139ÔåÆ- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism\n   140ÔåÆ\n   141ÔåÆ---\n   142ÔåÆ\n   143ÔåÆ## Phase 3: Pipeline Integration\n   144ÔåÆ\n   145ÔåÆ### 3.1 Update `refresh_pathways.py` indication processing\n   146ÔåÆ- [ ] In the `elif current_chart_type == \"indication\":` block:\n   147ÔåÆ  1. Call `get_patient_indication_groups()` as before (but now returns ALL matches)\n   148ÔåÆ  2. Load drug mapping: `drug_mapping = load_drug_indication_mapping()`\n   149ÔåÆ  3. Call `assign_drug_indications(df, gp_matches_df, drug_mapping)`\n   150ÔåÆ  4. Use modified_df (with indication-aware UPIDs) for pathway processing\n   151ÔåÆ  5. Use indication_df for the indication mapping\n   152ÔåÆ- [ ] Pass modified_df (not original df) to `process_indication_pathway_for_date_filter()`\n   153ÔåÆ- [ ] Verify: Pipeline compiles, `python -m py_compile cli/refresh_pathways.py`\n   154ÔåÆ\n   155ÔåÆ### 3.2 Test with dry run\n   156ÔåÆ- [ ] Run `python -m cli.refresh_pathways --chart-type indication --dry-run -v`\n   157ÔåÆ- [ ] Verify:\n   158ÔåÆ  - Modified UPIDs appear in pipeline log (e.g., `RMV12345|rheumatoid arthritis`)\n   159ÔåÆ  - Patient counts are reasonable (will be higher than before since same patient can appear under multiple indications)\n   160ÔåÆ  - Drug-indication matching is logged (match rate, fallback rate)\n   161ÔåÆ  - Pathway hierarchy shows drug-specific grouping under correct indications\n   162ÔåÆ\n   163ÔåÆ---\n   164ÔåÆ\n   165ÔåÆ## Phase 4: Full Refresh & Validation\n   166ÔåÆ\n   167ÔåÆ### 4.1 Full refresh with both chart types\n   168ÔåÆ- [ ] Run `python -m cli.refresh_pathways --chart-type all`\n   169ÔåÆ- [ ] Verify:\n   170ÔåÆ  - Both chart types generate data\n   171ÔåÆ  - Directory charts unchanged (no modified UPIDs)\n   172ÔåÆ  - Indication charts reflect drug-aware matching\n   173ÔåÆ\n   174ÔåÆ### 4.2 Validate indication chart correctness\n   175ÔåÆ- [ ] Check that drugs under an indication all appear in that Search_Term's drug list\n   176ÔåÆ- [ ] Verify that a patient on drugs for different indications creates separate pathway branches\n   177ÔåÆ- [ ] Verify that drugs sharing an indication are grouped in the same pathway\n   178ÔåÆ- [ ] Log: patient count comparison (old vs new approach)\n   179ÔåÆ\n   180ÔåÆ### 4.3 Validate Reflex UI\n   181ÔåÆ- [ ] Run `python -m reflex compile` to verify app compiles\n   182ÔåÆ- [ ] Verify chart type toggle still works\n   183ÔåÆ- [ ] Verify indication chart shows correct hierarchy\n   184ÔåÆ\n   185ÔåÆ---\n   186ÔåÆ\n   187ÔåÆ## Completion Criteria\n   188ÔåÆ\n   189ÔåÆAll tasks marked `[x]` AND:\n   190ÔåÆ- [ ] App compiles without errors (`reflex compile` succeeds)\n   191ÔåÆ- [ ] Both chart types generate pathway data\n   192ÔåÆ- [ ] Indication charts show drug-specific indication matching\n   193ÔåÆ- [ ] Drugs under the same indication for the same patient are in one pathway\n   194ÔåÆ- [ ] Drugs under different indications for the same patient create separate pathways\n   195ÔåÆ- [ ] Fallback works for drugs with no indication match\n   196ÔåÆ- [ ] Full refresh completes successfully\n   197ÔåÆ- [ ] Existing directory charts are unaffected\n   198ÔåÆ\n   199ÔåÆ---\n   200ÔåÆ\n   201ÔåÆ## Reference\n   202ÔåÆ\n   203ÔåÆ### DimSearchTerm.csv Structure\n   204ÔåÆ```\n   205ÔåÆSearch_Term,CleanedDrugName,PrimaryDirectorate\n   206ÔåÆrheumatoid arthritis,ABATACEPT|ADALIMUMAB|ANAKINRA|BARICITINIB|...,RHEUMATOLOGY\n   207ÔåÆasthma,BENRALIZUMAB|DUPILUMAB|INHALED|MEPOLIZUMAB|OMALIZUMAB|RESLIZUMAB,THORACIC MEDICINE\n   208ÔåÆ```\n   209ÔåÆ\n   210ÔåÆ### Modified UPID Format\n   211ÔåÆ```\n   212ÔåÆOriginal:  RMV12345\n   213ÔåÆModified:  RMV12345|rheumatoid arthritis\n   214ÔåÆFallback:  RMV12345|RHEUMATOLOGY (no GP dx)\n   215ÔåÆ```\n   216ÔåÆ\n   217ÔåÆ### Current vs New Indication Flow\n   218ÔåÆ```\n   219ÔåÆCURRENT:\n   220ÔåÆ  Patient ÔåÆ GP dx (most recent) ÔåÆ single Search_Term ÔåÆ one pathway\n   221ÔåÆ\n   222ÔåÆNEW:\n   223ÔåÆ  Patient + Drug A ÔåÆ GP dx matching Drug A ÔåÆ Search_Term X\n   224ÔåÆ  Patient + Drug B ÔåÆ GP dx matching Drug B ÔåÆ Search_Term Y\n   225ÔåÆ  ÔåÆ If X == Y: one pathway under X\n   226ÔåÆ  ÔåÆ If X != Y: two pathways (modified UPIDs)\n   227ÔåÆ```\n   228ÔåÆ\n   229ÔåÆ### Key Files\n   230ÔåÆ\n   231ÔåÆ| File | Changes |\n   232ÔåÆ|------|---------|\n   233ÔåÆ| `data_processing/diagnosis_lookup.py` | Update query, add drug mapping functions |\n   234ÔåÆ| `data_processing/pathway_pipeline.py` | Possibly minor changes for modified UPIDs |\n   235ÔåÆ| `cli/refresh_pathways.py` | Integrate drug-aware matching into pipeline |\n   236ÔåÆ| `data/DimSearchTerm.csv` | Reference data (read-only) |\n   237ÔåÆ| `analysis/pathway_analyzer.py` | No changes expected (UPID changes are transparent) |\n   238ÔåÆ| `pathways_app/pathways_app.py` | No changes expected |\n   239ÔåÆ\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"c578eb42-8f02-4ee6-bef1-b47a03445afb","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\IMPLEMENTATION_PLAN.md","content":"# Implementation Plan - Drug-Aware Indication Matching\n\n## Project Overview\n\nUpdate the indication-based pathway charts so that patient indications are matched **per drug**, not just per patient. Currently, each patient gets ONE indication (most recent GP diagnosis match). This ignores which drugs the patient is actually taking.\n\n### The Problem\n\nA patient on ADALIMUMAB + OMALIZUMAB currently gets assigned a single indication (e.g., \"rheumatoid arthritis\" ÔÇö the most recent GP match). But:\n- ADALIMUMAB is used for rheumatoid arthritis, axial spondyloarthritis, crohn's disease, etc.\n- OMALIZUMAB is used for asthma, allergic asthma, urticaria\n\nThese are different clinical pathways and should be treated as separate treatment journeys.\n\n### The Solution\n\nMatch each drug to an indication by cross-referencing:\n1. **GP diagnosis** ÔÇö which Search_Terms the patient has matching SNOMED codes for\n2. **Drug mapping** ÔÇö which Search_Terms list each drug (from `DimSearchTerm.csv`)\n\nOnly assign a drug to an indication if BOTH conditions are met. If a patient's drugs map to different indications, they become separate pathways (via modified UPID).\n\n### Key Design Decisions\n\n| Aspect | Decision |\n|--------|----------|\n| Drug-indication source | `data/DimSearchTerm.csv` ÔÇö Search_Term ÔåÆ CleanedDrugName mapping |\n| UPID modification | `{original_UPID}\\|{search_term}` for drugs with matched indication |\n| GP diagnosis matching | Return ALL matches per patient (not just most recent) |\n| Drug matching | Substring match: HCD drug name contains DimSearchTerm fragment |\n| Multiple indication matches per drug | Use highest GP code frequency as tiebreaker (COUNT of matching SNOMED codes per Search_Term) |\n| GP code time range | Only codes from MIN(Intervention Date) onwards ÔÇö restricts to HCD data window |\n| No indication match | Fallback to directory (same as current behavior) |\n| Same patient, different indications | Separate pathways via different modified UPIDs |\n\n### Examples\n\n**Patient on ADALIMUMAB + GOLIMUMAB, GP dx: axial spondyloarthritis + asthma**\n- axial spondyloarthritis drug list includes both ADALIMUMAB and GOLIMUMAB\n- ÔåÆ Both drugs grouped under \"axial spondyloarthritis\", single pathway\n- Modified UPID: `RMV12345|axial spondyloarthritis`\n\n**Patient on ADALIMUMAB + OMALIZUMAB, GP dx: axial spondyloarthritis + asthma**\n- axial spondyloarthritis lists ADALIMUMAB but not OMALIZUMAB\n- asthma lists OMALIZUMAB but not ADALIMUMAB\n- ÔåÆ Two separate pathways:\n  - `RMV12345|axial spondyloarthritis` with ADALIMUMAB\n  - `RMV12345|asthma` with OMALIZUMAB\n\n**Patient on ADALIMUMAB, GP dx: rheumatoid arthritis (47 codes) + crohn's disease (2 codes)**\n- Both Search_Terms list ADALIMUMAB AND patient has GP dx for both\n- ÔåÆ Tiebreaker: highest code frequency ÔÇö rheumatoid arthritis has 47 matching SNOMED codes vs 2 for crohn's\n- ÔåÆ Single pathway under rheumatoid arthritis (more clinical activity = more likely the treatment indication)\n\n---\n\n## Phase 1: Update Snowflake Query & Drug Mapping\n\n### 1.1 Update `get_patient_indication_groups()` to return ALL matches with frequency\n- [x] Modify the Snowflake query in `get_patient_indication_groups()` (diagnosis_lookup.py):\n  - Remove `QUALIFY ROW_NUMBER() OVER (PARTITION BY ... ORDER BY EventDateTime DESC) = 1`\n  - Return ALL matching Search_Terms per patient with code frequency:\n    ```sql\n    SELECT pc.\"PatientPseudonym\" AS \"PatientPseudonym\",\n           aic.Search_Term AS \"Search_Term\",\n           COUNT(*) AS \"code_frequency\"\n    FROM PrimaryCareClinicalCoding pc\n    JOIN AllIndicationCodes aic ON pc.\"SNOMEDCode\" = aic.SNOMEDCode\n    WHERE pc.\"PatientPseudonym\" IN (...)\n      AND pc.\"EventDateTime\" >= :earliest_hcd_date\n    GROUP BY pc.\"PatientPseudonym\", aic.Search_Term\n    ```\n  - `code_frequency` = number of matching SNOMED codes per Search_Term per patient\n  - Higher frequency = more clinical activity = stronger signal for tiebreaker\n  - `earliest_hcd_date` = `MIN(Intervention Date)` from the HCD DataFrame ÔÇö restricts GP codes to the HCD data window, reducing noise from old/irrelevant diagnoses\n- [x] Accept `earliest_hcd_date` parameter in `get_patient_indication_groups()` and pass to query\n- [x] Keep batch processing (500 patients per query)\n- [x] Update return type: DataFrame now has multiple rows per patient (PatientPseudonym, Search_Term, code_frequency)\n- [ ] Verify: Query returns more rows than before (patients with multiple matching diagnoses) *(requires live Snowflake ÔÇö will be verified in Phase 3/4)*\n\n### 1.2 Merge related asthma Search_Terms in CLUSTER_MAPPING_SQL\n- [x] In `CLUSTER_MAPPING_SQL` (diagnosis_lookup.py), merge these 3 Search_Terms into one `\"asthma\"` entry:\n  - `allergic asthma` (Cluster: OMALIZUMAB only)\n  - `asthma` (Cluster: BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, OMALIZUMAB, RESLIZUMAB)\n  - `severe persistent allergic asthma` (Cluster: OMALIZUMAB only)\n- [x] Map all 3 Cluster_IDs to `Search_Term = 'asthma'` in the CTE VALUES\n- [x] `urticaria` (OMALIZUMAB, DERMATOLOGY) stays SEPARATE ÔÇö do NOT merge with asthma\n- [x] Also update `load_drug_indication_mapping()` to apply the same merge when loading DimSearchTerm.csv:\n  - Combine drug lists from all 3 entries under a single `\"asthma\"` key\n  - Deduplicate drug fragments (OMALIZUMAB appears in all 3)\n- [x] Verify: GP code lookup returns `\"asthma\"` (not `\"allergic asthma\"` or `\"severe persistent allergic asthma\"`)\n- [x] Verify: Drug mapping for `\"asthma\"` includes full combined drug list: BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, OMALIZUMAB, RESLIZUMAB\n\n### 1.3 Build drug-to-Search_Term lookup from DimSearchTerm.csv\n- [x] Add function `load_drug_indication_mapping()` to `diagnosis_lookup.py`:\n  - Loads `data/DimSearchTerm.csv`\n  - Builds dict: `drug_fragment (uppercase) ÔåÆ list[Search_Term]`\n  - Also builds reverse: `search_term ÔåÆ list[drug_fragments]`\n  - CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\")\n- [x] Add function `get_search_terms_for_drug(drug_name, search_term_to_fragments) -> list[str]`:\n  - Returns all Search_Terms whose drug fragments are substrings of the drug name (case-insensitive)\n  - More practical than per-term boolean check ÔÇö returns all matches at once for Phase 2 use\n- [x] Verify: ADALIMUMAB matches \"axial spondyloarthritis\", OMALIZUMAB matches \"asthma\"\n\n---\n\n## Phase 2: Drug-Aware Indication Matching Logic\n\n### 2.1 Create `assign_drug_indications()` function\n- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n  ```\n  def assign_drug_indications(\n      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n      drug_mapping: dict,             # From load_drug_indication_mapping()\n  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n      Returns: (modified_df, indication_df)\n      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n  ```\n- [ ] Logic per UPID + Drug Name pair:\n  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n  2. Get which Search_Terms include this drug (from drug_mapping)\n  3. Intersection = valid indications for this drug-patient pair\n  4. If 1 match: use it\n  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n  6. If 0 matches: use fallback directory\n- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n\n### 2.2 Handle tiebreaker for multiple indication matches\n- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n  - code_frequency is already returned by the updated query in Task 1.1\n- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism\n\n---\n\n## Phase 3: Pipeline Integration\n\n### 3.1 Update `refresh_pathways.py` indication processing\n- [ ] In the `elif current_chart_type == \"indication\":` block:\n  1. Call `get_patient_indication_groups()` as before (but now returns ALL matches)\n  2. Load drug mapping: `drug_mapping = load_drug_indication_mapping()`\n  3. Call `assign_drug_indications(df, gp_matches_df, drug_mapping)`\n  4. Use modified_df (with indication-aware UPIDs) for pathway processing\n  5. Use indication_df for the indication mapping\n- [ ] Pass modified_df (not original df) to `process_indication_pathway_for_date_filter()`\n- [ ] Verify: Pipeline compiles, `python -m py_compile cli/refresh_pathways.py`\n\n### 3.2 Test with dry run\n- [ ] Run `python -m cli.refresh_pathways --chart-type indication --dry-run -v`\n- [ ] Verify:\n  - Modified UPIDs appear in pipeline log (e.g., `RMV12345|rheumatoid arthritis`)\n  - Patient counts are reasonable (will be higher than before since same patient can appear under multiple indications)\n  - Drug-indication matching is logged (match rate, fallback rate)\n  - Pathway hierarchy shows drug-specific grouping under correct indications\n\n---\n\n## Phase 4: Full Refresh & Validation\n\n### 4.1 Full refresh with both chart types\n- [ ] Run `python -m cli.refresh_pathways --chart-type all`\n- [ ] Verify:\n  - Both chart types generate data\n  - Directory charts unchanged (no modified UPIDs)\n  - Indication charts reflect drug-aware matching\n\n### 4.2 Validate indication chart correctness\n- [ ] Check that drugs under an indication all appear in that Search_Term's drug list\n- [ ] Verify that a patient on drugs for different indications creates separate pathway branches\n- [ ] Verify that drugs sharing an indication are grouped in the same pathway\n- [ ] Log: patient count comparison (old vs new approach)\n\n### 4.3 Validate Reflex UI\n- [ ] Run `python -m reflex compile` to verify app compiles\n- [ ] Verify chart type toggle still works\n- [ ] Verify indication chart shows correct hierarchy\n\n---\n\n## Completion Criteria\n\nAll tasks marked `[x]` AND:\n- [ ] App compiles without errors (`reflex compile` succeeds)\n- [ ] Both chart types generate pathway data\n- [ ] Indication charts show drug-specific indication matching\n- [ ] Drugs under the same indication for the same patient are in one pathway\n- [ ] Drugs under different indications for the same patient create separate pathways\n- [ ] Fallback works for drugs with no indication match\n- [ ] Full refresh completes successfully\n- [ ] Existing directory charts are unaffected\n\n---\n\n## Reference\n\n### DimSearchTerm.csv Structure\n```\nSearch_Term,CleanedDrugName,PrimaryDirectorate\nrheumatoid arthritis,ABATACEPT|ADALIMUMAB|ANAKINRA|BARICITINIB|...,RHEUMATOLOGY\nasthma,BENRALIZUMAB|DUPILUMAB|INHALED|MEPOLIZUMAB|OMALIZUMAB|RESLIZUMAB,THORACIC MEDICINE\n```\n\n### Modified UPID Format\n```\nOriginal:  RMV12345\nModified:  RMV12345|rheumatoid arthritis\nFallback:  RMV12345|RHEUMATOLOGY (no GP dx)\n```\n\n### Current vs New Indication Flow\n```\nCURRENT:\n  Patient ÔåÆ GP dx (most recent) ÔåÆ single Search_Term ÔåÆ one pathway\n\nNEW:\n  Patient + Drug A ÔåÆ GP dx matching Drug A ÔåÆ Search_Term X\n  Patient + Drug B ÔåÆ GP dx matching Drug B ÔåÆ Search_Term Y\n  ÔåÆ If X == Y: one pathway under X\n  ÔåÆ If X != Y: two pathways (modified UPIDs)\n```\n\n### Key Files\n\n| File | Changes |\n|------|---------|\n| `data_processing/diagnosis_lookup.py` | Update query, add drug mapping functions |\n| `data_processing/pathway_pipeline.py` | Possibly minor changes for modified UPIDs |\n| `cli/refresh_pathways.py` | Integrate drug-aware matching into pipeline |\n| `data/DimSearchTerm.csv` | Reference data (read-only) |\n| `analysis/pathway_analyzer.py` | No changes expected (UPID changes are transparent) |\n| `pathways_app/pathways_app.py` | No changes expected |\n","numLines":239,"startLine":1,"totalLines":239}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Kg3naBhMrg2SP4UXU7JM8q","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EDTENPSwPVDUcHXJsZypQK","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\guardrails.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":42290,"cache_read_input_tokens":27385,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":42290},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"408c621d-224b-4b7d-a2bb-f594c94db2d2"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EDTENPSwPVDUcHXJsZypQK","type":"tool_result","content":"     1ÔåÆ# Guardrails\n     2ÔåÆ\n     3ÔåÆKnown failure patterns. Read EVERY iteration. Follow ALL of these rules.\n     4ÔåÆIf you discover a new failure pattern during your work, add it to this file.\n     5ÔåÆ\n     6ÔåÆ---\n     7ÔåÆ\n     8ÔåÆ## Drug-Indication Matching Guardrails\n     9ÔåÆ\n    10ÔåÆ### Match drugs to indications, not just patients to indications\n    11ÔåÆ- **When**: Building the indication mapping for pathway charts\n    12ÔåÆ- **Rule**: Each drug must be validated against BOTH the patient's GP diagnoses AND the drug-to-indication mapping from DimSearchTerm.csv. A patient being diagnosed with rheumatoid arthritis does NOT mean all their drugs are for rheumatoid arthritis.\n    13ÔåÆ- **Why**: The previous approach assigned ONE indication per patient (most recent GP dx), ignoring which drugs actually treat which conditions. This produced misleading pathways.\n    14ÔåÆ\n    15ÔåÆ### Use DimSearchTerm.csv for drug-to-Search_Term mapping\n    16ÔåÆ- **When**: Determining which Search_Term a drug belongs to\n    17ÔåÆ- **Rule**: Load `data/DimSearchTerm.csv`. The `CleanedDrugName` column has pipe-separated drug name fragments. Match HCD drug names against these fragments using substring matching (case-insensitive).\n    18ÔåÆ- **Why**: This CSV is the authoritative mapping of which drugs are used for which clinical indications.\n    19ÔåÆ\n    20ÔåÆ### Use substring matching for drug fragments\n    21ÔåÆ- **When**: Matching HCD drug names against DimSearchTerm CleanedDrugName fragments\n    22ÔåÆ- **Rule**: Check if any fragment from DimSearchTerm is a SUBSTRING of the HCD drug name (case-insensitive). E.g., \"PEGYLATED\" should match \"PEGYLATED LIPOSOMAL DOXORUBICIN\".\n    23ÔåÆ- **Why**: DimSearchTerm contains both full drug names (ADALIMUMAB) and partial fragments (PEGYLATED, INHALED). Exact match would miss the partial ones.\n    24ÔåÆ\n    25ÔåÆ### Modified UPID uses pipe delimiter\n    26ÔåÆ- **When**: Creating indication-aware UPIDs\n    27ÔåÆ- **Rule**: Format is `{original_UPID}|{search_term}`. Use pipe `|` as delimiter. Do NOT use ` - ` (hyphen with spaces) as that's used for pathway hierarchy levels in the `ids` column.\n    28ÔåÆ- **Why**: The `ids` column uses \" - \" to separate hierarchy levels (e.g., \"N&WICS - NNUH - rheumatoid arthritis - ADALIMUMAB\"). Using the same delimiter in UPIDs would break hierarchy parsing.\n    29ÔåÆ\n    30ÔåÆ### Return ALL GP matches per patient, not just most recent\n    31ÔåÆ- **When**: Querying Snowflake for patient GP diagnoses\n    32ÔåÆ- **Rule**: Remove `QUALIFY ROW_NUMBER() OVER (PARTITION BY ... ORDER BY EventDateTime DESC) = 1`. Return ALL matching Search_Terms per patient with `GROUP BY + COUNT(*)` for code_frequency.\n    33ÔåÆ- **Why**: A patient may have GP diagnoses for both rheumatoid arthritis AND asthma. We need ALL matches to cross-reference with their drugs.\n    34ÔåÆ\n    35ÔåÆ### Restrict GP code lookup to HCD data window\n    36ÔåÆ- **When**: Building the WHERE clause for the GP record query\n    37ÔåÆ- **Rule**: Add `AND pc.\"EventDateTime\" >= :earliest_hcd_date` where `earliest_hcd_date` is `MIN(Intervention Date)` from the HCD DataFrame. Pass this as a parameter to `get_patient_indication_groups()`.\n    38ÔåÆ- **Why**: Old GP codes from years before treatment started add noise. A diagnosis coded 10 years ago may no longer be relevant. Restricting to the HCD window ensures code_frequency reflects recent clinical activity for the conditions being actively treated.\n    39ÔåÆ\n    40ÔåÆ### Tiebreaker: highest GP code frequency when a drug matches multiple indications\n    41ÔåÆ- **When**: A single drug maps to multiple Search_Terms AND the patient has GP dx for multiple\n    42ÔåÆ- **Rule**: Use `code_frequency` (COUNT of matching SNOMED codes per Search_Term per patient) from the GP query. The Search_Term with the most matching codes in the patient's GP record wins. If tied, use alphabetical Search_Term for determinism.\n    43ÔåÆ- **Why**: E.g., ADALIMUMAB is listed under rheumatoid arthritis, crohn's disease, psoriatic arthritis, etc. A patient with 47 RA codes and 2 crohn's codes is almost certainly on ADALIMUMAB for RA. Frequency of GP coding is a much stronger signal of clinical intent than recency ÔÇö a recent one-off asthma check doesn't mean ADALIMUMAB is for asthma.\n    44ÔåÆ\n    45ÔåÆ### Same patient, different indications = separate modified UPIDs\n    46ÔåÆ- **When**: A patient's drugs map to different Search_Terms\n    47ÔåÆ- **Rule**: Create separate modified UPIDs for each indication. E.g., `RMV12345|rheumatoid arthritis` and `RMV12345|asthma`. These are treated as separate \"patients\" by the pathway analyzer.\n    48ÔåÆ- **Why**: This is the core design ÔÇö drugs for different indications should create separate treatment pathways, even for the same physical patient.\n    49ÔåÆ\n    50ÔåÆ### Fallback to directory for unmatched drugs\n    51ÔåÆ- **When**: A drug doesn't match any Search_Term OR the patient has no GP dx for any of the drug's Search_Terms\n    52ÔåÆ- **Rule**: Use fallback format: `{UPID}|{Directory} (no GP dx)`. The indication_df maps this to `\"{Directory} (no GP dx)\"`.\n    53ÔåÆ- **Why**: Maintains consistent behavior with the previous approach for patients/drugs without GP diagnosis matches.\n    54ÔåÆ\n    55ÔåÆ### Merge asthma Search_Terms but keep urticaria separate\n    56ÔåÆ- **When**: Working with asthma-related Search_Terms from CLUSTER_MAPPING_SQL or DimSearchTerm.csv\n    57ÔåÆ- **Rule**: Merge \"allergic asthma\", \"asthma\", and \"severe persistent allergic asthma\" into a single \"asthma\" Search_Term. Keep \"urticaria\" as a separate Search_Term ÔÇö do NOT merge it with asthma.\n    58ÔåÆ- **Why**: These are clinically the same condition at different severity levels. Splitting them fragments the data. Urticaria is a distinct dermatological condition that happens to share OMALIZUMAB.\n    59ÔåÆ\n    60ÔåÆ### Don't modify directory chart processing\n    61ÔåÆ- **When**: Making changes to the indication matching logic\n    62ÔåÆ- **Rule**: Only modify the indication chart path (`elif current_chart_type == \"indication\":`). Directory charts use unmodified UPIDs and directory-based grouping.\n    63ÔåÆ- **Why**: Directory charts work correctly and should not be affected by indication matching changes.\n    64ÔåÆ\n    65ÔåÆ---\n    66ÔåÆ\n    67ÔåÆ## Snowflake Query Guardrails\n    68ÔåÆ\n    69ÔåÆ### Use PseudoNHSNoLinked for GP record matching\n    70ÔåÆ- **When**: Querying GP records (PrimaryCareClinicalCoding) for patient diagnoses\n    71ÔåÆ- **Rule**: Use `PseudoNHSNoLinked` column from HCD data, NOT `PersonKey` (LocalPatientID)\n    72ÔåÆ- **Why**: PersonKey is provider-specific local ID. Only PseudoNHSNoLinked matches PatientPseudonym in GP records.\n    73ÔåÆ\n    74ÔåÆ### Embed cluster query as CTE in Snowflake\n    75ÔåÆ- **When**: Looking up patient indications during data refresh\n    76ÔåÆ- **Rule**: Use the `CLUSTER_MAPPING_SQL` content as a WITH clause in the patient lookup query\n    77ÔåÆ- **Why**: This ensures we always use the complete cluster mapping and don't need local storage\n    78ÔåÆ\n    79ÔåÆ### Quote mixed-case column aliases in Snowflake SQL\n    80ÔåÆ- **When**: Writing SELECT queries that return results to Python code\n    81ÔåÆ- **Rule**: Use `AS \"ColumnName\"` (quoted) for any column alias you'll access by name in Python\n    82ÔåÆ- **Why**: Snowflake uppercases unquoted identifiers. `SELECT foo AS Search_Term` returns `SEARCH_TERM`, so `row.get('Search_Term')` returns None. Fix: `SELECT foo AS \"Search_Term\"`\n    83ÔåÆ\n    84ÔåÆ### Build indication_df from all unique UPIDs, not PseudoNHSNoLinked\n    85ÔåÆ- **When**: Creating the indication mapping DataFrame for pathway processing\n    86ÔåÆ- **Rule**: Use `df.drop_duplicates(subset=['UPID'])` not `drop_duplicates(subset=['PseudoNHSNoLinked'])`\n    87ÔåÆ- **Why**: A patient visiting multiple providers has multiple UPIDs. Using unique PseudoNHSNoLinked only maps one UPID per patient, leaving others as NaN.\n    88ÔåÆ\n    89ÔåÆ---\n    90ÔåÆ\n    91ÔåÆ## Data Processing Guardrails\n    92ÔåÆ\n    93ÔåÆ### Copy DataFrames in functions that modify columns\n    94ÔåÆ- **When**: Writing functions like `prepare_data()` that modify DataFrame columns\n    95ÔåÆ- **Rule**: Always `df = df.copy()` at the start of any function that modifies column values on the input DataFrame\n    96ÔåÆ- **Why**: `prepare_data()` mapped Provider Code ÔåÆ Name in-place. When called multiple times on the same DataFrame, only the first call worked. The fix: `df.copy()` prevents destructive mutation.\n    97ÔåÆ\n    98ÔåÆ### Include chart_type in UNIQUE constraints for pathway_nodes\n    99ÔåÆ- **When**: Creating or modifying the pathway_nodes table schema\n   100ÔåÆ- **Rule**: The UNIQUE constraint MUST include `chart_type`: `UNIQUE(date_filter_id, chart_type, ids)`\n   101ÔåÆ- **Why**: Without `chart_type`, `INSERT OR REPLACE` silently overwrites directory chart nodes when indication chart nodes are inserted.\n   102ÔåÆ\n   103ÔåÆ### Handle NaN in Directory when building fallback labels\n   104ÔåÆ- **When**: Creating fallback indication labels for patients without GP diagnosis match\n   105ÔåÆ- **Rule**: Check `pd.notna(directory)` before concatenating to string. Use `\"UNKNOWN (no GP dx)\"` for NaN cases.\n   106ÔåÆ- **Why**: NaN handling prevents TypeError and ensures meaningful fallback labels.\n   107ÔåÆ\n   108ÔåÆ### Use parameterized queries for SQLite\n   109ÔåÆ- **When**: Building WHERE clauses with user-selected filters\n   110ÔåÆ- **Rule**: Use `?` placeholders and pass params tuple ÔÇö never string interpolation\n   111ÔåÆ- **Why**: Prevents SQL injection and handles special characters in drug/directory names\n   112ÔåÆ\n   113ÔåÆ### Use existing pathway_analyzer functions\n   114ÔåÆ- **When**: Processing pathway data for the icicle chart\n   115ÔåÆ- **Rule**: Reuse functions from `analysis/pathway_analyzer.py` ÔÇö don't reinvent\n   116ÔåÆ- **Why**: The existing code handles edge cases (empty groups, statistics calculation, color mapping)\n   117ÔåÆ\n   118ÔåÆ---\n   119ÔåÆ\n   120ÔåÆ## Reflex Guardrails\n   121ÔåÆ\n   122ÔåÆ### Use .to() methods for Var operations in rx.foreach\n   123ÔåÆ- **When**: Working with items inside `rx.foreach` render functions\n   124ÔåÆ- **Rule**: Use `item.to(int)` for numeric comparisons, `item.to_string()` for text operations\n   125ÔåÆ- **Why**: Items from rx.foreach are Var objects, not plain Python values.\n   126ÔåÆ\n   127ÔåÆ### Use rx.cond for conditional rendering, not Python if\n   128ÔåÆ- **When**: Conditionally showing/hiding components or changing styles based on state\n   129ÔåÆ- **Rule**: Use `rx.cond(condition, true_component, false_component)` ÔÇö not Python `if`\n   130ÔåÆ- **Why**: Python `if` evaluates at definition time; `rx.cond` evaluates reactively at render time\n   131ÔåÆ\n   132ÔåÆ---\n   133ÔåÆ\n   134ÔåÆ## Process Guardrails\n   135ÔåÆ\n   136ÔåÆ### One task per iteration\n   137ÔåÆ- **When**: Temptation to do additional tasks after completing the current one\n   138ÔåÆ- **Rule**: Complete ONE task, validate it, commit it, update progress, then stop\n   139ÔåÆ- **Why**: Multiple tasks increase error risk and make failures harder to diagnose\n   140ÔåÆ\n   141ÔåÆ### Never mark complete without validation\n   142ÔåÆ- **When**: Task feels \"done\" but hasn't been tested\n   143ÔåÆ- **Rule**: All validation tiers must pass before marking `[x]`\n   144ÔåÆ- **Why**: \"Feels done\" is not \"is done\"\n   145ÔåÆ\n   146ÔåÆ### Write explicit handoff notes\n   147ÔåÆ- **When**: Every iteration, before stopping\n   148ÔåÆ- **Rule**: The \"Next iteration should\" section must contain specific, actionable guidance\n   149ÔåÆ- **Why**: The next iteration has zero memory. If you don't write it down, it's lost.\n   150ÔåÆ\n   151ÔåÆ### Check existing code for patterns\n   152ÔåÆ- **When**: Unsure how to implement something\n   153ÔåÆ- **Rule**: Look at `pathways_app/pathways_app.py`, `analysis/pathway_analyzer.py`, `cli/refresh_pathways.py`\n   154ÔåÆ- **Why**: The existing codebase has solved many quirks already\n   155ÔåÆ\n   156ÔåÆ<!--\n   157ÔåÆADD NEW GUARDRAILS BELOW as failures are observed during the loop.\n   158ÔåÆ\n   159ÔåÆFormat:\n   160ÔåÆ### [Short descriptive name]\n   161ÔåÆ- **When**: What situation triggers this guardrail?\n   162ÔåÆ- **Rule**: What must you do (or not do)?\n   163ÔåÆ- **Why**: What failure prompted adding this guardrail?\n   164ÔåÆ-->\n   165ÔåÆ\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"e82516bf-019c-4180-9a5e-a4f0340dbfe7","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\guardrails.md","content":"# Guardrails\n\nKnown failure patterns. Read EVERY iteration. Follow ALL of these rules.\nIf you discover a new failure pattern during your work, add it to this file.\n\n---\n\n## Drug-Indication Matching Guardrails\n\n### Match drugs to indications, not just patients to indications\n- **When**: Building the indication mapping for pathway charts\n- **Rule**: Each drug must be validated against BOTH the patient's GP diagnoses AND the drug-to-indication mapping from DimSearchTerm.csv. A patient being diagnosed with rheumatoid arthritis does NOT mean all their drugs are for rheumatoid arthritis.\n- **Why**: The previous approach assigned ONE indication per patient (most recent GP dx), ignoring which drugs actually treat which conditions. This produced misleading pathways.\n\n### Use DimSearchTerm.csv for drug-to-Search_Term mapping\n- **When**: Determining which Search_Term a drug belongs to\n- **Rule**: Load `data/DimSearchTerm.csv`. The `CleanedDrugName` column has pipe-separated drug name fragments. Match HCD drug names against these fragments using substring matching (case-insensitive).\n- **Why**: This CSV is the authoritative mapping of which drugs are used for which clinical indications.\n\n### Use substring matching for drug fragments\n- **When**: Matching HCD drug names against DimSearchTerm CleanedDrugName fragments\n- **Rule**: Check if any fragment from DimSearchTerm is a SUBSTRING of the HCD drug name (case-insensitive). E.g., \"PEGYLATED\" should match \"PEGYLATED LIPOSOMAL DOXORUBICIN\".\n- **Why**: DimSearchTerm contains both full drug names (ADALIMUMAB) and partial fragments (PEGYLATED, INHALED). Exact match would miss the partial ones.\n\n### Modified UPID uses pipe delimiter\n- **When**: Creating indication-aware UPIDs\n- **Rule**: Format is `{original_UPID}|{search_term}`. Use pipe `|` as delimiter. Do NOT use ` - ` (hyphen with spaces) as that's used for pathway hierarchy levels in the `ids` column.\n- **Why**: The `ids` column uses \" - \" to separate hierarchy levels (e.g., \"N&WICS - NNUH - rheumatoid arthritis - ADALIMUMAB\"). Using the same delimiter in UPIDs would break hierarchy parsing.\n\n### Return ALL GP matches per patient, not just most recent\n- **When**: Querying Snowflake for patient GP diagnoses\n- **Rule**: Remove `QUALIFY ROW_NUMBER() OVER (PARTITION BY ... ORDER BY EventDateTime DESC) = 1`. Return ALL matching Search_Terms per patient with `GROUP BY + COUNT(*)` for code_frequency.\n- **Why**: A patient may have GP diagnoses for both rheumatoid arthritis AND asthma. We need ALL matches to cross-reference with their drugs.\n\n### Restrict GP code lookup to HCD data window\n- **When**: Building the WHERE clause for the GP record query\n- **Rule**: Add `AND pc.\"EventDateTime\" >= :earliest_hcd_date` where `earliest_hcd_date` is `MIN(Intervention Date)` from the HCD DataFrame. Pass this as a parameter to `get_patient_indication_groups()`.\n- **Why**: Old GP codes from years before treatment started add noise. A diagnosis coded 10 years ago may no longer be relevant. Restricting to the HCD window ensures code_frequency reflects recent clinical activity for the conditions being actively treated.\n\n### Tiebreaker: highest GP code frequency when a drug matches multiple indications\n- **When**: A single drug maps to multiple Search_Terms AND the patient has GP dx for multiple\n- **Rule**: Use `code_frequency` (COUNT of matching SNOMED codes per Search_Term per patient) from the GP query. The Search_Term with the most matching codes in the patient's GP record wins. If tied, use alphabetical Search_Term for determinism.\n- **Why**: E.g., ADALIMUMAB is listed under rheumatoid arthritis, crohn's disease, psoriatic arthritis, etc. A patient with 47 RA codes and 2 crohn's codes is almost certainly on ADALIMUMAB for RA. Frequency of GP coding is a much stronger signal of clinical intent than recency ÔÇö a recent one-off asthma check doesn't mean ADALIMUMAB is for asthma.\n\n### Same patient, different indications = separate modified UPIDs\n- **When**: A patient's drugs map to different Search_Terms\n- **Rule**: Create separate modified UPIDs for each indication. E.g., `RMV12345|rheumatoid arthritis` and `RMV12345|asthma`. These are treated as separate \"patients\" by the pathway analyzer.\n- **Why**: This is the core design ÔÇö drugs for different indications should create separate treatment pathways, even for the same physical patient.\n\n### Fallback to directory for unmatched drugs\n- **When**: A drug doesn't match any Search_Term OR the patient has no GP dx for any of the drug's Search_Terms\n- **Rule**: Use fallback format: `{UPID}|{Directory} (no GP dx)`. The indication_df maps this to `\"{Directory} (no GP dx)\"`.\n- **Why**: Maintains consistent behavior with the previous approach for patients/drugs without GP diagnosis matches.\n\n### Merge asthma Search_Terms but keep urticaria separate\n- **When**: Working with asthma-related Search_Terms from CLUSTER_MAPPING_SQL or DimSearchTerm.csv\n- **Rule**: Merge \"allergic asthma\", \"asthma\", and \"severe persistent allergic asthma\" into a single \"asthma\" Search_Term. Keep \"urticaria\" as a separate Search_Term ÔÇö do NOT merge it with asthma.\n- **Why**: These are clinically the same condition at different severity levels. Splitting them fragments the data. Urticaria is a distinct dermatological condition that happens to share OMALIZUMAB.\n\n### Don't modify directory chart processing\n- **When**: Making changes to the indication matching logic\n- **Rule**: Only modify the indication chart path (`elif current_chart_type == \"indication\":`). Directory charts use unmodified UPIDs and directory-based grouping.\n- **Why**: Directory charts work correctly and should not be affected by indication matching changes.\n\n---\n\n## Snowflake Query Guardrails\n\n### Use PseudoNHSNoLinked for GP record matching\n- **When**: Querying GP records (PrimaryCareClinicalCoding) for patient diagnoses\n- **Rule**: Use `PseudoNHSNoLinked` column from HCD data, NOT `PersonKey` (LocalPatientID)\n- **Why**: PersonKey is provider-specific local ID. Only PseudoNHSNoLinked matches PatientPseudonym in GP records.\n\n### Embed cluster query as CTE in Snowflake\n- **When**: Looking up patient indications during data refresh\n- **Rule**: Use the `CLUSTER_MAPPING_SQL` content as a WITH clause in the patient lookup query\n- **Why**: This ensures we always use the complete cluster mapping and don't need local storage\n\n### Quote mixed-case column aliases in Snowflake SQL\n- **When**: Writing SELECT queries that return results to Python code\n- **Rule**: Use `AS \"ColumnName\"` (quoted) for any column alias you'll access by name in Python\n- **Why**: Snowflake uppercases unquoted identifiers. `SELECT foo AS Search_Term` returns `SEARCH_TERM`, so `row.get('Search_Term')` returns None. Fix: `SELECT foo AS \"Search_Term\"`\n\n### Build indication_df from all unique UPIDs, not PseudoNHSNoLinked\n- **When**: Creating the indication mapping DataFrame for pathway processing\n- **Rule**: Use `df.drop_duplicates(subset=['UPID'])` not `drop_duplicates(subset=['PseudoNHSNoLinked'])`\n- **Why**: A patient visiting multiple providers has multiple UPIDs. Using unique PseudoNHSNoLinked only maps one UPID per patient, leaving others as NaN.\n\n---\n\n## Data Processing Guardrails\n\n### Copy DataFrames in functions that modify columns\n- **When**: Writing functions like `prepare_data()` that modify DataFrame columns\n- **Rule**: Always `df = df.copy()` at the start of any function that modifies column values on the input DataFrame\n- **Why**: `prepare_data()` mapped Provider Code ÔåÆ Name in-place. When called multiple times on the same DataFrame, only the first call worked. The fix: `df.copy()` prevents destructive mutation.\n\n### Include chart_type in UNIQUE constraints for pathway_nodes\n- **When**: Creating or modifying the pathway_nodes table schema\n- **Rule**: The UNIQUE constraint MUST include `chart_type`: `UNIQUE(date_filter_id, chart_type, ids)`\n- **Why**: Without `chart_type`, `INSERT OR REPLACE` silently overwrites directory chart nodes when indication chart nodes are inserted.\n\n### Handle NaN in Directory when building fallback labels\n- **When**: Creating fallback indication labels for patients without GP diagnosis match\n- **Rule**: Check `pd.notna(directory)` before concatenating to string. Use `\"UNKNOWN (no GP dx)\"` for NaN cases.\n- **Why**: NaN handling prevents TypeError and ensures meaningful fallback labels.\n\n### Use parameterized queries for SQLite\n- **When**: Building WHERE clauses with user-selected filters\n- **Rule**: Use `?` placeholders and pass params tuple ÔÇö never string interpolation\n- **Why**: Prevents SQL injection and handles special characters in drug/directory names\n\n### Use existing pathway_analyzer functions\n- **When**: Processing pathway data for the icicle chart\n- **Rule**: Reuse functions from `analysis/pathway_analyzer.py` ÔÇö don't reinvent\n- **Why**: The existing code handles edge cases (empty groups, statistics calculation, color mapping)\n\n---\n\n## Reflex Guardrails\n\n### Use .to() methods for Var operations in rx.foreach\n- **When**: Working with items inside `rx.foreach` render functions\n- **Rule**: Use `item.to(int)` for numeric comparisons, `item.to_string()` for text operations\n- **Why**: Items from rx.foreach are Var objects, not plain Python values.\n\n### Use rx.cond for conditional rendering, not Python if\n- **When**: Conditionally showing/hiding components or changing styles based on state\n- **Rule**: Use `rx.cond(condition, true_component, false_component)` ÔÇö not Python `if`\n- **Why**: Python `if` evaluates at definition time; `rx.cond` evaluates reactively at render time\n\n---\n\n## Process Guardrails\n\n### One task per iteration\n- **When**: Temptation to do additional tasks after completing the current one\n- **Rule**: Complete ONE task, validate it, commit it, update progress, then stop\n- **Why**: Multiple tasks increase error risk and make failures harder to diagnose\n\n### Never mark complete without validation\n- **When**: Task feels \"done\" but hasn't been tested\n- **Rule**: All validation tiers must pass before marking `[x]`\n- **Why**: \"Feels done\" is not \"is done\"\n\n### Write explicit handoff notes\n- **When**: Every iteration, before stopping\n- **Rule**: The \"Next iteration should\" section must contain specific, actionable guidance\n- **Why**: The next iteration has zero memory. If you don't write it down, it's lost.\n\n### Check existing code for patterns\n- **When**: Unsure how to implement something\n- **Rule**: Look at `pathways_app/pathways_app.py`, `analysis/pathway_analyzer.py`, `cli/refresh_pathways.py`\n- **Why**: The existing codebase has solved many quirks already\n\n<!--\nADD NEW GUARDRAILS BELOW as failures are observed during the loop.\n\nFormat:\n### [Short descriptive name]\n- **When**: What situation triggers this guardrail?\n- **Rule**: What must you do (or not do)?\n- **Why**: What failure prompted adding this guardrail?\n-->\n","numLines":165,"startLine":1,"totalLines":165}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Q9w8T9sGf2TQJv2TqA5yEh","type":"message","role":"assistant","content":[{"type":"text","text":"Now checking recent commits and reading the current state of the key file I'll be modifying."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":13169,"cache_read_input_tokens":69675,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":13169},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"2ac940f0-d1c0-4321-b4bf-ec34b9aa6589"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Q9w8T9sGf2TQJv2TqA5yEh","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Ki3HGrDfkiiqwLWBCFdn6f","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && git log --oneline -5","description":"Show recent git commits"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":13169,"cache_read_input_tokens":69675,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":13169},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"31754a92-433c-452e-aa05-3c28d3982216"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Q9w8T9sGf2TQJv2TqA5yEh","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CkSRDKo4jEBABQejbBaVJT","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":13169,"cache_read_input_tokens":69675,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":13169},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f783b4b1-c868-48de-81b0-ae117cb040a7"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Ki3HGrDfkiiqwLWBCFdn6f","type":"tool_result","content":"947b87a docs: update progress.txt with Iteration 3 results (Task 1.1)\nc93417f feat: return ALL GP matches with code_frequency in get_patient_indication_groups (Task 1.1)\n4fed0e5 docs: update progress.txt with Iteration 2 results (Task 1.2)\nb0a8a9d feat: merge asthma Search_Term variants in CLUSTER_MAPPING_SQL and drug mapping (Task 1.2)\nc85aae4 docs: update progress.txt with Iteration 1 results (Task 1.2)","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"4b774ed6-446e-4ca0-936d-45ab21c29331","tool_use_result":{"stdout":"947b87a docs: update progress.txt with Iteration 3 results (Task 1.1)\nc93417f feat: return ALL GP matches with code_frequency in get_patient_indication_groups (Task 1.1)\n4fed0e5 docs: update progress.txt with Iteration 2 results (Task 1.2)\nb0a8a9d feat: merge asthma Search_Term variants in CLUSTER_MAPPING_SQL and drug mapping (Task 1.2)\nc85aae4 docs: update progress.txt with Iteration 1 results (Task 1.2)\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CkSRDKo4jEBABQejbBaVJT","type":"tool_result","content":"     1ÔåÆ\"\"\"\n     2ÔåÆDiagnosis lookup module for NHS Patient Pathway Analysis.\n     3ÔåÆ\n     4ÔåÆProvides functions to validate patient indications by checking GP diagnosis records\n     5ÔåÆagainst SNOMED cluster codes. Uses the drug-to-cluster mapping from\n     6ÔåÆdrug_indication_clusters.csv and queries Snowflake for SNOMED codes and GP records.\n     7ÔåÆ\n     8ÔåÆKey workflow:\n     9ÔåÆ1. Get drug's valid indication clusters from local mapping\n    10ÔåÆ2. Get all SNOMED codes for those clusters from Snowflake\n    11ÔåÆ3. Check if patient has any of those SNOMED codes in GP records\n    12ÔåÆ4. Report indication validation status\n    13ÔåÆ\n    14ÔåÆIMPORTANT: HCD activity data indication codes are UNRELIABLE. This module uses\n    15ÔåÆGP/Primary Care data (PrimaryCareClinicalCoding) as the authoritative source.\n    16ÔåÆ\"\"\"\n    17ÔåÆ\n    18ÔåÆfrom dataclasses import dataclass, field\n    19ÔåÆfrom datetime import date, datetime\n    20ÔåÆfrom pathlib import Path\n    21ÔåÆfrom typing import Optional, Callable, Any, cast, TYPE_CHECKING\n    22ÔåÆimport csv\n    23ÔåÆ\n    24ÔåÆif TYPE_CHECKING:\n    25ÔåÆ    import pandas as pd\n    26ÔåÆ\n    27ÔåÆfrom core.logging_config import get_logger\n    28ÔåÆfrom data_processing.database import DatabaseManager, default_db_manager\n    29ÔåÆfrom data_processing.snowflake_connector import (\n    30ÔåÆ    SnowflakeConnector,\n    31ÔåÆ    get_connector,\n    32ÔåÆ    is_snowflake_available,\n    33ÔåÆ    is_snowflake_configured,\n    34ÔåÆ    SNOWFLAKE_AVAILABLE,\n    35ÔåÆ)\n    36ÔåÆfrom data_processing.cache import get_cache, is_cache_enabled\n    37ÔåÆ\n    38ÔåÆlogger = get_logger(__name__)\n    39ÔåÆ\n    40ÔåÆ\n    41ÔåÆ@dataclass\n    42ÔåÆclass ClusterSnomedCodes:\n    43ÔåÆ    \"\"\"SNOMED codes for a clinical coding cluster.\"\"\"\n    44ÔåÆ    cluster_id: str\n    45ÔåÆ    cluster_description: str\n    46ÔåÆ    snomed_codes: list[str] = field(default_factory=list)\n    47ÔåÆ    snomed_descriptions: dict[str, str] = field(default_factory=dict)\n    48ÔåÆ\n    49ÔåÆ    @property\n    50ÔåÆ    def code_count(self) -> int:\n    51ÔåÆ        return len(self.snomed_codes)\n    52ÔåÆ\n    53ÔåÆ\n    54ÔåÆ@dataclass\n    55ÔåÆclass IndicationValidationResult:\n    56ÔåÆ    \"\"\"Result of validating a patient's indication for a drug.\"\"\"\n    57ÔåÆ    patient_pseudonym: str\n    58ÔåÆ    drug_name: str\n    59ÔåÆ    has_valid_indication: bool\n    60ÔåÆ    matched_cluster_id: Optional[str] = None\n    61ÔåÆ    matched_snomed_code: Optional[str] = None\n    62ÔåÆ    matched_snomed_description: Optional[str] = None\n    63ÔåÆ    checked_clusters: list[str] = field(default_factory=list)\n    64ÔåÆ    total_codes_checked: int = 0\n    65ÔåÆ    source: str = \"GP_SNOMED\"  # GP_SNOMED | NONE\n    66ÔåÆ    error_message: Optional[str] = None\n    67ÔåÆ\n    68ÔåÆ\n    69ÔåÆ@dataclass\n    70ÔåÆclass DrugIndicationMatchRate:\n    71ÔåÆ    \"\"\"Match rate statistics for a drug's indication validation.\"\"\"\n    72ÔåÆ    drug_name: str\n    73ÔåÆ    total_patients: int\n    74ÔåÆ    patients_with_indication: int\n    75ÔåÆ    patients_without_indication: int\n    76ÔåÆ    match_rate: float  # 0.0 to 1.0\n    77ÔåÆ    clusters_checked: list[str] = field(default_factory=list)\n    78ÔåÆ    sample_unmatched: list[str] = field(default_factory=list)  # Sample patient IDs\n    79ÔåÆ\n    80ÔåÆ\n    81ÔåÆ@dataclass\n    82ÔåÆclass DrugSnomedMapping:\n    83ÔåÆ    \"\"\"SNOMED code mapping for a drug from ref_drug_snomed_mapping.\"\"\"\n    84ÔåÆ    snomed_code: str\n    85ÔåÆ    snomed_description: str\n    86ÔåÆ    search_term: str\n    87ÔåÆ    primary_directorate: str\n    88ÔåÆ    indication: str = \"\"\n    89ÔåÆ    ta_id: str = \"\"\n    90ÔåÆ\n    91ÔåÆ\n    92ÔåÆ@dataclass\n    93ÔåÆclass DirectSnomedMatchResult:\n    94ÔåÆ    \"\"\"Result of direct SNOMED code lookup in GP records.\"\"\"\n    95ÔåÆ    patient_pseudonym: str\n    96ÔåÆ    matched: bool\n    97ÔåÆ    snomed_code: Optional[str] = None\n    98ÔåÆ    snomed_description: Optional[str] = None\n    99ÔåÆ    search_term: Optional[str] = None\n   100ÔåÆ    primary_directorate: Optional[str] = None\n   101ÔåÆ    event_date: Optional[datetime] = None\n   102ÔåÆ    source: str = \"DIRECT_SNOMED\"  # DIRECT_SNOMED | NONE\n   103ÔåÆ\n   104ÔåÆ\n   105ÔåÆ@dataclass\n   106ÔåÆclass DirectorateAssignment:\n   107ÔåÆ    \"\"\"Result of directorate assignment for a patient-drug combination.\"\"\"\n   108ÔåÆ    upid: str\n   109ÔåÆ    drug_name: str\n   110ÔåÆ    directorate: Optional[str]\n   111ÔåÆ    search_term: Optional[str] = None\n   112ÔåÆ    source: str = \"FALLBACK\"  # DIAGNOSIS | FALLBACK\n   113ÔåÆ    snomed_code: Optional[str] = None\n   114ÔåÆ    event_date: Optional[datetime] = None\n   115ÔåÆ\n   116ÔåÆ\n   117ÔåÆdef get_drug_clusters(\n   118ÔåÆ    drug_name: str,\n   119ÔåÆ    db_manager: Optional[DatabaseManager] = None\n   120ÔåÆ) -> list[dict]:\n   121ÔåÆ    \"\"\"\n   122ÔåÆ    Get all SNOMED cluster mappings for a drug from local SQLite.\n   123ÔåÆ\n   124ÔåÆ    Args:\n   125ÔåÆ        drug_name: Drug name to look up (case-insensitive)\n   126ÔåÆ        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n   127ÔåÆ\n   128ÔåÆ    Returns:\n   129ÔåÆ        List of dicts with keys: drug_name, indication, cluster_id,\n   130ÔåÆ        cluster_description, nice_ta_reference\n   131ÔåÆ    \"\"\"\n   132ÔåÆ    if db_manager is None:\n   133ÔåÆ        db_manager = default_db_manager\n   134ÔåÆ\n   135ÔåÆ    query = \"\"\"\n   136ÔåÆ        SELECT drug_name, indication, cluster_id, cluster_description, nice_ta_reference\n   137ÔåÆ        FROM ref_drug_indication_clusters\n   138ÔåÆ        WHERE UPPER(drug_name) = UPPER(?)\n   139ÔåÆ        ORDER BY indication, cluster_id\n   140ÔåÆ    \"\"\"\n   141ÔåÆ\n   142ÔåÆ    try:\n   143ÔåÆ        with db_manager.get_connection() as conn:\n   144ÔåÆ            cursor = conn.execute(query, (drug_name,))\n   145ÔåÆ            rows = cursor.fetchall()\n   146ÔåÆ\n   147ÔåÆ            results = []\n   148ÔåÆ            for row in rows:\n   149ÔåÆ                results.append({\n   150ÔåÆ                    \"drug_name\": row[\"drug_name\"],\n   151ÔåÆ                    \"indication\": row[\"indication\"],\n   152ÔåÆ                    \"cluster_id\": row[\"cluster_id\"],\n   153ÔåÆ                    \"cluster_description\": row[\"cluster_description\"],\n   154ÔåÆ                    \"nice_ta_reference\": row[\"nice_ta_reference\"],\n   155ÔåÆ                })\n   156ÔåÆ\n   157ÔåÆ            logger.debug(f\"Found {len(results)} cluster mappings for drug '{drug_name}'\")\n   158ÔåÆ            return results\n   159ÔåÆ\n   160ÔåÆ    except Exception as e:\n   161ÔåÆ        logger.error(f\"Error getting clusters for drug '{drug_name}': {e}\")\n   162ÔåÆ        return []\n   163ÔåÆ\n   164ÔåÆ\n   165ÔåÆdef get_drug_cluster_ids(\n   166ÔåÆ    drug_name: str,\n   167ÔåÆ    db_manager: Optional[DatabaseManager] = None\n   168ÔåÆ) -> list[str]:\n   169ÔåÆ    \"\"\"\n   170ÔåÆ    Get unique cluster IDs for a drug.\n   171ÔåÆ\n   172ÔåÆ    Args:\n   173ÔåÆ        drug_name: Drug name to look up\n   174ÔåÆ        db_manager: Optional DatabaseManager\n   175ÔåÆ\n   176ÔåÆ    Returns:\n   177ÔåÆ        List of unique cluster IDs\n   178ÔåÆ    \"\"\"\n   179ÔåÆ    clusters = get_drug_clusters(drug_name, db_manager)\n   180ÔåÆ    return list(set(c[\"cluster_id\"] for c in clusters))\n   181ÔåÆ\n   182ÔåÆ\n   183ÔåÆdef get_drug_snomed_codes(\n   184ÔåÆ    drug_name: str,\n   185ÔåÆ    db_manager: Optional[DatabaseManager] = None\n   186ÔåÆ) -> list[DrugSnomedMapping]:\n   187ÔåÆ    \"\"\"\n   188ÔåÆ    Get all SNOMED codes for a drug from local ref_drug_snomed_mapping table.\n   189ÔåÆ\n   190ÔåÆ    This uses the enriched mapping CSV data loaded into SQLite, which provides\n   191ÔåÆ    direct SNOMED-to-drug mappings with Search_Term and PrimaryDirectorate.\n   192ÔåÆ\n   193ÔåÆ    Args:\n   194ÔåÆ        drug_name: Drug name to look up (case-insensitive, matches cleaned_drug_name)\n   195ÔåÆ        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n   196ÔåÆ\n   197ÔåÆ    Returns:\n   198ÔåÆ        List of DrugSnomedMapping with snomed_code, snomed_description,\n   199ÔåÆ        search_term, primary_directorate, indication, ta_id\n   200ÔåÆ    \"\"\"\n   201ÔåÆ    if db_manager is None:\n   202ÔåÆ        db_manager = default_db_manager\n   203ÔåÆ\n   204ÔåÆ    query = \"\"\"\n   205ÔåÆ        SELECT DISTINCT\n   206ÔåÆ            snomed_code,\n   207ÔåÆ            snomed_description,\n   208ÔåÆ            search_term,\n   209ÔåÆ            primary_directorate,\n   210ÔåÆ            indication,\n   211ÔåÆ            ta_id\n   212ÔåÆ        FROM ref_drug_snomed_mapping\n   213ÔåÆ        WHERE UPPER(cleaned_drug_name) = UPPER(?)\n   214ÔåÆ           OR UPPER(drug_name) = UPPER(?)\n   215ÔåÆ        ORDER BY search_term, snomed_code\n   216ÔåÆ    \"\"\"\n   217ÔåÆ\n   218ÔåÆ    try:\n   219ÔåÆ        with db_manager.get_connection() as conn:\n   220ÔåÆ            cursor = conn.execute(query, (drug_name, drug_name))\n   221ÔåÆ            rows = cursor.fetchall()\n   222ÔåÆ\n   223ÔåÆ            results = []\n   224ÔåÆ            for row in rows:\n   225ÔåÆ                results.append(DrugSnomedMapping(\n   226ÔåÆ                    snomed_code=row[\"snomed_code\"],\n   227ÔåÆ                    snomed_description=row[\"snomed_description\"] or \"\",\n   228ÔåÆ                    search_term=row[\"search_term\"] or \"\",\n   229ÔåÆ                    primary_directorate=row[\"primary_directorate\"] or \"\",\n   230ÔåÆ                    indication=row[\"indication\"] or \"\",\n   231ÔåÆ                    ta_id=row[\"ta_id\"] or \"\",\n   232ÔåÆ                ))\n   233ÔåÆ\n   234ÔåÆ            logger.debug(f\"Found {len(results)} SNOMED mappings for drug '{drug_name}'\")\n   235ÔåÆ            return results\n   236ÔåÆ\n   237ÔåÆ    except Exception as e:\n   238ÔåÆ        logger.error(f\"Error getting SNOMED codes for drug '{drug_name}': {e}\")\n   239ÔåÆ        return []\n   240ÔåÆ\n   241ÔåÆ\n   242ÔåÆdef patient_has_indication_direct(\n   243ÔåÆ    patient_pseudonym: str,\n   244ÔåÆ    drug_snomed_mappings: list[DrugSnomedMapping],\n   245ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   246ÔåÆ    before_date: Optional[date] = None,\n   247ÔåÆ) -> DirectSnomedMatchResult:\n   248ÔåÆ    \"\"\"\n   249ÔåÆ    Check if patient has any of the SNOMED codes in their GP records.\n   250ÔåÆ\n   251ÔåÆ    This is the direct SNOMED lookup - it queries PrimaryCareClinicalCoding\n   252ÔåÆ    for exact SNOMED code matches (not via cluster). Returns the most recent\n   253ÔåÆ    match by EventDateTime if multiple matches exist.\n   254ÔåÆ\n   255ÔåÆ    Args:\n   256ÔåÆ        patient_pseudonym: Patient's pseudonymised NHS number\n   257ÔåÆ        drug_snomed_mappings: List of DrugSnomedMapping from get_drug_snomed_codes()\n   258ÔåÆ        connector: Optional SnowflakeConnector (defaults to singleton)\n   259ÔåÆ        before_date: Optional date - only check diagnoses before this date\n   260ÔåÆ\n   261ÔåÆ    Returns:\n   262ÔåÆ        DirectSnomedMatchResult with match details (most recent by EventDateTime)\n   263ÔåÆ    \"\"\"\n   264ÔåÆ    result = DirectSnomedMatchResult(\n   265ÔåÆ        patient_pseudonym=patient_pseudonym,\n   266ÔåÆ        matched=False,\n   267ÔåÆ        source=\"NONE\",\n   268ÔåÆ    )\n   269ÔåÆ\n   270ÔåÆ    if not drug_snomed_mappings:\n   271ÔåÆ        return result\n   272ÔåÆ\n   273ÔåÆ    if not SNOWFLAKE_AVAILABLE:\n   274ÔåÆ        logger.warning(\"Snowflake connector not available\")\n   275ÔåÆ        return result\n   276ÔåÆ\n   277ÔåÆ    if not is_snowflake_configured():\n   278ÔåÆ        logger.warning(\"Snowflake not configured - cannot check GP records\")\n   279ÔåÆ        return result\n   280ÔåÆ\n   281ÔåÆ    if connector is None:\n   282ÔåÆ        connector = get_connector()\n   283ÔåÆ\n   284ÔåÆ    # Build lookup dict for mapping snomed_code -> (search_term, primary_directorate, snomed_description)\n   285ÔåÆ    snomed_lookup = {\n   286ÔåÆ        m.snomed_code: (m.search_term, m.primary_directorate, m.snomed_description)\n   287ÔåÆ        for m in drug_snomed_mappings\n   288ÔåÆ    }\n   289ÔåÆ\n   290ÔåÆ    # Get unique SNOMED codes\n   291ÔåÆ    snomed_codes = list(snomed_lookup.keys())\n   292ÔåÆ\n   293ÔåÆ    # Build placeholders for SNOMED codes\n   294ÔåÆ    placeholders = \", \".join([\"%s\"] * len(snomed_codes))\n   295ÔåÆ\n   296ÔåÆ    # Query to find most recent matching SNOMED code in GP records\n   297ÔåÆ    query = f'''\n   298ÔåÆ        SELECT\n   299ÔåÆ            \"SNOMEDCode\",\n   300ÔåÆ            \"EventDateTime\"\n   301ÔåÆ        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n   302ÔåÆ        WHERE \"PatientPseudonym\" = %s\n   303ÔåÆ            AND \"SNOMEDCode\" IN ({placeholders})\n   304ÔåÆ    '''\n   305ÔåÆ\n   306ÔåÆ    params: list = [patient_pseudonym] + snomed_codes\n   307ÔåÆ\n   308ÔåÆ    if before_date:\n   309ÔåÆ        query += ' AND \"EventDateTime\" < %s'\n   310ÔåÆ        params.append(before_date.isoformat())\n   311ÔåÆ\n   312ÔåÆ    query += ' ORDER BY \"EventDateTime\" DESC LIMIT 1'\n   313ÔåÆ\n   314ÔåÆ    try:\n   315ÔåÆ        results = connector.execute_dict(query, tuple(params))\n   316ÔåÆ\n   317ÔåÆ        if results:\n   318ÔåÆ            row = results[0]\n   319ÔåÆ            matched_code = row.get(\"SNOMEDCode\")\n   320ÔåÆ            event_dt = row.get(\"EventDateTime\")\n   321ÔåÆ\n   322ÔåÆ            if matched_code and matched_code in snomed_lookup:\n   323ÔåÆ                search_term, primary_dir, snomed_desc = snomed_lookup[matched_code]\n   324ÔåÆ\n   325ÔåÆ                return DirectSnomedMatchResult(\n   326ÔåÆ                    patient_pseudonym=patient_pseudonym,\n   327ÔåÆ                    matched=True,\n   328ÔåÆ                    snomed_code=matched_code,\n   329ÔåÆ                    snomed_description=snomed_desc,\n   330ÔåÆ                    search_term=search_term,\n   331ÔåÆ                    primary_directorate=primary_dir,\n   332ÔåÆ                    event_date=event_dt,\n   333ÔåÆ                    source=\"DIRECT_SNOMED\",\n   334ÔåÆ                )\n   335ÔåÆ\n   336ÔåÆ        return result\n   337ÔåÆ\n   338ÔåÆ    except Exception as e:\n   339ÔåÆ        logger.error(f\"Error checking direct SNOMED for patient '{patient_pseudonym}': {e}\")\n   340ÔåÆ        return result\n   341ÔåÆ\n   342ÔåÆ\n   343ÔåÆdef get_directorate_from_diagnosis(\n   344ÔåÆ    upid: str,\n   345ÔåÆ    drug_name: str,\n   346ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   347ÔåÆ    db_manager: Optional[DatabaseManager] = None,\n   348ÔåÆ    before_date: Optional[date] = None,\n   349ÔåÆ) -> DirectorateAssignment:\n   350ÔåÆ    \"\"\"\n   351ÔåÆ    Get directorate assignment for a patient-drug combination using diagnosis-based lookup.\n   352ÔåÆ\n   353ÔåÆ    This function attempts to assign a directorate based on the patient's GP records\n   354ÔåÆ    (direct SNOMED code matching). If no match is found, it returns a FALLBACK result\n   355ÔåÆ    indicating that the caller should use alternative assignment methods (e.g.,\n   356ÔåÆ    department_identification() from tools/data.py).\n   357ÔåÆ\n   358ÔåÆ    Workflow:\n   359ÔåÆ    1. Get all SNOMED codes for the drug from ref_drug_snomed_mapping\n   360ÔåÆ    2. Query patient's GP records for matching SNOMED codes\n   361ÔåÆ    3. If match found ÔåÆ return diagnosis-based directorate and search_term\n   362ÔåÆ    4. If no match ÔåÆ return FALLBACK result (caller handles fallback logic)\n   363ÔåÆ\n   364ÔåÆ    Args:\n   365ÔåÆ        upid: Patient's unique patient ID (Provider Code[:3] + PersonKey)\n   366ÔåÆ        drug_name: Drug name to look up\n   367ÔåÆ        connector: Optional SnowflakeConnector (defaults to singleton)\n   368ÔåÆ        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n   369ÔåÆ        before_date: Optional date - only check diagnoses before this date\n   370ÔåÆ\n   371ÔåÆ    Returns:\n   372ÔåÆ        DirectorateAssignment with directorate, search_term, and source\n   373ÔåÆ    \"\"\"\n   374ÔåÆ    result = DirectorateAssignment(\n   375ÔåÆ        upid=upid,\n   376ÔåÆ        drug_name=drug_name,\n   377ÔåÆ        directorate=None,\n   378ÔåÆ        source=\"FALLBACK\",\n   379ÔåÆ    )\n   380ÔåÆ\n   381ÔåÆ    # Step 1: Get SNOMED codes for the drug\n   382ÔåÆ    drug_snomed_mappings = get_drug_snomed_codes(drug_name, db_manager)\n   383ÔåÆ\n   384ÔåÆ    if not drug_snomed_mappings:\n   385ÔåÆ        logger.debug(f\"No SNOMED mappings found for drug '{drug_name}' - using fallback\")\n   386ÔåÆ        return result\n   387ÔåÆ\n   388ÔåÆ    # Step 2: Check Snowflake availability\n   389ÔåÆ    if not SNOWFLAKE_AVAILABLE:\n   390ÔåÆ        logger.debug(\"Snowflake not available - using fallback\")\n   391ÔåÆ        return result\n   392ÔåÆ\n   393ÔåÆ    if not is_snowflake_configured():\n   394ÔåÆ        logger.debug(\"Snowflake not configured - using fallback\")\n   395ÔåÆ        return result\n   396ÔåÆ\n   397ÔåÆ    # Step 3: Get patient pseudonym from UPID\n   398ÔåÆ    # UPID format is Provider Code (3 chars) + PersonKey\n   399ÔåÆ    # We need to query Snowflake to get the PatientPseudonym for this PersonKey\n   400ÔåÆ    # However, patient_has_indication_direct expects PatientPseudonym, not UPID\n   401ÔåÆ    # For now, we'll use UPID as the identifier - the actual integration\n   402ÔåÆ    # will need to happen at the DataFrame level where we have PersonKey\n   403ÔåÆ    #\n   404ÔåÆ    # NOTE: This function will be called from the pipeline where we have\n   405ÔåÆ    # access to PatientPseudonym. The UPID is passed for logging/tracking.\n   406ÔåÆ\n   407ÔåÆ    # Actually, looking at the pipeline, we need PatientPseudonym, not UPID.\n   408ÔåÆ    # The caller should pass the PatientPseudonym or we need to look it up.\n   409ÔåÆ    # For now, let's assume the caller will use this in a batch context\n   410ÔåÆ    # where they can map UPID -> PatientPseudonym.\n   411ÔåÆ\n   412ÔåÆ    # Let me reconsider: the function signature takes UPID but we need\n   413ÔåÆ    # PatientPseudonym for Snowflake. In the pipeline context (fetch_and_transform_data),\n   414ÔåÆ    # we'll have the PersonKey column which IS the PatientPseudonym.\n   415ÔåÆ    # So UPID = ProviderCode[:3] + PersonKey, and PersonKey = PatientPseudonym.\n   416ÔåÆ    #\n   417ÔåÆ    # We can extract PatientPseudonym from UPID by removing the first 3 chars.\n   418ÔåÆ    patient_pseudonym = upid[3:] if len(upid) > 3 else upid\n   419ÔåÆ\n   420ÔåÆ    # Step 4: Check patient's GP records for matching SNOMED codes\n   421ÔåÆ    match_result = patient_has_indication_direct(\n   422ÔåÆ        patient_pseudonym=patient_pseudonym,\n   423ÔåÆ        drug_snomed_mappings=drug_snomed_mappings,\n   424ÔåÆ        connector=connector,\n   425ÔåÆ        before_date=before_date,\n   426ÔåÆ    )\n   427ÔåÆ\n   428ÔåÆ    if match_result.matched and match_result.primary_directorate:\n   429ÔåÆ        return DirectorateAssignment(\n   430ÔåÆ            upid=upid,\n   431ÔåÆ            drug_name=drug_name,\n   432ÔåÆ            directorate=match_result.primary_directorate,\n   433ÔåÆ            search_term=match_result.search_term,\n   434ÔåÆ            source=\"DIAGNOSIS\",\n   435ÔåÆ            snomed_code=match_result.snomed_code,\n   436ÔåÆ            event_date=match_result.event_date,\n   437ÔåÆ        )\n   438ÔåÆ\n   439ÔåÆ    # No match found - return fallback result\n   440ÔåÆ    return result\n   441ÔåÆ\n   442ÔåÆ\n   443ÔåÆdef get_cluster_snomed_codes(\n   444ÔåÆ    cluster_id: str,\n   445ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   446ÔåÆ    use_cache: bool = True,\n   447ÔåÆ) -> ClusterSnomedCodes:\n   448ÔåÆ    \"\"\"\n   449ÔåÆ    Get all SNOMED codes for a cluster from Snowflake.\n   450ÔåÆ\n   451ÔåÆ    Queries the ClinicalCodingClusterSnomedCodes table to get all SNOMED codes\n   452ÔåÆ    that belong to the specified cluster.\n   453ÔåÆ\n   454ÔåÆ    Args:\n   455ÔåÆ        cluster_id: Cluster ID to look up (e.g., 'RARTH_COD', 'PSORIASIS_COD')\n   456ÔåÆ        connector: Optional SnowflakeConnector (defaults to singleton)\n   457ÔåÆ        use_cache: Whether to use cached results (default True)\n   458ÔåÆ\n   459ÔåÆ    Returns:\n   460ÔåÆ        ClusterSnomedCodes with list of SNOMED codes and descriptions\n   461ÔåÆ    \"\"\"\n   462ÔåÆ    if not SNOWFLAKE_AVAILABLE:\n   463ÔåÆ        logger.warning(\"Snowflake connector not available\")\n   464ÔåÆ        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n   465ÔåÆ\n   466ÔåÆ    if not is_snowflake_configured():\n   467ÔåÆ        logger.warning(\"Snowflake not configured - cannot get cluster codes\")\n   468ÔåÆ        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n   469ÔåÆ\n   470ÔåÆ    # Check cache first\n   471ÔåÆ    cache_key = f\"cluster_snomed_{cluster_id}\"\n   472ÔåÆ    if use_cache and is_cache_enabled():\n   473ÔåÆ        cache = get_cache()\n   474ÔåÆ        cached = cache.get(cache_key)\n   475ÔåÆ        if cached is not None and len(cached) > 0:\n   476ÔåÆ            logger.debug(f\"Using cached SNOMED codes for cluster '{cluster_id}'\")\n   477ÔåÆ            cached_dict = cached[0]  # First element is our data dict\n   478ÔåÆ            return ClusterSnomedCodes(\n   479ÔåÆ                cluster_id=cluster_id,\n   480ÔåÆ                cluster_description=str(cached_dict.get(\"description\", \"\")),\n   481ÔåÆ                snomed_codes=list(cached_dict.get(\"codes\", [])),\n   482ÔåÆ                snomed_descriptions=dict(cached_dict.get(\"descriptions\", {})),\n   483ÔåÆ            )\n   484ÔåÆ\n   485ÔåÆ    if connector is None:\n   486ÔåÆ        connector = get_connector()\n   487ÔåÆ\n   488ÔåÆ    query = '''\n   489ÔåÆ        SELECT DISTINCT\n   490ÔåÆ            \"Cluster_ID\",\n   491ÔåÆ            \"Cluster_Description\",\n   492ÔåÆ            \"SNOMEDCode\",\n   493ÔåÆ            \"SNOMEDDescription\"\n   494ÔåÆ        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n   495ÔåÆ        WHERE \"Cluster_ID\" = %s\n   496ÔåÆ        ORDER BY \"SNOMEDCode\"\n   497ÔåÆ    '''\n   498ÔåÆ\n   499ÔåÆ    try:\n   500ÔåÆ        results = connector.execute_dict(query, (cluster_id,))\n   501ÔåÆ\n   502ÔåÆ        if not results:\n   503ÔåÆ            logger.warning(f\"No SNOMED codes found for cluster '{cluster_id}'\")\n   504ÔåÆ            return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n   505ÔåÆ\n   506ÔåÆ        codes = []\n   507ÔåÆ        descriptions = {}\n   508ÔåÆ        description = results[0].get(\"Cluster_Description\", \"\") if results else \"\"\n   509ÔåÆ\n   510ÔåÆ        for row in results:\n   511ÔåÆ            code = row.get(\"SNOMEDCode\")\n   512ÔåÆ            if code:\n   513ÔåÆ                codes.append(code)\n   514ÔåÆ                descriptions[code] = row.get(\"SNOMEDDescription\", \"\")\n   515ÔåÆ\n   516ÔåÆ        logger.info(f\"Found {len(codes)} SNOMED codes for cluster '{cluster_id}'\")\n   517ÔåÆ\n   518ÔåÆ        # Cache the results (using query-based cache with fake params)\n   519ÔåÆ        if use_cache and is_cache_enabled():\n   520ÔåÆ            cache = get_cache()\n   521ÔåÆ            cache_data = [{\n   522ÔåÆ                \"description\": description,\n   523ÔåÆ                \"codes\": codes,\n   524ÔåÆ                \"descriptions\": descriptions,\n   525ÔåÆ            }]\n   526ÔåÆ            cache.set(cache_key, None, cache_data)  # type: ignore[arg-type]\n   527ÔåÆ\n   528ÔåÆ        return ClusterSnomedCodes(\n   529ÔåÆ            cluster_id=cluster_id,\n   530ÔåÆ            cluster_description=description,\n   531ÔåÆ            snomed_codes=codes,\n   532ÔåÆ            snomed_descriptions=descriptions,\n   533ÔåÆ        )\n   534ÔåÆ\n   535ÔåÆ    except Exception as e:\n   536ÔåÆ        logger.error(f\"Error getting SNOMED codes for cluster '{cluster_id}': {e}\")\n   537ÔåÆ        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n   538ÔåÆ\n   539ÔåÆ\n   540ÔåÆdef patient_has_indication(\n   541ÔåÆ    patient_pseudonym: str,\n   542ÔåÆ    cluster_ids: list[str],\n   543ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   544ÔåÆ    before_date: Optional[date] = None,\n   545ÔåÆ) -> tuple[bool, Optional[str], Optional[str], Optional[str]]:\n   546ÔåÆ    \"\"\"\n   547ÔåÆ    Check if a patient has any SNOMED codes from the specified clusters in GP records.\n   548ÔåÆ\n   549ÔåÆ    Args:\n   550ÔåÆ        patient_pseudonym: Patient's pseudonymised NHS number\n   551ÔåÆ        cluster_ids: List of cluster IDs to check against\n   552ÔåÆ        connector: Optional SnowflakeConnector\n   553ÔåÆ        before_date: Optional date - only check diagnoses before this date\n   554ÔåÆ\n   555ÔåÆ    Returns:\n   556ÔåÆ        Tuple of (has_indication, matched_cluster_id, matched_snomed_code, matched_description)\n   557ÔåÆ    \"\"\"\n   558ÔåÆ    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n   559ÔåÆ        return False, None, None, None\n   560ÔåÆ\n   561ÔåÆ    if not cluster_ids:\n   562ÔåÆ        return False, None, None, None\n   563ÔåÆ\n   564ÔåÆ    if connector is None:\n   565ÔåÆ        connector = get_connector()\n   566ÔåÆ\n   567ÔåÆ    # Build placeholders for cluster IDs\n   568ÔåÆ    placeholders = \", \".join([\"%s\"] * len(cluster_ids))\n   569ÔåÆ\n   570ÔåÆ    # Query to check if patient has any matching SNOMED code\n   571ÔåÆ    query = f'''\n   572ÔåÆ        SELECT\n   573ÔåÆ            pc.\"SNOMEDCode\",\n   574ÔåÆ            cc.\"Cluster_ID\",\n   575ÔåÆ            cc.\"SNOMEDDescription\"\n   576ÔåÆ        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\n   577ÔåÆ        INNER JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" cc\n   578ÔåÆ            ON pc.\"SNOMEDCode\" = cc.\"SNOMEDCode\"\n   579ÔåÆ        WHERE pc.\"PatientPseudonym\" = %s\n   580ÔåÆ            AND cc.\"Cluster_ID\" IN ({placeholders})\n   581ÔåÆ    '''\n   582ÔåÆ\n   583ÔåÆ    params = [patient_pseudonym] + cluster_ids\n   584ÔåÆ\n   585ÔåÆ    if before_date:\n   586ÔåÆ        query += ' AND pc.\"EventDateTime\" < %s'\n   587ÔåÆ        params.append(before_date.isoformat())\n   588ÔåÆ\n   589ÔåÆ    query += ' LIMIT 1'\n   590ÔåÆ\n   591ÔåÆ    try:\n   592ÔåÆ        results = connector.execute_dict(query, tuple(params))\n   593ÔåÆ\n   594ÔåÆ        if results:\n   595ÔåÆ            row = results[0]\n   596ÔåÆ            return (\n   597ÔåÆ                True,\n   598ÔåÆ                row.get(\"Cluster_ID\"),\n   599ÔåÆ                row.get(\"SNOMEDCode\"),\n   600ÔåÆ                row.get(\"SNOMEDDescription\"),\n   601ÔåÆ            )\n   602ÔåÆ\n   603ÔåÆ        return False, None, None, None\n   604ÔåÆ\n   605ÔåÆ    except Exception as e:\n   606ÔåÆ        logger.error(f\"Error checking indication for patient '{patient_pseudonym}': {e}\")\n   607ÔåÆ        return False, None, None, None\n   608ÔåÆ\n   609ÔåÆ\n   610ÔåÆdef validate_indication(\n   611ÔåÆ    patient_pseudonym: str,\n   612ÔåÆ    drug_name: str,\n   613ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   614ÔåÆ    db_manager: Optional[DatabaseManager] = None,\n   615ÔåÆ    before_date: Optional[date] = None,\n   616ÔåÆ) -> IndicationValidationResult:\n   617ÔåÆ    \"\"\"\n   618ÔåÆ    Validate that a patient has an appropriate indication for a drug.\n   619ÔåÆ\n   620ÔåÆ    Full validation workflow:\n   621ÔåÆ    1. Get drug's valid indication clusters from local mapping\n   622ÔåÆ    2. Check if patient has any matching SNOMED codes in GP records\n   623ÔåÆ    3. Return detailed validation result\n   624ÔåÆ\n   625ÔåÆ    Args:\n   626ÔåÆ        patient_pseudonym: Patient's pseudonymised NHS number\n   627ÔåÆ        drug_name: Drug name to validate indication for\n   628ÔåÆ        connector: Optional SnowflakeConnector\n   629ÔåÆ        db_manager: Optional DatabaseManager\n   630ÔåÆ        before_date: Optional date - only check diagnoses before this date\n   631ÔåÆ\n   632ÔåÆ    Returns:\n   633ÔåÆ        IndicationValidationResult with validation details\n   634ÔåÆ    \"\"\"\n   635ÔåÆ    result = IndicationValidationResult(\n   636ÔåÆ        patient_pseudonym=patient_pseudonym,\n   637ÔåÆ        drug_name=drug_name,\n   638ÔåÆ        has_valid_indication=False,\n   639ÔåÆ    )\n   640ÔåÆ\n   641ÔåÆ    # Step 1: Get drug's cluster mappings\n   642ÔåÆ    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n   643ÔåÆ\n   644ÔåÆ    if not cluster_ids:\n   645ÔåÆ        result.error_message = f\"No cluster mappings found for drug '{drug_name}'\"\n   646ÔåÆ        result.source = \"NONE\"\n   647ÔåÆ        return result\n   648ÔåÆ\n   649ÔåÆ    result.checked_clusters = cluster_ids\n   650ÔåÆ\n   651ÔåÆ    # Step 2: Check Snowflake availability\n   652ÔåÆ    if not SNOWFLAKE_AVAILABLE:\n   653ÔåÆ        result.error_message = \"Snowflake connector not installed\"\n   654ÔåÆ        result.source = \"NONE\"\n   655ÔåÆ        return result\n   656ÔåÆ\n   657ÔåÆ    if not is_snowflake_configured():\n   658ÔåÆ        result.error_message = \"Snowflake not configured\"\n   659ÔåÆ        result.source = \"NONE\"\n   660ÔåÆ        return result\n   661ÔåÆ\n   662ÔåÆ    # Step 3: Check patient GP records\n   663ÔåÆ    has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n   664ÔåÆ        patient_pseudonym=patient_pseudonym,\n   665ÔåÆ        cluster_ids=cluster_ids,\n   666ÔåÆ        connector=connector,\n   667ÔåÆ        before_date=before_date,\n   668ÔåÆ    )\n   669ÔåÆ\n   670ÔåÆ    result.has_valid_indication = has_indication\n   671ÔåÆ    result.matched_cluster_id = matched_cluster\n   672ÔåÆ    result.matched_snomed_code = matched_code\n   673ÔåÆ    result.matched_snomed_description = matched_desc\n   674ÔåÆ    result.source = \"GP_SNOMED\" if has_indication else \"NONE\"\n   675ÔåÆ\n   676ÔåÆ    return result\n   677ÔåÆ\n   678ÔåÆ\n   679ÔåÆdef get_indication_match_rate(\n   680ÔåÆ    drug_name: str,\n   681ÔåÆ    patient_pseudonyms: list[str],\n   682ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   683ÔåÆ    db_manager: Optional[DatabaseManager] = None,\n   684ÔåÆ    sample_unmatched_count: int = 10,\n   685ÔåÆ) -> DrugIndicationMatchRate:\n   686ÔåÆ    \"\"\"\n   687ÔåÆ    Calculate indication match rate for a drug across a list of patients.\n   688ÔåÆ\n   689ÔåÆ    Args:\n   690ÔåÆ        drug_name: Drug name to check\n   691ÔåÆ        patient_pseudonyms: List of patient pseudonymised NHS numbers\n   692ÔåÆ        connector: Optional SnowflakeConnector\n   693ÔåÆ        db_manager: Optional DatabaseManager\n   694ÔåÆ        sample_unmatched_count: Number of unmatched patient IDs to include in sample\n   695ÔåÆ\n   696ÔåÆ    Returns:\n   697ÔåÆ        DrugIndicationMatchRate with match statistics\n   698ÔåÆ    \"\"\"\n   699ÔåÆ    if connector is None and SNOWFLAKE_AVAILABLE and is_snowflake_configured():\n   700ÔåÆ        connector = get_connector()\n   701ÔåÆ\n   702ÔåÆ    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n   703ÔåÆ\n   704ÔåÆ    total = len(patient_pseudonyms)\n   705ÔåÆ    matched = 0\n   706ÔåÆ    unmatched = 0\n   707ÔåÆ    sample_unmatched: list[str] = []\n   708ÔåÆ\n   709ÔåÆ    if not cluster_ids:\n   710ÔåÆ        logger.warning(f\"No cluster mappings for drug '{drug_name}' - all patients will be unmatched\")\n   711ÔåÆ        return DrugIndicationMatchRate(\n   712ÔåÆ            drug_name=drug_name,\n   713ÔåÆ            total_patients=total,\n   714ÔåÆ            patients_with_indication=0,\n   715ÔåÆ            patients_without_indication=total,\n   716ÔåÆ            match_rate=0.0,\n   717ÔåÆ            clusters_checked=[],\n   718ÔåÆ            sample_unmatched=patient_pseudonyms[:sample_unmatched_count],\n   719ÔåÆ        )\n   720ÔåÆ\n   721ÔåÆ    for i, pseudonym in enumerate(patient_pseudonyms):\n   722ÔåÆ        if i > 0 and i % 100 == 0:\n   723ÔåÆ            logger.info(f\"Validating indications: {i}/{total} ({100*i/total:.1f}%)\")\n   724ÔåÆ\n   725ÔåÆ        has_indication, _, _, _ = patient_has_indication(\n   726ÔåÆ            patient_pseudonym=pseudonym,\n   727ÔåÆ            cluster_ids=cluster_ids,\n   728ÔåÆ            connector=connector,\n   729ÔåÆ        )\n   730ÔåÆ\n   731ÔåÆ        if has_indication:\n   732ÔåÆ            matched += 1\n   733ÔåÆ        else:\n   734ÔåÆ            unmatched += 1\n   735ÔåÆ            if len(sample_unmatched) < sample_unmatched_count:\n   736ÔåÆ                sample_unmatched.append(pseudonym)\n   737ÔåÆ\n   738ÔåÆ    match_rate = matched / total if total > 0 else 0.0\n   739ÔåÆ\n   740ÔåÆ    logger.info(f\"Indication match rate for '{drug_name}': {100*match_rate:.1f}% ({matched}/{total})\")\n   741ÔåÆ\n   742ÔåÆ    return DrugIndicationMatchRate(\n   743ÔåÆ        drug_name=drug_name,\n   744ÔåÆ        total_patients=total,\n   745ÔåÆ        patients_with_indication=matched,\n   746ÔåÆ        patients_without_indication=unmatched,\n   747ÔåÆ        match_rate=match_rate,\n   748ÔåÆ        clusters_checked=cluster_ids,\n   749ÔåÆ        sample_unmatched=sample_unmatched,\n   750ÔåÆ    )\n   751ÔåÆ\n   752ÔåÆ\n   753ÔåÆdef batch_validate_indications(\n   754ÔåÆ    patient_drug_pairs: list[tuple[str, str]],\n   755ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   756ÔåÆ    db_manager: Optional[DatabaseManager] = None,\n   757ÔåÆ    progress_callback: Optional[Callable[[int, int], None]] = None,\n   758ÔåÆ) -> list[IndicationValidationResult]:\n   759ÔåÆ    \"\"\"\n   760ÔåÆ    Validate indications for multiple patient-drug pairs efficiently.\n   761ÔåÆ\n   762ÔåÆ    Args:\n   763ÔåÆ        patient_drug_pairs: List of (patient_pseudonym, drug_name) tuples\n   764ÔåÆ        connector: Optional SnowflakeConnector\n   765ÔåÆ        db_manager: Optional DatabaseManager\n   766ÔåÆ        progress_callback: Optional callback(current, total) for progress updates\n   767ÔåÆ\n   768ÔåÆ    Returns:\n   769ÔåÆ        List of IndicationValidationResult for each pair\n   770ÔåÆ    \"\"\"\n   771ÔåÆ    results = []\n   772ÔåÆ    total = len(patient_drug_pairs)\n   773ÔåÆ\n   774ÔåÆ    # Cache cluster lookups by drug\n   775ÔåÆ    drug_clusters_cache = {}\n   776ÔåÆ\n   777ÔåÆ    for i, (pseudonym, drug_name) in enumerate(patient_drug_pairs):\n   778ÔåÆ        if progress_callback:\n   779ÔåÆ            progress_callback(i + 1, total)\n   780ÔåÆ\n   781ÔåÆ        # Get clusters from cache or lookup\n   782ÔåÆ        drug_upper = drug_name.upper()\n   783ÔåÆ        if drug_upper not in drug_clusters_cache:\n   784ÔåÆ            drug_clusters_cache[drug_upper] = get_drug_cluster_ids(drug_name, db_manager)\n   785ÔåÆ\n   786ÔåÆ        cluster_ids = drug_clusters_cache[drug_upper]\n   787ÔåÆ\n   788ÔåÆ        if not cluster_ids:\n   789ÔåÆ            results.append(IndicationValidationResult(\n   790ÔåÆ                patient_pseudonym=pseudonym,\n   791ÔåÆ                drug_name=drug_name,\n   792ÔåÆ                has_valid_indication=False,\n   793ÔåÆ                source=\"NONE\",\n   794ÔåÆ                error_message=f\"No cluster mappings for drug '{drug_name}'\",\n   795ÔåÆ            ))\n   796ÔåÆ            continue\n   797ÔåÆ\n   798ÔåÆ        # Check patient indication\n   799ÔåÆ        has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n   800ÔåÆ            patient_pseudonym=pseudonym,\n   801ÔåÆ            cluster_ids=cluster_ids,\n   802ÔåÆ            connector=connector,\n   803ÔåÆ        )\n   804ÔåÆ\n   805ÔåÆ        results.append(IndicationValidationResult(\n   806ÔåÆ            patient_pseudonym=pseudonym,\n   807ÔåÆ            drug_name=drug_name,\n   808ÔåÆ            has_valid_indication=has_indication,\n   809ÔåÆ            matched_cluster_id=matched_cluster,\n   810ÔåÆ            matched_snomed_code=matched_code,\n   811ÔåÆ            matched_snomed_description=matched_desc,\n   812ÔåÆ            checked_clusters=cluster_ids,\n   813ÔåÆ            source=\"GP_SNOMED\" if has_indication else \"NONE\",\n   814ÔåÆ        ))\n   815ÔåÆ\n   816ÔåÆ    matched_count = sum(1 for r in results if r.has_valid_indication)\n   817ÔåÆ    logger.info(f\"Batch validation complete: {matched_count}/{total} ({100*matched_count/total:.1f}%) with valid indications\")\n   818ÔåÆ\n   819ÔåÆ    return results\n   820ÔåÆ\n   821ÔåÆ\n   822ÔåÆdef get_available_clusters(\n   823ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   824ÔåÆ) -> list[dict]:\n   825ÔåÆ    \"\"\"\n   826ÔåÆ    Get list of all available SNOMED clusters from Snowflake.\n   827ÔåÆ\n   828ÔåÆ    Returns:\n   829ÔåÆ        List of dicts with cluster_id, cluster_description, code_count\n   830ÔåÆ    \"\"\"\n   831ÔåÆ    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n   832ÔåÆ        logger.warning(\"Snowflake not available - cannot list clusters\")\n   833ÔåÆ        return []\n   834ÔåÆ\n   835ÔåÆ    if connector is None:\n   836ÔåÆ        connector = get_connector()\n   837ÔåÆ\n   838ÔåÆ    query = '''\n   839ÔåÆ        SELECT\n   840ÔåÆ            \"Cluster_ID\",\n   841ÔåÆ            \"Cluster_Description\",\n   842ÔåÆ            COUNT(DISTINCT \"SNOMEDCode\") as code_count\n   843ÔåÆ        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n   844ÔåÆ        GROUP BY \"Cluster_ID\", \"Cluster_Description\"\n   845ÔåÆ        ORDER BY \"Cluster_ID\"\n   846ÔåÆ    '''\n   847ÔåÆ\n   848ÔåÆ    try:\n   849ÔåÆ        results = connector.execute_dict(query)\n   850ÔåÆ\n   851ÔåÆ        clusters = []\n   852ÔåÆ        for row in results:\n   853ÔåÆ            clusters.append({\n   854ÔåÆ                \"cluster_id\": row.get(\"Cluster_ID\"),\n   855ÔåÆ                \"cluster_description\": row.get(\"Cluster_Description\"),\n   856ÔåÆ                \"code_count\": row.get(\"code_count\", 0),\n   857ÔåÆ            })\n   858ÔåÆ\n   859ÔåÆ        logger.info(f\"Found {len(clusters)} available SNOMED clusters\")\n   860ÔåÆ        return clusters\n   861ÔåÆ\n   862ÔåÆ    except Exception as e:\n   863ÔåÆ        logger.error(f\"Error getting available clusters: {e}\")\n   864ÔåÆ        return []\n   865ÔåÆ\n   866ÔåÆ\n   867ÔåÆdef batch_lookup_indication_groups(\n   868ÔåÆ    df: \"pd.DataFrame\",\n   869ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n   870ÔåÆ    db_manager: Optional[DatabaseManager] = None,\n   871ÔåÆ    batch_size: int = 500,\n   872ÔåÆ) -> \"pd.DataFrame\":\n   873ÔåÆ    \"\"\"\n   874ÔåÆ    Batch lookup GP diagnosis-based indication groups for a DataFrame of patients.\n   875ÔåÆ\n   876ÔåÆ    This is the efficient batch version of get_directorate_from_diagnosis().\n   877ÔåÆ    Instead of querying Snowflake per patient, it batches the lookups for performance.\n   878ÔåÆ\n   879ÔåÆ    Strategy:\n   880ÔåÆ    1. Get all unique (PersonKey, Drug Name) pairs from DataFrame\n   881ÔåÆ    2. For each unique drug, get all SNOMED codes from local SQLite\n   882ÔåÆ    3. Build batched Snowflake queries to check GP records\n   883ÔåÆ    4. Return indication_df mapping UPID ÔåÆ Indication_Group\n   884ÔåÆ\n   885ÔåÆ    For unmatched patients, Indication_Group will be their Directory (with suffix).\n   886ÔåÆ\n   887ÔåÆ    Args:\n   888ÔåÆ        df: DataFrame with columns: UPID, Drug Name, Directory, PersonKey\n   889ÔåÆ        connector: Optional SnowflakeConnector (defaults to singleton)\n   890ÔåÆ        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n   891ÔåÆ        batch_size: Number of patients per Snowflake query batch\n   892ÔåÆ\n   893ÔåÆ    Returns:\n   894ÔåÆ        DataFrame with columns: UPID, Indication_Group, Source\n   895ÔåÆ        - Indication_Group: Search_Term (if matched) or \"Directory (no GP dx)\" (if not)\n   896ÔåÆ        - Source: \"DIAGNOSIS\" or \"FALLBACK\"\n   897ÔåÆ    \"\"\"\n   898ÔåÆ    import pandas as pd\n   899ÔåÆ\n   900ÔåÆ    if db_manager is None:\n   901ÔåÆ        db_manager = default_db_manager\n   902ÔåÆ\n   903ÔåÆ    logger.info(f\"Starting batch indication lookup for {len(df)} records...\")\n   904ÔåÆ\n   905ÔåÆ    # Step 1: Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combinations\n   906ÔåÆ    # We need PseudoNHSNoLinked to query Snowflake - this matches PatientPseudonym in GP records\n   907ÔåÆ    # Note: PersonKey is LocalPatientID which is provider-specific and does NOT match GP records\n   908ÔåÆ    if 'PseudoNHSNoLinked' not in df.columns:\n   909ÔåÆ        logger.error(\"DataFrame missing 'PseudoNHSNoLinked' column - cannot lookup GP records\")\n   910ÔåÆ        # Return fallback for all patients\n   911ÔåÆ        result_df = df[['UPID', 'Directory']].drop_duplicates().copy()\n   912ÔåÆ        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n   913ÔåÆ        result_df['Source'] = \"FALLBACK\"\n   914ÔåÆ        return result_df[['UPID', 'Indication_Group', 'Source']]\n   915ÔåÆ\n   916ÔåÆ    # Get unique patient-drug combinations (we need one lookup per patient-drug pair)\n   917ÔåÆ    unique_pairs = df[['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']].drop_duplicates()\n   918ÔåÆ    logger.info(f\"Found {len(unique_pairs)} unique patient-drug combinations\")\n   919ÔåÆ\n   920ÔåÆ    # Step 2: Get all unique drugs and their SNOMED codes\n   921ÔåÆ    unique_drugs = unique_pairs['Drug Name'].unique()\n   922ÔåÆ    logger.info(f\"Building SNOMED lookup for {len(unique_drugs)} unique drugs...\")\n   923ÔåÆ\n   924ÔåÆ    # Build drug -> list of DrugSnomedMapping dict\n   925ÔåÆ    drug_snomed_map: dict[str, list[DrugSnomedMapping]] = {}\n   926ÔåÆ    all_snomed_codes: set[str] = set()\n   927ÔåÆ    snomed_to_drug_searchterm: dict[str, list[tuple[str, str, str]]] = {}  # snomed -> [(drug, search_term, primary_dir), ...]\n   928ÔåÆ\n   929ÔåÆ    for drug_name in unique_drugs:\n   930ÔåÆ        mappings = get_drug_snomed_codes(drug_name, db_manager)\n   931ÔåÆ        drug_snomed_map[drug_name] = mappings\n   932ÔåÆ\n   933ÔåÆ        for m in mappings:\n   934ÔåÆ            all_snomed_codes.add(m.snomed_code)\n   935ÔåÆ            if m.snomed_code not in snomed_to_drug_searchterm:\n   936ÔåÆ                snomed_to_drug_searchterm[m.snomed_code] = []\n   937ÔåÆ            snomed_to_drug_searchterm[m.snomed_code].append(\n   938ÔåÆ                (drug_name, m.search_term, m.primary_directorate)\n   939ÔåÆ            )\n   940ÔåÆ\n   941ÔåÆ    logger.info(f\"Total SNOMED codes to check: {len(all_snomed_codes)}\")\n   942ÔåÆ\n   943ÔåÆ    # Step 3: Check Snowflake availability\n   944ÔåÆ    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n   945ÔåÆ        logger.warning(\"Snowflake not available - returning fallback for all patients\")\n   946ÔåÆ        result_df = unique_pairs[['UPID', 'Directory']].copy()\n   947ÔåÆ        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n   948ÔåÆ        result_df['Source'] = \"FALLBACK\"\n   949ÔåÆ        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n   950ÔåÆ\n   951ÔåÆ    if connector is None:\n   952ÔåÆ        connector = get_connector()\n   953ÔåÆ\n   954ÔåÆ    # Step 4: Query GP records for all patients in batches\n   955ÔåÆ    # The query finds the most recent matching SNOMED code for each patient\n   956ÔåÆ\n   957ÔåÆ    # Get unique PseudoNHSNoLinked values (each = one patient in GP records)\n   958ÔåÆ    unique_patients = unique_pairs[['PseudoNHSNoLinked', 'UPID', 'Directory']].drop_duplicates(subset=['PseudoNHSNoLinked'])\n   959ÔåÆ    patient_pseudonyms = unique_patients['PseudoNHSNoLinked'].tolist()\n   960ÔåÆ\n   961ÔåÆ    logger.info(f\"Querying GP records for {len(patient_pseudonyms)} unique patients in batches of {batch_size}...\")\n   962ÔåÆ\n   963ÔåÆ    # Results dict: PersonKey -> (snomed_code, event_date)\n   964ÔåÆ    gp_matches: dict[str, tuple[str, Any]] = {}\n   965ÔåÆ\n   966ÔåÆ    # Convert SNOMED codes to list for query\n   967ÔåÆ    snomed_list = list(all_snomed_codes)\n   968ÔåÆ\n   969ÔåÆ    if not snomed_list:\n   970ÔåÆ        logger.warning(\"No SNOMED codes to check - returning fallback for all patients\")\n   971ÔåÆ        result_df = unique_pairs[['UPID', 'Directory']].copy()\n   972ÔåÆ        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n   973ÔåÆ        result_df['Source'] = \"FALLBACK\"\n   974ÔåÆ        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n   975ÔåÆ\n   976ÔåÆ    # Build SNOMED IN clause (reused across batches)\n   977ÔåÆ    snomed_placeholders = \", \".join([\"%s\"] * len(snomed_list))\n   978ÔåÆ\n   979ÔåÆ    # Process patients in batches\n   980ÔåÆ    for batch_start in range(0, len(patient_pseudonyms), batch_size):\n   981ÔåÆ        batch_end = min(batch_start + batch_size, len(patient_pseudonyms))\n   982ÔåÆ        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n   983ÔåÆ\n   984ÔåÆ        logger.info(f\"Batch {batch_start//batch_size + 1}: patients {batch_start} to {batch_end}\")\n   985ÔåÆ\n   986ÔåÆ        # Build patient IN clause\n   987ÔåÆ        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n   988ÔåÆ\n   989ÔåÆ        # Query to find all matching SNOMED codes for these patients\n   990ÔåÆ        # We'll get all matches and pick the most recent per patient in Python\n   991ÔåÆ        query = f'''\n   992ÔåÆ            SELECT\n   993ÔåÆ                \"PatientPseudonym\",\n   994ÔåÆ                \"SNOMEDCode\",\n   995ÔåÆ                \"EventDateTime\"\n   996ÔåÆ            FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n   997ÔåÆ            WHERE \"PatientPseudonym\" IN ({patient_placeholders})\n   998ÔåÆ              AND \"SNOMEDCode\" IN ({snomed_placeholders})\n   999ÔåÆ            ORDER BY \"PatientPseudonym\", \"EventDateTime\" DESC\n  1000ÔåÆ        '''\n  1001ÔåÆ\n  1002ÔåÆ        params = tuple(batch_pseudonyms) + tuple(snomed_list)\n  1003ÔåÆ\n  1004ÔåÆ        try:\n  1005ÔåÆ            results = connector.execute_dict(query, params)\n  1006ÔåÆ\n  1007ÔåÆ            # Process results - pick most recent per patient\n  1008ÔåÆ            for row in results:\n  1009ÔåÆ                person_key = row.get(\"PatientPseudonym\")\n  1010ÔåÆ                snomed_code = row.get(\"SNOMEDCode\")\n  1011ÔåÆ                event_date = row.get(\"EventDateTime\")\n  1012ÔåÆ\n  1013ÔåÆ                if person_key and snomed_code:\n  1014ÔåÆ                    # Keep only if we haven't seen this patient yet (first = most recent due to ORDER BY)\n  1015ÔåÆ                    if person_key not in gp_matches:\n  1016ÔåÆ                        gp_matches[person_key] = (snomed_code, event_date)\n  1017ÔåÆ\n  1018ÔåÆ        except Exception as e:\n  1019ÔåÆ            logger.error(f\"Error querying GP records for batch: {e}\")\n  1020ÔåÆ            # Continue with other batches\n  1021ÔåÆ\n  1022ÔåÆ    logger.info(f\"Found GP matches for {len(gp_matches)} patients\")\n  1023ÔåÆ\n  1024ÔåÆ    # Step 5: Build result DataFrame\n  1025ÔåÆ    # For each unique_pair, determine Indication_Group based on match status\n  1026ÔåÆ    results_list = []\n  1027ÔåÆ\n  1028ÔåÆ    # We need to dedupe by UPID - a patient might be on multiple drugs\n  1029ÔåÆ    # Strategy: For each UPID, use the most recent match (if any)\n  1030ÔåÆ    upid_to_match: dict[str, tuple[str, str]] = {}  # UPID -> (Indication_Group, Source)\n  1031ÔåÆ\n  1032ÔåÆ    for _, row in unique_pairs.iterrows():\n  1033ÔåÆ        upid = row['UPID']\n  1034ÔåÆ        drug_name = row['Drug Name']\n  1035ÔåÆ        patient_pseudonym = row['PseudoNHSNoLinked']\n  1036ÔåÆ        directory = row['Directory']\n  1037ÔåÆ\n  1038ÔåÆ        # Check if patient has GP match (using PseudoNHSNoLinked which matches PatientPseudonym in GP)\n  1039ÔåÆ        if patient_pseudonym in gp_matches:\n  1040ÔåÆ            matched_snomed, event_date = gp_matches[patient_pseudonym]\n  1041ÔåÆ\n  1042ÔåÆ            # Find the search_term for this SNOMED code and drug\n  1043ÔåÆ            # (A SNOMED code might map to multiple drugs with different search_terms)\n  1044ÔåÆ            if matched_snomed in snomed_to_drug_searchterm:\n  1045ÔåÆ                # Look for match with current drug first\n  1046ÔåÆ                search_term = None\n  1047ÔåÆ                for drug, st, pd in snomed_to_drug_searchterm[matched_snomed]:\n  1048ÔåÆ                    if drug.upper() == drug_name.upper():\n  1049ÔåÆ                        search_term = st\n  1050ÔåÆ                        break\n  1051ÔåÆ                # If no drug-specific match, use any match\n  1052ÔåÆ                if search_term is None:\n  1053ÔåÆ                    search_term = snomed_to_drug_searchterm[matched_snomed][0][1]\n  1054ÔåÆ\n  1055ÔåÆ                # Only update if we don't have a match for this UPID yet\n  1056ÔåÆ                if upid not in upid_to_match:\n  1057ÔåÆ                    upid_to_match[upid] = (search_term, \"DIAGNOSIS\")\n  1058ÔåÆ            else:\n  1059ÔåÆ                # Shouldn't happen but fallback just in case\n  1060ÔåÆ                if upid not in upid_to_match:\n  1061ÔåÆ                    upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n  1062ÔåÆ        else:\n  1063ÔåÆ            # No GP match - use fallback\n  1064ÔåÆ            if upid not in upid_to_match:\n  1065ÔåÆ                upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n  1066ÔåÆ\n  1067ÔåÆ    # Build result DataFrame\n  1068ÔåÆ    for upid, (indication_group, source) in upid_to_match.items():\n  1069ÔåÆ        results_list.append({\n  1070ÔåÆ            'UPID': upid,\n  1071ÔåÆ            'Indication_Group': indication_group,\n  1072ÔåÆ            'Source': source,\n  1073ÔåÆ        })\n  1074ÔåÆ\n  1075ÔåÆ    result_df = pd.DataFrame(results_list)\n  1076ÔåÆ\n  1077ÔåÆ    # Log statistics\n  1078ÔåÆ    diagnosis_count = len([s for s in result_df['Source'] if s == \"DIAGNOSIS\"])\n  1079ÔåÆ    fallback_count = len([s for s in result_df['Source'] if s == \"FALLBACK\"])\n  1080ÔåÆ    total = len(result_df)\n  1081ÔåÆ\n  1082ÔåÆ    logger.info(f\"Indication lookup complete:\")\n  1083ÔåÆ    logger.info(f\"  Total unique patients: {total}\")\n  1084ÔåÆ    logger.info(f\"  DIAGNOSIS matches: {diagnosis_count} ({100*diagnosis_count/total:.1f}%)\")\n  1085ÔåÆ    logger.info(f\"  FALLBACK (no GP match): {fallback_count} ({100*fallback_count/total:.1f}%)\")\n  1086ÔåÆ\n  1087ÔåÆ    return result_df\n  1088ÔåÆ\n  1089ÔåÆ\n  1090ÔåÆ# === Drug-to-indication mapping from DimSearchTerm.csv ===\n  1091ÔåÆ\n  1092ÔåÆ\n  1093ÔåÆ# Merge related Search_Terms into canonical names.\n  1094ÔåÆ# Asthma variants are clinically the same condition at different severity levels.\n  1095ÔåÆ# Urticaria is a separate condition ÔÇö do NOT merge with asthma.\n  1096ÔåÆSEARCH_TERM_MERGE_MAP: dict[str, str] = {\n  1097ÔåÆ    \"allergic asthma\": \"asthma\",\n  1098ÔåÆ    \"severe persistent allergic asthma\": \"asthma\",\n  1099ÔåÆ}\n  1100ÔåÆ\n  1101ÔåÆ\n  1102ÔåÆdef load_drug_indication_mapping(\n  1103ÔåÆ    csv_path: Optional[str] = None,\n  1104ÔåÆ) -> tuple[dict[str, list[str]], dict[str, list[str]]]:\n  1105ÔåÆ    \"\"\"\n  1106ÔåÆ    Load the drug-to-Search_Term mapping from DimSearchTerm.csv.\n  1107ÔåÆ\n  1108ÔåÆ    Builds two lookup dicts:\n  1109ÔåÆ    - fragment_to_search_terms: drug fragment (UPPERCASE) -> list of Search_Terms containing it\n  1110ÔåÆ    - search_term_to_fragments: search_term -> list of drug fragments (UPPERCASE)\n  1111ÔåÆ\n  1112ÔåÆ    DimSearchTerm.csv has columns: Search_Term, CleanedDrugName, PrimaryDirectorate\n  1113ÔåÆ    CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\").\n  1114ÔåÆ\n  1115ÔåÆ    Note: A Search_Term can appear multiple times with different PrimaryDirectorates\n  1116ÔåÆ    (e.g., \"diabetes\" appears under both DIABETIC MEDICINE and OPHTHALMOLOGY).\n  1117ÔåÆ    Drug fragments from all rows for the same Search_Term are combined.\n  1118ÔåÆ\n  1119ÔåÆ    Asthma-related Search_Terms (\"allergic asthma\", \"severe persistent allergic asthma\")\n  1120ÔåÆ    are merged into \"asthma\" to match the CLUSTER_MAPPING_SQL normalization.\n  1121ÔåÆ    \"urticaria\" stays separate.\n  1122ÔåÆ\n  1123ÔåÆ    Args:\n  1124ÔåÆ        csv_path: Path to DimSearchTerm.csv. Defaults to data/DimSearchTerm.csv.\n  1125ÔåÆ\n  1126ÔåÆ    Returns:\n  1127ÔåÆ        Tuple of (fragment_to_search_terms, search_term_to_fragments)\n  1128ÔåÆ    \"\"\"\n  1129ÔåÆ    if csv_path is None:\n  1130ÔåÆ        csv_path = str(Path(__file__).parent.parent / \"data\" / \"DimSearchTerm.csv\")\n  1131ÔåÆ\n  1132ÔåÆ    fragment_to_search_terms: dict[str, list[str]] = {}\n  1133ÔåÆ    search_term_to_fragments: dict[str, list[str]] = {}\n  1134ÔåÆ\n  1135ÔåÆ    try:\n  1136ÔåÆ        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n  1137ÔåÆ            reader = csv.DictReader(f)\n  1138ÔåÆ            for row in reader:\n  1139ÔåÆ                search_term = row.get(\"Search_Term\", \"\").strip()\n  1140ÔåÆ                drug_names_raw = row.get(\"CleanedDrugName\", \"\").strip()\n  1141ÔåÆ\n  1142ÔåÆ                # Normalize asthma variants to canonical \"asthma\"\n  1143ÔåÆ                search_term = SEARCH_TERM_MERGE_MAP.get(search_term, search_term)\n  1144ÔåÆ\n  1145ÔåÆ                if not search_term or not drug_names_raw:\n  1146ÔåÆ                    continue\n  1147ÔåÆ\n  1148ÔåÆ                fragments = [frag.strip().upper() for frag in drug_names_raw.split(\"|\") if frag.strip()]\n  1149ÔåÆ\n  1150ÔåÆ                # Build search_term -> fragments (accumulate for duplicate Search_Terms)\n  1151ÔåÆ                if search_term not in search_term_to_fragments:\n  1152ÔåÆ                    search_term_to_fragments[search_term] = []\n  1153ÔåÆ                for frag in fragments:\n  1154ÔåÆ                    if frag not in search_term_to_fragments[search_term]:\n  1155ÔåÆ                        search_term_to_fragments[search_term].append(frag)\n  1156ÔåÆ\n  1157ÔåÆ                # Build fragment -> search_terms\n  1158ÔåÆ                for frag in fragments:\n  1159ÔåÆ                    if frag not in fragment_to_search_terms:\n  1160ÔåÆ                        fragment_to_search_terms[frag] = []\n  1161ÔåÆ                    if search_term not in fragment_to_search_terms[frag]:\n  1162ÔåÆ                        fragment_to_search_terms[frag].append(search_term)\n  1163ÔåÆ\n  1164ÔåÆ        logger.info(\n  1165ÔåÆ            f\"Loaded drug-indication mapping: {len(search_term_to_fragments)} Search_Terms, \"\n  1166ÔåÆ            f\"{len(fragment_to_search_terms)} drug fragments\"\n  1167ÔåÆ        )\n  1168ÔåÆ\n  1169ÔåÆ    except FileNotFoundError:\n  1170ÔåÆ        logger.error(f\"DimSearchTerm.csv not found at {csv_path}\")\n  1171ÔåÆ    except Exception as e:\n  1172ÔåÆ        logger.error(f\"Error loading DimSearchTerm.csv: {e}\")\n  1173ÔåÆ\n  1174ÔåÆ    return fragment_to_search_terms, search_term_to_fragments\n  1175ÔåÆ\n  1176ÔåÆ\n  1177ÔåÆdef get_search_terms_for_drug(\n  1178ÔåÆ    drug_name: str,\n  1179ÔåÆ    search_term_to_fragments: dict[str, list[str]],\n  1180ÔåÆ) -> list[str]:\n  1181ÔåÆ    \"\"\"\n  1182ÔåÆ    Get all Search_Terms that list a given drug using substring matching.\n  1183ÔåÆ\n  1184ÔåÆ    Checks if any drug fragment from DimSearchTerm is a SUBSTRING of the given\n  1185ÔåÆ    drug name (case-insensitive). This handles both exact matches (ADALIMUMAB)\n  1186ÔåÆ    and partial fragments (PEGYLATED, INHALED).\n  1187ÔåÆ\n  1188ÔåÆ    Args:\n  1189ÔåÆ        drug_name: HCD drug name (e.g., \"ADALIMUMAB 40MG\", \"PEGYLATED LIPOSOMAL DOXORUBICIN\")\n  1190ÔåÆ        search_term_to_fragments: Mapping of search_term -> list of drug fragments\n  1191ÔåÆ\n  1192ÔåÆ    Returns:\n  1193ÔåÆ        List of Search_Terms whose drug fragments match the drug name\n  1194ÔåÆ    \"\"\"\n  1195ÔåÆ    drug_name_upper = drug_name.upper()\n  1196ÔåÆ    matched_terms: list[str] = []\n  1197ÔåÆ\n  1198ÔåÆ    for search_term, fragments in search_term_to_fragments.items():\n  1199ÔåÆ        for frag in fragments:\n  1200ÔåÆ            if frag in drug_name_upper:\n  1201ÔåÆ                matched_terms.append(search_term)\n  1202ÔåÆ                break  # One matching fragment is enough for this Search_Term\n  1203ÔåÆ\n  1204ÔåÆ    return matched_terms\n  1205ÔåÆ\n  1206ÔåÆ\n  1207ÔåÆ# === NEW APPROACH: Query Snowflake directly using cluster CTE ===\n  1208ÔåÆ\n  1209ÔåÆ# The cluster query mapping (embedded from snomed_indication_mapping_query.sql)\n  1210ÔåÆ# This maps Search_Term -> Cluster_ID for ~148 clinical conditions\n  1211ÔåÆCLUSTER_MAPPING_SQL = \"\"\"\n  1212ÔåÆWITH SearchTermClusters AS (\n  1213ÔåÆ    SELECT Search_Term, Cluster_ID FROM (VALUES\n  1214ÔåÆ        ('acute lymphoblastic leukaemia', 'HAEMCANMORPH_COD'),\n  1215ÔåÆ        ('acute myeloid leukaemia', 'C19HAEMCAN_COD'),\n  1216ÔåÆ        ('acute promyelocytic leukaemia', 'HAEMCANMORPH_COD'),\n  1217ÔåÆ        ('asthma', 'AST_COD'),\n  1218ÔåÆ        ('allergic rhinitis', 'MILDINTAST_COD'),\n  1219ÔåÆ        ('alzheimer''s disease', 'DEMALZ_COD'),\n  1220ÔåÆ        ('amyloidosis', 'AMYLOID_COD'),\n  1221ÔåÆ        ('anaemia', 'eFI2_AnaemiaTimeSensitive'),\n  1222ÔåÆ        ('anaplastic large cell lymphoma', 'C19HAEMCAN_COD'),\n  1223ÔåÆ        ('apixaban', 'DOACCON_COD'),\n  1224ÔåÆ        ('aplastic anaemia', 'eFI2_AnaemiaEver'),\n  1225ÔåÆ        ('arthritis', 'eFI2_InflammatoryArthritis'),\n  1226ÔåÆ        ('asthma', 'eFI2_Asthma'),\n  1227ÔåÆ        ('atopic dermatitis', 'ATOPDERM_COD'),\n  1228ÔåÆ        ('atrial fibrillation', 'eFI2_AtrialFibrillation'),\n  1229ÔåÆ        ('attention deficit hyperactivity disorder', 'ADHD_COD'),\n  1230ÔåÆ        ('bipolar disorder', 'MH_COD'),\n  1231ÔåÆ        ('bladder', 'eFI2_UrinaryIncontinence'),\n  1232ÔåÆ        ('breast cancer', 'BRCANSCR_COD'),\n  1233ÔåÆ        ('cardiomyopathy', 'eFI2_HarmfulDrinking'),\n  1234ÔåÆ        ('cardiovascular disease', 'CVDRISKASS_COD'),\n  1235ÔåÆ        ('cervical cancer', 'CSDEC_COD'),\n  1236ÔåÆ        ('cholangiocarcinoma', 'eFI2_Cancer'),\n  1237ÔåÆ        ('chronic kidney disease', 'CKD_COD'),\n  1238ÔåÆ        ('chronic liver disease', 'eFI2_LiverProblems'),\n  1239ÔåÆ        ('chronic lymphocytic leukaemia', 'EPPHAEMCAN_COD'),\n  1240ÔåÆ        ('chronic myeloid leukaemia', 'EPPHAEMCAN_COD'),\n  1241ÔåÆ        ('chronic obstructive pulmonary disease', 'eFI2_COPD'),\n  1242ÔåÆ        ('colon cancer', 'eFI2_Cancer'),\n  1243ÔåÆ        ('colorectal cancer', 'GICANREF_COD'),\n  1244ÔåÆ        ('constipation', 'CHRONCONSTIP_COD'),\n  1245ÔåÆ        ('covid-19', 'POSSPOSTCOVID_COD'),\n  1246ÔåÆ        ('crohn''s disease', 'eFI2_InflammatoryBowelDisease'),\n  1247ÔåÆ        ('cutaneous t-cell lymphoma', 'C19HAEMCAN_COD'),\n  1248ÔåÆ        ('cystic fibrosis', 'CUST_ICB_CYSTIC_FIBROSIS'),\n  1249ÔåÆ        ('deep vein thrombosis', 'VTE_COD'),\n  1250ÔåÆ        ('depression', 'eFI2_Depression'),\n  1251ÔåÆ        ('diabetes', 'eFI2_DiabetesEver'),\n  1252ÔåÆ        ('diabetic retinopathy', 'DRSELIGIBILITY_COD'),\n  1253ÔåÆ        ('diffuse large b-cell lymphoma', 'C19HAEMCAN_COD'),\n  1254ÔåÆ        ('dravet syndrome', 'EPIL_COD'),\n  1255ÔåÆ        ('drug misuse', 'ILLSUBINT_COD'),\n  1256ÔåÆ        ('dyspepsia', 'eFI2_AbdominalPain'),\n  1257ÔåÆ        ('epilepsy', 'eFI2_Seizures'),\n  1258ÔåÆ        ('fallopian tube', 'STERIL_COD'),\n  1259ÔåÆ        ('follicular lymphoma', 'C19HAEMCAN_COD'),\n  1260ÔåÆ        ('gastric cancer', 'eFI2_Cancer'),\n  1261ÔåÆ        ('giant cell arteritis', 'GCA_COD'),\n  1262ÔåÆ        ('glioma', 'NHAEMCANMORPH_COD'),\n  1263ÔåÆ        ('gout', 'eFI2_InflammatoryArthritis'),\n  1264ÔåÆ        ('graft versus host disease', 'GVHD_COD'),\n  1265ÔåÆ        ('granulomatosis with polyangiitis', 'WEGENERVASC_COD'),\n  1266ÔåÆ        ('growth hormone deficiency', 'HYPOPITUITARY_COD'),\n  1267ÔåÆ        ('hand eczema', 'ECZEMA_COD'),\n  1268ÔåÆ        ('heart failure', 'eFI2_HeartFailure'),\n  1269ÔåÆ        ('hepatitis b', 'HEPBCVAC_COD'),\n  1270ÔåÆ        ('hepatocellular carcinoma', 'eFI2_Cancer'),\n  1271ÔåÆ        ('hiv', 'PREFLANG_COD'),\n  1272ÔåÆ        ('hodgkin lymphoma', 'HAEMCANMORPH_COD'),\n  1273ÔåÆ        ('hormone receptor', 'eFI2_ThyroidProblems'),\n  1274ÔåÆ        ('hypercholesterolaemia', 'CLASSFH_COD'),\n  1275ÔåÆ        ('immune thrombocytopenia', 'ITP_COD'),\n  1276ÔåÆ        ('influenza', 'FLUINVITE_COD'),\n  1277ÔåÆ        ('insomnia', 'eFI2_SleepProblems'),\n  1278ÔåÆ        ('irritable bowel syndrome', 'IBS_COD'),\n  1279ÔåÆ        ('ischaemic stroke', 'OSTR_COD'),\n  1280ÔåÆ        ('juvenile idiopathic arthritis', 'RARTHAD_COD'),\n  1281ÔåÆ        ('kidney transplant', 'RENALTRANSP_COD'),\n  1282ÔåÆ        ('leukaemia', 'eFI2_Cancer'),\n  1283ÔåÆ        ('lung cancer', 'FTCANREF_COD'),\n  1284ÔåÆ        ('lymphoma', 'C19HAEMCAN_COD'),\n  1285ÔåÆ        ('macular degeneration', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n  1286ÔåÆ        ('macular oedema', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n  1287ÔåÆ        ('major depressive episodes', 'eFI2_Depression'),\n  1288ÔåÆ        ('malignant melanoma', 'eFI2_Cancer'),\n  1289ÔåÆ        ('malignant pleural mesothelioma', 'LUNGCAN_COD'),\n  1290ÔåÆ        ('manic episode', 'MH_COD'),\n  1291ÔåÆ        ('mantle cell lymphoma', 'HAEMCANMORPH_COD'),\n  1292ÔåÆ        ('melanoma', 'eFI2_Cancer'),\n  1293ÔåÆ        ('merkel cell carcinoma', 'C19CAN_COD'),\n  1294ÔåÆ        ('migraine', 'eFI2_Headache'),\n  1295ÔåÆ        ('motor neurone disease', 'MND_COD'),\n  1296ÔåÆ        ('multiple myeloma', 'C19HAEMCAN_COD'),\n  1297ÔåÆ        ('multiple sclerosis', 'MS_COD'),\n  1298ÔåÆ        ('myelodysplastic', 'eFI2_AnaemiaEver'),\n  1299ÔåÆ        ('myelofibrosis', 'MDS_COD'),\n  1300ÔåÆ        ('myocardial infarction', 'eFI2_IschaemicHeartDisease'),\n  1301ÔåÆ        ('myotonia', 'CNDATRISK2_COD'),\n  1302ÔåÆ        ('narcolepsy', 'LD_COD'),\n  1303ÔåÆ        ('neuroendocrine tumour', 'LUNGCAN_COD'),\n  1304ÔåÆ        ('non-small cell lung cancer', 'LUNGCAN_COD'),\n  1305ÔåÆ        ('non-small-cell lung cancer', 'FTCANREF_COD'),\n  1306ÔåÆ        ('obesity', 'BMI30_COD'),\n  1307ÔåÆ        ('osteoarthritis', 'CUST_ICB_OSTEOARTHRITIS'),\n  1308ÔåÆ        ('osteoporosis', 'eFI2_Osteoporosis'),\n  1309ÔåÆ        ('osteosarcoma', 'NHAEMCANMORPH_COD'),\n  1310ÔåÆ        ('ovarian cancer', 'C19CAN_COD'),\n  1311ÔåÆ        ('peripheral arterial disease', 'PADEXC_COD'),\n  1312ÔåÆ        ('plaque psoriasis', 'PSORIASIS_COD'),\n  1313ÔåÆ        ('polycystic kidney disease', 'EPPCONGMALF_COD'),\n  1314ÔåÆ        ('polycythaemia vera', 'C19HAEMCAN_COD'),\n  1315ÔåÆ        ('pregnancy', 'C19PREG_COD'),\n  1316ÔåÆ        ('primary biliary cholangitis', 'eFI2_LiverProblems'),\n  1317ÔåÆ        ('primary hypercholesterolaemia', 'FNFHYP_COD'),\n  1318ÔåÆ        ('prostate cancer', 'EPPSOLIDCAN_COD'),\n  1319ÔåÆ        ('psoriasis', 'PSORIASIS_COD'),\n  1320ÔåÆ        ('psoriatic arthritis', 'RARTHAD_COD'),\n  1321ÔåÆ        ('pulmonary embolism', 'eFI2_RespiratoryDiseaseTimeSensitive'),\n  1322ÔåÆ        ('pulmonary fibrosis', 'ILD_COD'),\n  1323ÔåÆ        ('relapsing multiple sclerosis', 'MS_COD'),\n  1324ÔåÆ        ('renal cell carcinoma', 'C19CAN_COD'),\n  1325ÔåÆ        ('renal transplantation', 'RENALTRANSP_COD'),\n  1326ÔåÆ        ('retinal vein occlusion', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n  1327ÔåÆ        ('rheumatoid arthritis', 'eFI2_InflammatoryArthritis'),\n  1328ÔåÆ        ('rivaroxaban', 'DOACCON_COD'),\n  1329ÔåÆ        ('schizophrenia', 'MH_COD'),\n  1330ÔåÆ        ('seizures', 'LSZFREQ_COD'),\n  1331ÔåÆ        ('sepsis', 'C19ACTIVITY_COD'),\n  1332ÔåÆ        ('asthma', 'SEVAST_COD'),\n  1333ÔåÆ        ('sickle cell disease', 'SICKLE_COD'),\n  1334ÔåÆ        ('sleep apnoea', 'CUST_ICB_NON_SEVERE_LDA'),\n  1335ÔåÆ        ('smoking cessation', 'SMOKINGINT_COD'),\n  1336ÔåÆ        ('soft tissue sarcoma', 'NHAEMCANMORPH_COD'),\n  1337ÔåÆ        ('spinal muscular atrophy', 'MND_COD'),\n  1338ÔåÆ        ('squamous cell', 'C19CAN_COD'),\n  1339ÔåÆ        ('squamous cell carcinoma', 'C19CAN_COD'),\n  1340ÔåÆ        ('stem cell transplant', 'ALLOTRANSP_COD'),\n  1341ÔåÆ        ('stroke', 'eFI2_Stroke'),\n  1342ÔåÆ        ('systemic lupus erythematosus', 'SLUPUS_COD'),\n  1343ÔåÆ        ('systemic mastocytosis', 'HAEMCANMORPH_COD'),\n  1344ÔåÆ        ('thrombocytopenic purpura', 'TTP_COD'),\n  1345ÔåÆ        ('thrombotic thrombocytopenic purpura', 'TTP_COD'),\n  1346ÔåÆ        ('thyroid cancer', 'C19CAN_COD'),\n  1347ÔåÆ        ('tophaceous gout', 'CUST_ICB_OSTEOARTHRITIS'),\n  1348ÔåÆ        ('transitional cell carcinoma', 'C19CAN_COD'),\n  1349ÔåÆ        ('type 1 diabetes', 'DMTYPE1_COD'),\n  1350ÔåÆ        ('type 2 diabetes', 'DMTYPE2_COD'),\n  1351ÔåÆ        ('ulcerative colitis', 'eFI2_InflammatoryBowelDisease'),\n  1352ÔåÆ        ('urothelial carcinoma', 'NHAEMCANMORPH_COD'),\n  1353ÔåÆ        ('urticaria', 'XSAL_COD'),\n  1354ÔåÆ        ('uveitis', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n  1355ÔåÆ        ('vascular disease', 'CVDINVITE_COD'),\n  1356ÔåÆ        ('vasculitis', 'CRYOGLOBVASC_COD')\n  1357ÔåÆ    ) AS t(Search_Term, Cluster_ID)\n  1358ÔåÆ),\n  1359ÔåÆ\n  1360ÔåÆClusterCodes AS (\n  1361ÔåÆ    SELECT\n  1362ÔåÆ        stc.Search_Term,\n  1363ÔåÆ        c.\"SNOMEDCode\",\n  1364ÔåÆ        c.\"SNOMEDDescription\"\n  1365ÔåÆ    FROM SearchTermClusters stc\n  1366ÔåÆ    JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" c\n  1367ÔåÆ        ON stc.Cluster_ID = c.\"Cluster_ID\"\n  1368ÔåÆ    WHERE c.\"SNOMEDCode\" IS NOT NULL\n  1369ÔåÆ),\n  1370ÔåÆ\n  1371ÔåÆExplicitCodes AS (\n  1372ÔåÆ    SELECT Search_Term, SNOMEDCode, SNOMEDDescription FROM (VALUES\n  1373ÔåÆ        ('acute coronary syndrome', '837091000000100', 'Manual mapping'),\n  1374ÔåÆ        ('ankylosing spondylitis', '162930007', 'Manual mapping'),\n  1375ÔåÆ        ('ankylosing spondylitis', '239805001', 'Manual mapping'),\n  1376ÔåÆ        ('ankylosing spondylitis', '239810002', 'Manual mapping'),\n  1377ÔåÆ        ('ankylosing spondylitis', '239811003', 'Manual mapping'),\n  1378ÔåÆ        ('ankylosing spondylitis', '394990003', 'Manual mapping'),\n  1379ÔåÆ        ('ankylosing spondylitis', '429712009', 'Manual mapping'),\n  1380ÔåÆ        ('ankylosing spondylitis', '441562009', 'Manual mapping'),\n  1381ÔåÆ        ('ankylosing spondylitis', '441680005', 'Manual mapping'),\n  1382ÔåÆ        ('ankylosing spondylitis', '441930001', 'Manual mapping'),\n  1383ÔåÆ        ('axial spondyloarthritis', '723116002', 'Manual mapping'),\n  1384ÔåÆ        ('choroidal neovascularisation', '380621000000102', 'Manual mapping'),\n  1385ÔåÆ        ('choroidal neovascularisation', '733124000', 'Manual mapping')\n  1386ÔåÆ    ) AS t(Search_Term, SNOMEDCode, SNOMEDDescription)\n  1387ÔåÆ),\n  1388ÔåÆ\n  1389ÔåÆAllIndicationCodes AS (\n  1390ÔåÆ    SELECT Search_Term, \"SNOMEDCode\" AS SNOMEDCode, \"SNOMEDDescription\" AS SNOMEDDescription\n  1391ÔåÆ    FROM ClusterCodes\n  1392ÔåÆ    UNION ALL\n  1393ÔåÆ    SELECT Search_Term, SNOMEDCode, SNOMEDDescription\n  1394ÔåÆ    FROM ExplicitCodes\n  1395ÔåÆ)\n  1396ÔåÆ\"\"\"\n  1397ÔåÆ\n  1398ÔåÆ\n  1399ÔåÆdef get_patient_indication_groups(\n  1400ÔåÆ    patient_pseudonyms: list[str],\n  1401ÔåÆ    connector: Optional[SnowflakeConnector] = None,\n  1402ÔåÆ    batch_size: int = 500,\n  1403ÔåÆ    earliest_hcd_date: Optional[str] = None,\n  1404ÔåÆ) -> \"pd.DataFrame\":\n  1405ÔåÆ    \"\"\"\n  1406ÔåÆ    Batch lookup GP diagnosis-based indication groups using Snowflake cluster query.\n  1407ÔåÆ\n  1408ÔåÆ    Returns ALL matching Search_Terms per patient with code_frequency (count of\n  1409ÔåÆ    matching SNOMED codes). This enables drug-aware indication matching where\n  1410ÔåÆ    each drug is cross-referenced against the patient's GP diagnoses.\n  1411ÔåÆ\n  1412ÔåÆ    The query:\n  1413ÔåÆ    1. Uses the cluster mapping CTE to get all Search_Term -> SNOMED code mappings\n  1414ÔåÆ    2. Joins with PrimaryCareClinicalCoding to find patients with matching codes\n  1415ÔåÆ    3. Groups by patient + Search_Term and counts matching codes (code_frequency)\n  1416ÔåÆ    4. Optionally restricts to GP codes from earliest_hcd_date onwards\n  1417ÔåÆ\n  1418ÔåÆ    Args:\n  1419ÔåÆ        patient_pseudonyms: List of PseudoNHSNoLinked values (matches PatientPseudonym in GP records)\n  1420ÔåÆ        connector: Optional SnowflakeConnector (defaults to singleton)\n  1421ÔåÆ        batch_size: Number of patients per Snowflake query batch (default 500)\n  1422ÔåÆ        earliest_hcd_date: Optional ISO date string (YYYY-MM-DD). If provided, only\n  1423ÔåÆ            counts GP codes from this date onwards. Should be MIN(Intervention Date)\n  1424ÔåÆ            from the HCD DataFrame to restrict to the HCD data window.\n  1425ÔåÆ\n  1426ÔåÆ    Returns:\n  1427ÔåÆ        DataFrame with columns:\n  1428ÔåÆ        - PatientPseudonym: The patient identifier (PseudoNHSNoLinked value)\n  1429ÔåÆ        - Search_Term: The matched indication (e.g., \"rheumatoid arthritis\")\n  1430ÔåÆ        - code_frequency: Count of matching SNOMED codes for this Search_Term\n  1431ÔåÆ\n  1432ÔåÆ        Multiple rows per patient (one per matched Search_Term).\n  1433ÔåÆ        Patients not found in results have no matching GP diagnosis.\n  1434ÔåÆ    \"\"\"\n  1435ÔåÆ    import pandas as pd\n  1436ÔåÆ\n  1437ÔåÆ    logger.info(f\"Starting Snowflake-direct indication lookup for {len(patient_pseudonyms)} patients...\")\n  1438ÔåÆ    if earliest_hcd_date:\n  1439ÔåÆ        logger.info(f\"  Restricting GP codes to >= {earliest_hcd_date}\")\n  1440ÔåÆ\n  1441ÔåÆ    # Handle edge case: empty patient list\n  1442ÔåÆ    if not patient_pseudonyms:\n  1443ÔåÆ        logger.warning(\"Empty patient list provided\")\n  1444ÔåÆ        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n  1445ÔåÆ\n  1446ÔåÆ    # Check Snowflake availability\n  1447ÔåÆ    if not SNOWFLAKE_AVAILABLE:\n  1448ÔåÆ        logger.error(\"Snowflake connector not available - cannot lookup GP records\")\n  1449ÔåÆ        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n  1450ÔåÆ\n  1451ÔåÆ    if not is_snowflake_configured():\n  1452ÔåÆ        logger.error(\"Snowflake not configured - cannot lookup GP records\")\n  1453ÔåÆ        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n  1454ÔåÆ\n  1455ÔåÆ    if connector is None:\n  1456ÔåÆ        connector = get_connector()\n  1457ÔåÆ\n  1458ÔåÆ    # Results list to collect all matches\n  1459ÔåÆ    all_results: list[dict] = []\n  1460ÔåÆ\n  1461ÔåÆ    # Process patients in batches\n  1462ÔåÆ    total_patients = len(patient_pseudonyms)\n  1463ÔåÆ    for batch_start in range(0, total_patients, batch_size):\n  1464ÔåÆ        batch_end = min(batch_start + batch_size, total_patients)\n  1465ÔåÆ        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n  1466ÔåÆ        batch_num = batch_start // batch_size + 1\n  1467ÔåÆ        total_batches = (total_patients + batch_size - 1) // batch_size\n  1468ÔåÆ\n  1469ÔåÆ        logger.info(f\"Batch {batch_num}/{total_batches}: patients {batch_start + 1} to {batch_end}\")\n  1470ÔåÆ\n  1471ÔåÆ        # Build patient IN clause placeholders\n  1472ÔåÆ        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n  1473ÔåÆ\n  1474ÔåÆ        # Build WHERE clause with optional date filter\n  1475ÔåÆ        date_filter = \"\"\n  1476ÔåÆ        if earliest_hcd_date:\n  1477ÔåÆ            date_filter = f\"\\n    AND pc.\\\"EventDateTime\\\" >= %s\"\n  1478ÔåÆ\n  1479ÔåÆ        # Build the full query with cluster CTE\n  1480ÔåÆ        # Returns ALL matching Search_Terms per patient with code_frequency\n  1481ÔåÆ        # code_frequency = COUNT of matching SNOMED codes per Search_Term per patient\n  1482ÔåÆ        query = f\"\"\"\n  1483ÔåÆ{CLUSTER_MAPPING_SQL}\n  1484ÔåÆSELECT\n  1485ÔåÆ    pc.\"PatientPseudonym\" AS \"PatientPseudonym\",\n  1486ÔåÆ    aic.Search_Term AS \"Search_Term\",\n  1487ÔåÆ    COUNT(*) AS \"code_frequency\"\n  1488ÔåÆFROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\n  1489ÔåÆINNER JOIN AllIndicationCodes aic\n  1490ÔåÆ    ON pc.\"SNOMEDCode\" = aic.SNOMEDCode\n  1491ÔåÆWHERE pc.\"PatientPseudonym\" IN ({patient_placeholders}){date_filter}\n  1492ÔåÆGROUP BY pc.\"PatientPseudonym\", aic.Search_Term\n  1493ÔåÆ\"\"\"\n  1494ÔåÆ\n  1495ÔåÆ        # Build params: patient pseudonyms + optional date\n  1496ÔåÆ        params = list(batch_pseudonyms)\n  1497ÔåÆ        if earliest_hcd_date:\n  1498ÔåÆ            params.append(earliest_hcd_date)\n  1499ÔåÆ\n  1500ÔåÆ        try:\n  1501ÔåÆ            results = connector.execute_dict(query, tuple(params))\n  1502ÔåÆ\n  1503ÔåÆ            for row in results:\n  1504ÔåÆ                all_results.append({\n  1505ÔåÆ                    'PatientPseudonym': row.get('PatientPseudonym'),\n  1506ÔåÆ                    'Search_Term': row.get('Search_Term'),\n  1507ÔåÆ                    'code_frequency': row.get('code_frequency', 0),\n  1508ÔåÆ                })\n  1509ÔåÆ\n  1510ÔåÆ            logger.debug(f\"Batch {batch_num}: found {len(results)} patient-indication matches\")\n  1511ÔåÆ\n  1512ÔåÆ        except Exception as e:\n  1513ÔåÆ            logger.error(f\"Error querying GP records for batch {batch_num}: {e}\")\n  1514ÔåÆ            # Continue with other batches - partial results are better than none\n  1515ÔåÆ\n  1516ÔåÆ    # Build result DataFrame\n  1517ÔåÆ    result_df = pd.DataFrame(all_results)\n  1518ÔåÆ\n  1519ÔåÆ    # Log summary statistics\n  1520ÔåÆ    if len(result_df) > 0:\n  1521ÔåÆ        unique_patients = result_df['PatientPseudonym'].nunique()\n  1522ÔåÆ        total_rows = len(result_df)\n  1523ÔåÆ        match_rate = 100 * unique_patients / total_patients\n  1524ÔåÆ        unique_terms = result_df['Search_Term'].nunique()\n  1525ÔåÆ        avg_indications = total_rows / unique_patients if unique_patients > 0 else 0\n  1526ÔåÆ        logger.info(f\"Indication lookup complete:\")\n  1527ÔåÆ        logger.info(f\"  Total patients queried: {total_patients}\")\n  1528ÔåÆ        logger.info(f\"  Patients with GP match: {unique_patients} ({match_rate:.1f}%)\")\n  1529ÔåÆ        logger.info(f\"  Total patient-indication rows: {total_rows} (avg {avg_indications:.1f} per patient)\")\n  1530ÔåÆ        logger.info(f\"  Unique Search_Terms found: {unique_terms}\")\n  1531ÔåÆ\n  1532ÔåÆ        # Log top Search_Terms\n  1533ÔåÆ        top_terms = result_df['Search_Term'].value_counts().head(5)\n  1534ÔåÆ        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n  1535ÔåÆ    else:\n  1536ÔåÆ        logger.info(f\"Indication lookup complete: 0 matches from {total_patients} patients\")\n  1537ÔåÆ\n  1538ÔåÆ    return result_df\n  1539ÔåÆ\n  1540ÔåÆ\n  1541ÔåÆ# Export public API\n  1542ÔåÆ__all__ = [\n  1543ÔåÆ    # Dataclasses\n  1544ÔåÆ    \"ClusterSnomedCodes\",\n  1545ÔåÆ    \"IndicationValidationResult\",\n  1546ÔåÆ    \"DrugIndicationMatchRate\",\n  1547ÔåÆ    \"DrugSnomedMapping\",\n  1548ÔåÆ    \"DirectSnomedMatchResult\",\n  1549ÔåÆ    \"DirectorateAssignment\",\n  1550ÔåÆ    # Cluster-based lookup functions (existing)\n  1551ÔåÆ    \"get_drug_clusters\",\n  1552ÔåÆ    \"get_drug_cluster_ids\",\n  1553ÔåÆ    \"get_cluster_snomed_codes\",\n  1554ÔåÆ    \"patient_has_indication\",\n  1555ÔåÆ    \"validate_indication\",\n  1556ÔåÆ    \"get_indication_match_rate\",\n  1557ÔåÆ    \"batch_validate_indications\",\n  1558ÔåÆ    \"get_available_clusters\",\n  1559ÔåÆ    # Direct SNOMED lookup functions (new)\n  1560ÔåÆ    \"get_drug_snomed_codes\",\n  1561ÔåÆ    \"patient_has_indication_direct\",\n  1562ÔåÆ    # Diagnosis-based directorate assignment\n  1563ÔåÆ    \"get_directorate_from_diagnosis\",\n  1564ÔåÆ    # Batch lookup for indication groups\n  1565ÔåÆ    \"batch_lookup_indication_groups\",\n  1566ÔåÆ    # Drug-indication mapping from DimSearchTerm.csv\n  1567ÔåÆ    \"SEARCH_TERM_MERGE_MAP\",\n  1568ÔåÆ    \"load_drug_indication_mapping\",\n  1569ÔåÆ    \"get_search_terms_for_drug\",\n  1570ÔåÆ    # Snowflake-direct indication lookup (new approach)\n  1571ÔåÆ    \"get_patient_indication_groups\",\n  1572ÔåÆ    \"CLUSTER_MAPPING_SQL\",\n  1573ÔåÆ]\n  1574ÔåÆ\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"cffe6f08-ef04-42aa-9725-b7bda5124d8e","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","content":"\"\"\"\nDiagnosis lookup module for NHS Patient Pathway Analysis.\n\nProvides functions to validate patient indications by checking GP diagnosis records\nagainst SNOMED cluster codes. Uses the drug-to-cluster mapping from\ndrug_indication_clusters.csv and queries Snowflake for SNOMED codes and GP records.\n\nKey workflow:\n1. Get drug's valid indication clusters from local mapping\n2. Get all SNOMED codes for those clusters from Snowflake\n3. Check if patient has any of those SNOMED codes in GP records\n4. Report indication validation status\n\nIMPORTANT: HCD activity data indication codes are UNRELIABLE. This module uses\nGP/Primary Care data (PrimaryCareClinicalCoding) as the authoritative source.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom datetime import date, datetime\nfrom pathlib import Path\nfrom typing import Optional, Callable, Any, cast, TYPE_CHECKING\nimport csv\n\nif TYPE_CHECKING:\n    import pandas as pd\n\nfrom core.logging_config import get_logger\nfrom data_processing.database import DatabaseManager, default_db_manager\nfrom data_processing.snowflake_connector import (\n    SnowflakeConnector,\n    get_connector,\n    is_snowflake_available,\n    is_snowflake_configured,\n    SNOWFLAKE_AVAILABLE,\n)\nfrom data_processing.cache import get_cache, is_cache_enabled\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass ClusterSnomedCodes:\n    \"\"\"SNOMED codes for a clinical coding cluster.\"\"\"\n    cluster_id: str\n    cluster_description: str\n    snomed_codes: list[str] = field(default_factory=list)\n    snomed_descriptions: dict[str, str] = field(default_factory=dict)\n\n    @property\n    def code_count(self) -> int:\n        return len(self.snomed_codes)\n\n\n@dataclass\nclass IndicationValidationResult:\n    \"\"\"Result of validating a patient's indication for a drug.\"\"\"\n    patient_pseudonym: str\n    drug_name: str\n    has_valid_indication: bool\n    matched_cluster_id: Optional[str] = None\n    matched_snomed_code: Optional[str] = None\n    matched_snomed_description: Optional[str] = None\n    checked_clusters: list[str] = field(default_factory=list)\n    total_codes_checked: int = 0\n    source: str = \"GP_SNOMED\"  # GP_SNOMED | NONE\n    error_message: Optional[str] = None\n\n\n@dataclass\nclass DrugIndicationMatchRate:\n    \"\"\"Match rate statistics for a drug's indication validation.\"\"\"\n    drug_name: str\n    total_patients: int\n    patients_with_indication: int\n    patients_without_indication: int\n    match_rate: float  # 0.0 to 1.0\n    clusters_checked: list[str] = field(default_factory=list)\n    sample_unmatched: list[str] = field(default_factory=list)  # Sample patient IDs\n\n\n@dataclass\nclass DrugSnomedMapping:\n    \"\"\"SNOMED code mapping for a drug from ref_drug_snomed_mapping.\"\"\"\n    snomed_code: str\n    snomed_description: str\n    search_term: str\n    primary_directorate: str\n    indication: str = \"\"\n    ta_id: str = \"\"\n\n\n@dataclass\nclass DirectSnomedMatchResult:\n    \"\"\"Result of direct SNOMED code lookup in GP records.\"\"\"\n    patient_pseudonym: str\n    matched: bool\n    snomed_code: Optional[str] = None\n    snomed_description: Optional[str] = None\n    search_term: Optional[str] = None\n    primary_directorate: Optional[str] = None\n    event_date: Optional[datetime] = None\n    source: str = \"DIRECT_SNOMED\"  # DIRECT_SNOMED | NONE\n\n\n@dataclass\nclass DirectorateAssignment:\n    \"\"\"Result of directorate assignment for a patient-drug combination.\"\"\"\n    upid: str\n    drug_name: str\n    directorate: Optional[str]\n    search_term: Optional[str] = None\n    source: str = \"FALLBACK\"  # DIAGNOSIS | FALLBACK\n    snomed_code: Optional[str] = None\n    event_date: Optional[datetime] = None\n\n\ndef get_drug_clusters(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[dict]:\n    \"\"\"\n    Get all SNOMED cluster mappings for a drug from local SQLite.\n\n    Args:\n        drug_name: Drug name to look up (case-insensitive)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n\n    Returns:\n        List of dicts with keys: drug_name, indication, cluster_id,\n        cluster_description, nice_ta_reference\n    \"\"\"\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    query = \"\"\"\n        SELECT drug_name, indication, cluster_id, cluster_description, nice_ta_reference\n        FROM ref_drug_indication_clusters\n        WHERE UPPER(drug_name) = UPPER(?)\n        ORDER BY indication, cluster_id\n    \"\"\"\n\n    try:\n        with db_manager.get_connection() as conn:\n            cursor = conn.execute(query, (drug_name,))\n            rows = cursor.fetchall()\n\n            results = []\n            for row in rows:\n                results.append({\n                    \"drug_name\": row[\"drug_name\"],\n                    \"indication\": row[\"indication\"],\n                    \"cluster_id\": row[\"cluster_id\"],\n                    \"cluster_description\": row[\"cluster_description\"],\n                    \"nice_ta_reference\": row[\"nice_ta_reference\"],\n                })\n\n            logger.debug(f\"Found {len(results)} cluster mappings for drug '{drug_name}'\")\n            return results\n\n    except Exception as e:\n        logger.error(f\"Error getting clusters for drug '{drug_name}': {e}\")\n        return []\n\n\ndef get_drug_cluster_ids(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[str]:\n    \"\"\"\n    Get unique cluster IDs for a drug.\n\n    Args:\n        drug_name: Drug name to look up\n        db_manager: Optional DatabaseManager\n\n    Returns:\n        List of unique cluster IDs\n    \"\"\"\n    clusters = get_drug_clusters(drug_name, db_manager)\n    return list(set(c[\"cluster_id\"] for c in clusters))\n\n\ndef get_drug_snomed_codes(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[DrugSnomedMapping]:\n    \"\"\"\n    Get all SNOMED codes for a drug from local ref_drug_snomed_mapping table.\n\n    This uses the enriched mapping CSV data loaded into SQLite, which provides\n    direct SNOMED-to-drug mappings with Search_Term and PrimaryDirectorate.\n\n    Args:\n        drug_name: Drug name to look up (case-insensitive, matches cleaned_drug_name)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n\n    Returns:\n        List of DrugSnomedMapping with snomed_code, snomed_description,\n        search_term, primary_directorate, indication, ta_id\n    \"\"\"\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    query = \"\"\"\n        SELECT DISTINCT\n            snomed_code,\n            snomed_description,\n            search_term,\n            primary_directorate,\n            indication,\n            ta_id\n        FROM ref_drug_snomed_mapping\n        WHERE UPPER(cleaned_drug_name) = UPPER(?)\n           OR UPPER(drug_name) = UPPER(?)\n        ORDER BY search_term, snomed_code\n    \"\"\"\n\n    try:\n        with db_manager.get_connection() as conn:\n            cursor = conn.execute(query, (drug_name, drug_name))\n            rows = cursor.fetchall()\n\n            results = []\n            for row in rows:\n                results.append(DrugSnomedMapping(\n                    snomed_code=row[\"snomed_code\"],\n                    snomed_description=row[\"snomed_description\"] or \"\",\n                    search_term=row[\"search_term\"] or \"\",\n                    primary_directorate=row[\"primary_directorate\"] or \"\",\n                    indication=row[\"indication\"] or \"\",\n                    ta_id=row[\"ta_id\"] or \"\",\n                ))\n\n            logger.debug(f\"Found {len(results)} SNOMED mappings for drug '{drug_name}'\")\n            return results\n\n    except Exception as e:\n        logger.error(f\"Error getting SNOMED codes for drug '{drug_name}': {e}\")\n        return []\n\n\ndef patient_has_indication_direct(\n    patient_pseudonym: str,\n    drug_snomed_mappings: list[DrugSnomedMapping],\n    connector: Optional[SnowflakeConnector] = None,\n    before_date: Optional[date] = None,\n) -> DirectSnomedMatchResult:\n    \"\"\"\n    Check if patient has any of the SNOMED codes in their GP records.\n\n    This is the direct SNOMED lookup - it queries PrimaryCareClinicalCoding\n    for exact SNOMED code matches (not via cluster). Returns the most recent\n    match by EventDateTime if multiple matches exist.\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        drug_snomed_mappings: List of DrugSnomedMapping from get_drug_snomed_codes()\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        DirectSnomedMatchResult with match details (most recent by EventDateTime)\n    \"\"\"\n    result = DirectSnomedMatchResult(\n        patient_pseudonym=patient_pseudonym,\n        matched=False,\n        source=\"NONE\",\n    )\n\n    if not drug_snomed_mappings:\n        return result\n\n    if not SNOWFLAKE_AVAILABLE:\n        logger.warning(\"Snowflake connector not available\")\n        return result\n\n    if not is_snowflake_configured():\n        logger.warning(\"Snowflake not configured - cannot check GP records\")\n        return result\n\n    if connector is None:\n        connector = get_connector()\n\n    # Build lookup dict for mapping snomed_code -> (search_term, primary_directorate, snomed_description)\n    snomed_lookup = {\n        m.snomed_code: (m.search_term, m.primary_directorate, m.snomed_description)\n        for m in drug_snomed_mappings\n    }\n\n    # Get unique SNOMED codes\n    snomed_codes = list(snomed_lookup.keys())\n\n    # Build placeholders for SNOMED codes\n    placeholders = \", \".join([\"%s\"] * len(snomed_codes))\n\n    # Query to find most recent matching SNOMED code in GP records\n    query = f'''\n        SELECT\n            \"SNOMEDCode\",\n            \"EventDateTime\"\n        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n        WHERE \"PatientPseudonym\" = %s\n            AND \"SNOMEDCode\" IN ({placeholders})\n    '''\n\n    params: list = [patient_pseudonym] + snomed_codes\n\n    if before_date:\n        query += ' AND \"EventDateTime\" < %s'\n        params.append(before_date.isoformat())\n\n    query += ' ORDER BY \"EventDateTime\" DESC LIMIT 1'\n\n    try:\n        results = connector.execute_dict(query, tuple(params))\n\n        if results:\n            row = results[0]\n            matched_code = row.get(\"SNOMEDCode\")\n            event_dt = row.get(\"EventDateTime\")\n\n            if matched_code and matched_code in snomed_lookup:\n                search_term, primary_dir, snomed_desc = snomed_lookup[matched_code]\n\n                return DirectSnomedMatchResult(\n                    patient_pseudonym=patient_pseudonym,\n                    matched=True,\n                    snomed_code=matched_code,\n                    snomed_description=snomed_desc,\n                    search_term=search_term,\n                    primary_directorate=primary_dir,\n                    event_date=event_dt,\n                    source=\"DIRECT_SNOMED\",\n                )\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Error checking direct SNOMED for patient '{patient_pseudonym}': {e}\")\n        return result\n\n\ndef get_directorate_from_diagnosis(\n    upid: str,\n    drug_name: str,\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    before_date: Optional[date] = None,\n) -> DirectorateAssignment:\n    \"\"\"\n    Get directorate assignment for a patient-drug combination using diagnosis-based lookup.\n\n    This function attempts to assign a directorate based on the patient's GP records\n    (direct SNOMED code matching). If no match is found, it returns a FALLBACK result\n    indicating that the caller should use alternative assignment methods (e.g.,\n    department_identification() from tools/data.py).\n\n    Workflow:\n    1. Get all SNOMED codes for the drug from ref_drug_snomed_mapping\n    2. Query patient's GP records for matching SNOMED codes\n    3. If match found ÔåÆ return diagnosis-based directorate and search_term\n    4. If no match ÔåÆ return FALLBACK result (caller handles fallback logic)\n\n    Args:\n        upid: Patient's unique patient ID (Provider Code[:3] + PersonKey)\n        drug_name: Drug name to look up\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        DirectorateAssignment with directorate, search_term, and source\n    \"\"\"\n    result = DirectorateAssignment(\n        upid=upid,\n        drug_name=drug_name,\n        directorate=None,\n        source=\"FALLBACK\",\n    )\n\n    # Step 1: Get SNOMED codes for the drug\n    drug_snomed_mappings = get_drug_snomed_codes(drug_name, db_manager)\n\n    if not drug_snomed_mappings:\n        logger.debug(f\"No SNOMED mappings found for drug '{drug_name}' - using fallback\")\n        return result\n\n    # Step 2: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        logger.debug(\"Snowflake not available - using fallback\")\n        return result\n\n    if not is_snowflake_configured():\n        logger.debug(\"Snowflake not configured - using fallback\")\n        return result\n\n    # Step 3: Get patient pseudonym from UPID\n    # UPID format is Provider Code (3 chars) + PersonKey\n    # We need to query Snowflake to get the PatientPseudonym for this PersonKey\n    # However, patient_has_indication_direct expects PatientPseudonym, not UPID\n    # For now, we'll use UPID as the identifier - the actual integration\n    # will need to happen at the DataFrame level where we have PersonKey\n    #\n    # NOTE: This function will be called from the pipeline where we have\n    # access to PatientPseudonym. The UPID is passed for logging/tracking.\n\n    # Actually, looking at the pipeline, we need PatientPseudonym, not UPID.\n    # The caller should pass the PatientPseudonym or we need to look it up.\n    # For now, let's assume the caller will use this in a batch context\n    # where they can map UPID -> PatientPseudonym.\n\n    # Let me reconsider: the function signature takes UPID but we need\n    # PatientPseudonym for Snowflake. In the pipeline context (fetch_and_transform_data),\n    # we'll have the PersonKey column which IS the PatientPseudonym.\n    # So UPID = ProviderCode[:3] + PersonKey, and PersonKey = PatientPseudonym.\n    #\n    # We can extract PatientPseudonym from UPID by removing the first 3 chars.\n    patient_pseudonym = upid[3:] if len(upid) > 3 else upid\n\n    # Step 4: Check patient's GP records for matching SNOMED codes\n    match_result = patient_has_indication_direct(\n        patient_pseudonym=patient_pseudonym,\n        drug_snomed_mappings=drug_snomed_mappings,\n        connector=connector,\n        before_date=before_date,\n    )\n\n    if match_result.matched and match_result.primary_directorate:\n        return DirectorateAssignment(\n            upid=upid,\n            drug_name=drug_name,\n            directorate=match_result.primary_directorate,\n            search_term=match_result.search_term,\n            source=\"DIAGNOSIS\",\n            snomed_code=match_result.snomed_code,\n            event_date=match_result.event_date,\n        )\n\n    # No match found - return fallback result\n    return result\n\n\ndef get_cluster_snomed_codes(\n    cluster_id: str,\n    connector: Optional[SnowflakeConnector] = None,\n    use_cache: bool = True,\n) -> ClusterSnomedCodes:\n    \"\"\"\n    Get all SNOMED codes for a cluster from Snowflake.\n\n    Queries the ClinicalCodingClusterSnomedCodes table to get all SNOMED codes\n    that belong to the specified cluster.\n\n    Args:\n        cluster_id: Cluster ID to look up (e.g., 'RARTH_COD', 'PSORIASIS_COD')\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        use_cache: Whether to use cached results (default True)\n\n    Returns:\n        ClusterSnomedCodes with list of SNOMED codes and descriptions\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE:\n        logger.warning(\"Snowflake connector not available\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n    if not is_snowflake_configured():\n        logger.warning(\"Snowflake not configured - cannot get cluster codes\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n    # Check cache first\n    cache_key = f\"cluster_snomed_{cluster_id}\"\n    if use_cache and is_cache_enabled():\n        cache = get_cache()\n        cached = cache.get(cache_key)\n        if cached is not None and len(cached) > 0:\n            logger.debug(f\"Using cached SNOMED codes for cluster '{cluster_id}'\")\n            cached_dict = cached[0]  # First element is our data dict\n            return ClusterSnomedCodes(\n                cluster_id=cluster_id,\n                cluster_description=str(cached_dict.get(\"description\", \"\")),\n                snomed_codes=list(cached_dict.get(\"codes\", [])),\n                snomed_descriptions=dict(cached_dict.get(\"descriptions\", {})),\n            )\n\n    if connector is None:\n        connector = get_connector()\n\n    query = '''\n        SELECT DISTINCT\n            \"Cluster_ID\",\n            \"Cluster_Description\",\n            \"SNOMEDCode\",\n            \"SNOMEDDescription\"\n        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n        WHERE \"Cluster_ID\" = %s\n        ORDER BY \"SNOMEDCode\"\n    '''\n\n    try:\n        results = connector.execute_dict(query, (cluster_id,))\n\n        if not results:\n            logger.warning(f\"No SNOMED codes found for cluster '{cluster_id}'\")\n            return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n        codes = []\n        descriptions = {}\n        description = results[0].get(\"Cluster_Description\", \"\") if results else \"\"\n\n        for row in results:\n            code = row.get(\"SNOMEDCode\")\n            if code:\n                codes.append(code)\n                descriptions[code] = row.get(\"SNOMEDDescription\", \"\")\n\n        logger.info(f\"Found {len(codes)} SNOMED codes for cluster '{cluster_id}'\")\n\n        # Cache the results (using query-based cache with fake params)\n        if use_cache and is_cache_enabled():\n            cache = get_cache()\n            cache_data = [{\n                \"description\": description,\n                \"codes\": codes,\n                \"descriptions\": descriptions,\n            }]\n            cache.set(cache_key, None, cache_data)  # type: ignore[arg-type]\n\n        return ClusterSnomedCodes(\n            cluster_id=cluster_id,\n            cluster_description=description,\n            snomed_codes=codes,\n            snomed_descriptions=descriptions,\n        )\n\n    except Exception as e:\n        logger.error(f\"Error getting SNOMED codes for cluster '{cluster_id}': {e}\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n\ndef patient_has_indication(\n    patient_pseudonym: str,\n    cluster_ids: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    before_date: Optional[date] = None,\n) -> tuple[bool, Optional[str], Optional[str], Optional[str]]:\n    \"\"\"\n    Check if a patient has any SNOMED codes from the specified clusters in GP records.\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        cluster_ids: List of cluster IDs to check against\n        connector: Optional SnowflakeConnector\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        Tuple of (has_indication, matched_cluster_id, matched_snomed_code, matched_description)\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        return False, None, None, None\n\n    if not cluster_ids:\n        return False, None, None, None\n\n    if connector is None:\n        connector = get_connector()\n\n    # Build placeholders for cluster IDs\n    placeholders = \", \".join([\"%s\"] * len(cluster_ids))\n\n    # Query to check if patient has any matching SNOMED code\n    query = f'''\n        SELECT\n            pc.\"SNOMEDCode\",\n            cc.\"Cluster_ID\",\n            cc.\"SNOMEDDescription\"\n        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\n        INNER JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" cc\n            ON pc.\"SNOMEDCode\" = cc.\"SNOMEDCode\"\n        WHERE pc.\"PatientPseudonym\" = %s\n            AND cc.\"Cluster_ID\" IN ({placeholders})\n    '''\n\n    params = [patient_pseudonym] + cluster_ids\n\n    if before_date:\n        query += ' AND pc.\"EventDateTime\" < %s'\n        params.append(before_date.isoformat())\n\n    query += ' LIMIT 1'\n\n    try:\n        results = connector.execute_dict(query, tuple(params))\n\n        if results:\n            row = results[0]\n            return (\n                True,\n                row.get(\"Cluster_ID\"),\n                row.get(\"SNOMEDCode\"),\n                row.get(\"SNOMEDDescription\"),\n            )\n\n        return False, None, None, None\n\n    except Exception as e:\n        logger.error(f\"Error checking indication for patient '{patient_pseudonym}': {e}\")\n        return False, None, None, None\n\n\ndef validate_indication(\n    patient_pseudonym: str,\n    drug_name: str,\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    before_date: Optional[date] = None,\n) -> IndicationValidationResult:\n    \"\"\"\n    Validate that a patient has an appropriate indication for a drug.\n\n    Full validation workflow:\n    1. Get drug's valid indication clusters from local mapping\n    2. Check if patient has any matching SNOMED codes in GP records\n    3. Return detailed validation result\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        drug_name: Drug name to validate indication for\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        IndicationValidationResult with validation details\n    \"\"\"\n    result = IndicationValidationResult(\n        patient_pseudonym=patient_pseudonym,\n        drug_name=drug_name,\n        has_valid_indication=False,\n    )\n\n    # Step 1: Get drug's cluster mappings\n    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n\n    if not cluster_ids:\n        result.error_message = f\"No cluster mappings found for drug '{drug_name}'\"\n        result.source = \"NONE\"\n        return result\n\n    result.checked_clusters = cluster_ids\n\n    # Step 2: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        result.error_message = \"Snowflake connector not installed\"\n        result.source = \"NONE\"\n        return result\n\n    if not is_snowflake_configured():\n        result.error_message = \"Snowflake not configured\"\n        result.source = \"NONE\"\n        return result\n\n    # Step 3: Check patient GP records\n    has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n        patient_pseudonym=patient_pseudonym,\n        cluster_ids=cluster_ids,\n        connector=connector,\n        before_date=before_date,\n    )\n\n    result.has_valid_indication = has_indication\n    result.matched_cluster_id = matched_cluster\n    result.matched_snomed_code = matched_code\n    result.matched_snomed_description = matched_desc\n    result.source = \"GP_SNOMED\" if has_indication else \"NONE\"\n\n    return result\n\n\ndef get_indication_match_rate(\n    drug_name: str,\n    patient_pseudonyms: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    sample_unmatched_count: int = 10,\n) -> DrugIndicationMatchRate:\n    \"\"\"\n    Calculate indication match rate for a drug across a list of patients.\n\n    Args:\n        drug_name: Drug name to check\n        patient_pseudonyms: List of patient pseudonymised NHS numbers\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        sample_unmatched_count: Number of unmatched patient IDs to include in sample\n\n    Returns:\n        DrugIndicationMatchRate with match statistics\n    \"\"\"\n    if connector is None and SNOWFLAKE_AVAILABLE and is_snowflake_configured():\n        connector = get_connector()\n\n    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n\n    total = len(patient_pseudonyms)\n    matched = 0\n    unmatched = 0\n    sample_unmatched: list[str] = []\n\n    if not cluster_ids:\n        logger.warning(f\"No cluster mappings for drug '{drug_name}' - all patients will be unmatched\")\n        return DrugIndicationMatchRate(\n            drug_name=drug_name,\n            total_patients=total,\n            patients_with_indication=0,\n            patients_without_indication=total,\n            match_rate=0.0,\n            clusters_checked=[],\n            sample_unmatched=patient_pseudonyms[:sample_unmatched_count],\n        )\n\n    for i, pseudonym in enumerate(patient_pseudonyms):\n        if i > 0 and i % 100 == 0:\n            logger.info(f\"Validating indications: {i}/{total} ({100*i/total:.1f}%)\")\n\n        has_indication, _, _, _ = patient_has_indication(\n            patient_pseudonym=pseudonym,\n            cluster_ids=cluster_ids,\n            connector=connector,\n        )\n\n        if has_indication:\n            matched += 1\n        else:\n            unmatched += 1\n            if len(sample_unmatched) < sample_unmatched_count:\n                sample_unmatched.append(pseudonym)\n\n    match_rate = matched / total if total > 0 else 0.0\n\n    logger.info(f\"Indication match rate for '{drug_name}': {100*match_rate:.1f}% ({matched}/{total})\")\n\n    return DrugIndicationMatchRate(\n        drug_name=drug_name,\n        total_patients=total,\n        patients_with_indication=matched,\n        patients_without_indication=unmatched,\n        match_rate=match_rate,\n        clusters_checked=cluster_ids,\n        sample_unmatched=sample_unmatched,\n    )\n\n\ndef batch_validate_indications(\n    patient_drug_pairs: list[tuple[str, str]],\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -> list[IndicationValidationResult]:\n    \"\"\"\n    Validate indications for multiple patient-drug pairs efficiently.\n\n    Args:\n        patient_drug_pairs: List of (patient_pseudonym, drug_name) tuples\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        progress_callback: Optional callback(current, total) for progress updates\n\n    Returns:\n        List of IndicationValidationResult for each pair\n    \"\"\"\n    results = []\n    total = len(patient_drug_pairs)\n\n    # Cache cluster lookups by drug\n    drug_clusters_cache = {}\n\n    for i, (pseudonym, drug_name) in enumerate(patient_drug_pairs):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # Get clusters from cache or lookup\n        drug_upper = drug_name.upper()\n        if drug_upper not in drug_clusters_cache:\n            drug_clusters_cache[drug_upper] = get_drug_cluster_ids(drug_name, db_manager)\n\n        cluster_ids = drug_clusters_cache[drug_upper]\n\n        if not cluster_ids:\n            results.append(IndicationValidationResult(\n                patient_pseudonym=pseudonym,\n                drug_name=drug_name,\n                has_valid_indication=False,\n                source=\"NONE\",\n                error_message=f\"No cluster mappings for drug '{drug_name}'\",\n            ))\n            continue\n\n        # Check patient indication\n        has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n            patient_pseudonym=pseudonym,\n            cluster_ids=cluster_ids,\n            connector=connector,\n        )\n\n        results.append(IndicationValidationResult(\n            patient_pseudonym=pseudonym,\n            drug_name=drug_name,\n            has_valid_indication=has_indication,\n            matched_cluster_id=matched_cluster,\n            matched_snomed_code=matched_code,\n            matched_snomed_description=matched_desc,\n            checked_clusters=cluster_ids,\n            source=\"GP_SNOMED\" if has_indication else \"NONE\",\n        ))\n\n    matched_count = sum(1 for r in results if r.has_valid_indication)\n    logger.info(f\"Batch validation complete: {matched_count}/{total} ({100*matched_count/total:.1f}%) with valid indications\")\n\n    return results\n\n\ndef get_available_clusters(\n    connector: Optional[SnowflakeConnector] = None,\n) -> list[dict]:\n    \"\"\"\n    Get list of all available SNOMED clusters from Snowflake.\n\n    Returns:\n        List of dicts with cluster_id, cluster_description, code_count\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        logger.warning(\"Snowflake not available - cannot list clusters\")\n        return []\n\n    if connector is None:\n        connector = get_connector()\n\n    query = '''\n        SELECT\n            \"Cluster_ID\",\n            \"Cluster_Description\",\n            COUNT(DISTINCT \"SNOMEDCode\") as code_count\n        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n        GROUP BY \"Cluster_ID\", \"Cluster_Description\"\n        ORDER BY \"Cluster_ID\"\n    '''\n\n    try:\n        results = connector.execute_dict(query)\n\n        clusters = []\n        for row in results:\n            clusters.append({\n                \"cluster_id\": row.get(\"Cluster_ID\"),\n                \"cluster_description\": row.get(\"Cluster_Description\"),\n                \"code_count\": row.get(\"code_count\", 0),\n            })\n\n        logger.info(f\"Found {len(clusters)} available SNOMED clusters\")\n        return clusters\n\n    except Exception as e:\n        logger.error(f\"Error getting available clusters: {e}\")\n        return []\n\n\ndef batch_lookup_indication_groups(\n    df: \"pd.DataFrame\",\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    batch_size: int = 500,\n) -> \"pd.DataFrame\":\n    \"\"\"\n    Batch lookup GP diagnosis-based indication groups for a DataFrame of patients.\n\n    This is the efficient batch version of get_directorate_from_diagnosis().\n    Instead of querying Snowflake per patient, it batches the lookups for performance.\n\n    Strategy:\n    1. Get all unique (PersonKey, Drug Name) pairs from DataFrame\n    2. For each unique drug, get all SNOMED codes from local SQLite\n    3. Build batched Snowflake queries to check GP records\n    4. Return indication_df mapping UPID ÔåÆ Indication_Group\n\n    For unmatched patients, Indication_Group will be their Directory (with suffix).\n\n    Args:\n        df: DataFrame with columns: UPID, Drug Name, Directory, PersonKey\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n        batch_size: Number of patients per Snowflake query batch\n\n    Returns:\n        DataFrame with columns: UPID, Indication_Group, Source\n        - Indication_Group: Search_Term (if matched) or \"Directory (no GP dx)\" (if not)\n        - Source: \"DIAGNOSIS\" or \"FALLBACK\"\n    \"\"\"\n    import pandas as pd\n\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    logger.info(f\"Starting batch indication lookup for {len(df)} records...\")\n\n    # Step 1: Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combinations\n    # We need PseudoNHSNoLinked to query Snowflake - this matches PatientPseudonym in GP records\n    # Note: PersonKey is LocalPatientID which is provider-specific and does NOT match GP records\n    if 'PseudoNHSNoLinked' not in df.columns:\n        logger.error(\"DataFrame missing 'PseudoNHSNoLinked' column - cannot lookup GP records\")\n        # Return fallback for all patients\n        result_df = df[['UPID', 'Directory']].drop_duplicates().copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']]\n\n    # Get unique patient-drug combinations (we need one lookup per patient-drug pair)\n    unique_pairs = df[['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']].drop_duplicates()\n    logger.info(f\"Found {len(unique_pairs)} unique patient-drug combinations\")\n\n    # Step 2: Get all unique drugs and their SNOMED codes\n    unique_drugs = unique_pairs['Drug Name'].unique()\n    logger.info(f\"Building SNOMED lookup for {len(unique_drugs)} unique drugs...\")\n\n    # Build drug -> list of DrugSnomedMapping dict\n    drug_snomed_map: dict[str, list[DrugSnomedMapping]] = {}\n    all_snomed_codes: set[str] = set()\n    snomed_to_drug_searchterm: dict[str, list[tuple[str, str, str]]] = {}  # snomed -> [(drug, search_term, primary_dir), ...]\n\n    for drug_name in unique_drugs:\n        mappings = get_drug_snomed_codes(drug_name, db_manager)\n        drug_snomed_map[drug_name] = mappings\n\n        for m in mappings:\n            all_snomed_codes.add(m.snomed_code)\n            if m.snomed_code not in snomed_to_drug_searchterm:\n                snomed_to_drug_searchterm[m.snomed_code] = []\n            snomed_to_drug_searchterm[m.snomed_code].append(\n                (drug_name, m.search_term, m.primary_directorate)\n            )\n\n    logger.info(f\"Total SNOMED codes to check: {len(all_snomed_codes)}\")\n\n    # Step 3: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        logger.warning(\"Snowflake not available - returning fallback for all patients\")\n        result_df = unique_pairs[['UPID', 'Directory']].copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n\n    if connector is None:\n        connector = get_connector()\n\n    # Step 4: Query GP records for all patients in batches\n    # The query finds the most recent matching SNOMED code for each patient\n\n    # Get unique PseudoNHSNoLinked values (each = one patient in GP records)\n    unique_patients = unique_pairs[['PseudoNHSNoLinked', 'UPID', 'Directory']].drop_duplicates(subset=['PseudoNHSNoLinked'])\n    patient_pseudonyms = unique_patients['PseudoNHSNoLinked'].tolist()\n\n    logger.info(f\"Querying GP records for {len(patient_pseudonyms)} unique patients in batches of {batch_size}...\")\n\n    # Results dict: PersonKey -> (snomed_code, event_date)\n    gp_matches: dict[str, tuple[str, Any]] = {}\n\n    # Convert SNOMED codes to list for query\n    snomed_list = list(all_snomed_codes)\n\n    if not snomed_list:\n        logger.warning(\"No SNOMED codes to check - returning fallback for all patients\")\n        result_df = unique_pairs[['UPID', 'Directory']].copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n\n    # Build SNOMED IN clause (reused across batches)\n    snomed_placeholders = \", \".join([\"%s\"] * len(snomed_list))\n\n    # Process patients in batches\n    for batch_start in range(0, len(patient_pseudonyms), batch_size):\n        batch_end = min(batch_start + batch_size, len(patient_pseudonyms))\n        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n\n        logger.info(f\"Batch {batch_start//batch_size + 1}: patients {batch_start} to {batch_end}\")\n\n        # Build patient IN clause\n        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n\n        # Query to find all matching SNOMED codes for these patients\n        # We'll get all matches and pick the most recent per patient in Python\n        query = f'''\n            SELECT\n                \"PatientPseudonym\",\n                \"SNOMEDCode\",\n                \"EventDateTime\"\n            FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n            WHERE \"PatientPseudonym\" IN ({patient_placeholders})\n              AND \"SNOMEDCode\" IN ({snomed_placeholders})\n            ORDER BY \"PatientPseudonym\", \"EventDateTime\" DESC\n        '''\n\n        params = tuple(batch_pseudonyms) + tuple(snomed_list)\n\n        try:\n            results = connector.execute_dict(query, params)\n\n            # Process results - pick most recent per patient\n            for row in results:\n                person_key = row.get(\"PatientPseudonym\")\n                snomed_code = row.get(\"SNOMEDCode\")\n                event_date = row.get(\"EventDateTime\")\n\n                if person_key and snomed_code:\n                    # Keep only if we haven't seen this patient yet (first = most recent due to ORDER BY)\n                    if person_key not in gp_matches:\n                        gp_matches[person_key] = (snomed_code, event_date)\n\n        except Exception as e:\n            logger.error(f\"Error querying GP records for batch: {e}\")\n            # Continue with other batches\n\n    logger.info(f\"Found GP matches for {len(gp_matches)} patients\")\n\n    # Step 5: Build result DataFrame\n    # For each unique_pair, determine Indication_Group based on match status\n    results_list = []\n\n    # We need to dedupe by UPID - a patient might be on multiple drugs\n    # Strategy: For each UPID, use the most recent match (if any)\n    upid_to_match: dict[str, tuple[str, str]] = {}  # UPID -> (Indication_Group, Source)\n\n    for _, row in unique_pairs.iterrows():\n        upid = row['UPID']\n        drug_name = row['Drug Name']\n        patient_pseudonym = row['PseudoNHSNoLinked']\n        directory = row['Directory']\n\n        # Check if patient has GP match (using PseudoNHSNoLinked which matches PatientPseudonym in GP)\n        if patient_pseudonym in gp_matches:\n            matched_snomed, event_date = gp_matches[patient_pseudonym]\n\n            # Find the search_term for this SNOMED code and drug\n            # (A SNOMED code might map to multiple drugs with different search_terms)\n            if matched_snomed in snomed_to_drug_searchterm:\n                # Look for match with current drug first\n                search_term = None\n                for drug, st, pd in snomed_to_drug_searchterm[matched_snomed]:\n                    if drug.upper() == drug_name.upper():\n                        search_term = st\n                        break\n                # If no drug-specific match, use any match\n                if search_term is None:\n                    search_term = snomed_to_drug_searchterm[matched_snomed][0][1]\n\n                # Only update if we don't have a match for this UPID yet\n                if upid not in upid_to_match:\n                    upid_to_match[upid] = (search_term, \"DIAGNOSIS\")\n            else:\n                # Shouldn't happen but fallback just in case\n                if upid not in upid_to_match:\n                    upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n        else:\n            # No GP match - use fallback\n            if upid not in upid_to_match:\n                upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n\n    # Build result DataFrame\n    for upid, (indication_group, source) in upid_to_match.items():\n        results_list.append({\n            'UPID': upid,\n            'Indication_Group': indication_group,\n            'Source': source,\n        })\n\n    result_df = pd.DataFrame(results_list)\n\n    # Log statistics\n    diagnosis_count = len([s for s in result_df['Source'] if s == \"DIAGNOSIS\"])\n    fallback_count = len([s for s in result_df['Source'] if s == \"FALLBACK\"])\n    total = len(result_df)\n\n    logger.info(f\"Indication lookup complete:\")\n    logger.info(f\"  Total unique patients: {total}\")\n    logger.info(f\"  DIAGNOSIS matches: {diagnosis_count} ({100*diagnosis_count/total:.1f}%)\")\n    logger.info(f\"  FALLBACK (no GP match): {fallback_count} ({100*fallback_count/total:.1f}%)\")\n\n    return result_df\n\n\n# === Drug-to-indication mapping from DimSearchTerm.csv ===\n\n\n# Merge related Search_Terms into canonical names.\n# Asthma variants are clinically the same condition at different severity levels.\n# Urticaria is a separate condition ÔÇö do NOT merge with asthma.\nSEARCH_TERM_MERGE_MAP: dict[str, str] = {\n    \"allergic asthma\": \"asthma\",\n    \"severe persistent allergic asthma\": \"asthma\",\n}\n\n\ndef load_drug_indication_mapping(\n    csv_path: Optional[str] = None,\n) -> tuple[dict[str, list[str]], dict[str, list[str]]]:\n    \"\"\"\n    Load the drug-to-Search_Term mapping from DimSearchTerm.csv.\n\n    Builds two lookup dicts:\n    - fragment_to_search_terms: drug fragment (UPPERCASE) -> list of Search_Terms containing it\n    - search_term_to_fragments: search_term -> list of drug fragments (UPPERCASE)\n\n    DimSearchTerm.csv has columns: Search_Term, CleanedDrugName, PrimaryDirectorate\n    CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\").\n\n    Note: A Search_Term can appear multiple times with different PrimaryDirectorates\n    (e.g., \"diabetes\" appears under both DIABETIC MEDICINE and OPHTHALMOLOGY).\n    Drug fragments from all rows for the same Search_Term are combined.\n\n    Asthma-related Search_Terms (\"allergic asthma\", \"severe persistent allergic asthma\")\n    are merged into \"asthma\" to match the CLUSTER_MAPPING_SQL normalization.\n    \"urticaria\" stays separate.\n\n    Args:\n        csv_path: Path to DimSearchTerm.csv. Defaults to data/DimSearchTerm.csv.\n\n    Returns:\n        Tuple of (fragment_to_search_terms, search_term_to_fragments)\n    \"\"\"\n    if csv_path is None:\n        csv_path = str(Path(__file__).parent.parent / \"data\" / \"DimSearchTerm.csv\")\n\n    fragment_to_search_terms: dict[str, list[str]] = {}\n    search_term_to_fragments: dict[str, list[str]] = {}\n\n    try:\n        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                search_term = row.get(\"Search_Term\", \"\").strip()\n                drug_names_raw = row.get(\"CleanedDrugName\", \"\").strip()\n\n                # Normalize asthma variants to canonical \"asthma\"\n                search_term = SEARCH_TERM_MERGE_MAP.get(search_term, search_term)\n\n                if not search_term or not drug_names_raw:\n                    continue\n\n                fragments = [frag.strip().upper() for frag in drug_names_raw.split(\"|\") if frag.strip()]\n\n                # Build search_term -> fragments (accumulate for duplicate Search_Terms)\n                if search_term not in search_term_to_fragments:\n                    search_term_to_fragments[search_term] = []\n                for frag in fragments:\n                    if frag not in search_term_to_fragments[search_term]:\n                        search_term_to_fragments[search_term].append(frag)\n\n                # Build fragment -> search_terms\n                for frag in fragments:\n                    if frag not in fragment_to_search_terms:\n                        fragment_to_search_terms[frag] = []\n                    if search_term not in fragment_to_search_terms[frag]:\n                        fragment_to_search_terms[frag].append(search_term)\n\n        logger.info(\n            f\"Loaded drug-indication mapping: {len(search_term_to_fragments)} Search_Terms, \"\n            f\"{len(fragment_to_search_terms)} drug fragments\"\n        )\n\n    except FileNotFoundError:\n        logger.error(f\"DimSearchTerm.csv not found at {csv_path}\")\n    except Exception as e:\n        logger.error(f\"Error loading DimSearchTerm.csv: {e}\")\n\n    return fragment_to_search_terms, search_term_to_fragments\n\n\ndef get_search_terms_for_drug(\n    drug_name: str,\n    search_term_to_fragments: dict[str, list[str]],\n) -> list[str]:\n    \"\"\"\n    Get all Search_Terms that list a given drug using substring matching.\n\n    Checks if any drug fragment from DimSearchTerm is a SUBSTRING of the given\n    drug name (case-insensitive). This handles both exact matches (ADALIMUMAB)\n    and partial fragments (PEGYLATED, INHALED).\n\n    Args:\n        drug_name: HCD drug name (e.g., \"ADALIMUMAB 40MG\", \"PEGYLATED LIPOSOMAL DOXORUBICIN\")\n        search_term_to_fragments: Mapping of search_term -> list of drug fragments\n\n    Returns:\n        List of Search_Terms whose drug fragments match the drug name\n    \"\"\"\n    drug_name_upper = drug_name.upper()\n    matched_terms: list[str] = []\n\n    for search_term, fragments in search_term_to_fragments.items():\n        for frag in fragments:\n            if frag in drug_name_upper:\n                matched_terms.append(search_term)\n                break  # One matching fragment is enough for this Search_Term\n\n    return matched_terms\n\n\n# === NEW APPROACH: Query Snowflake directly using cluster CTE ===\n\n# The cluster query mapping (embedded from snomed_indication_mapping_query.sql)\n# This maps Search_Term -> Cluster_ID for ~148 clinical conditions\nCLUSTER_MAPPING_SQL = \"\"\"\nWITH SearchTermClusters AS (\n    SELECT Search_Term, Cluster_ID FROM (VALUES\n        ('acute lymphoblastic leukaemia', 'HAEMCANMORPH_COD'),\n        ('acute myeloid leukaemia', 'C19HAEMCAN_COD'),\n        ('acute promyelocytic leukaemia', 'HAEMCANMORPH_COD'),\n        ('asthma', 'AST_COD'),\n        ('allergic rhinitis', 'MILDINTAST_COD'),\n        ('alzheimer''s disease', 'DEMALZ_COD'),\n        ('amyloidosis', 'AMYLOID_COD'),\n        ('anaemia', 'eFI2_AnaemiaTimeSensitive'),\n        ('anaplastic large cell lymphoma', 'C19HAEMCAN_COD'),\n        ('apixaban', 'DOACCON_COD'),\n        ('aplastic anaemia', 'eFI2_AnaemiaEver'),\n        ('arthritis', 'eFI2_InflammatoryArthritis'),\n        ('asthma', 'eFI2_Asthma'),\n        ('atopic dermatitis', 'ATOPDERM_COD'),\n        ('atrial fibrillation', 'eFI2_AtrialFibrillation'),\n        ('attention deficit hyperactivity disorder', 'ADHD_COD'),\n        ('bipolar disorder', 'MH_COD'),\n        ('bladder', 'eFI2_UrinaryIncontinence'),\n        ('breast cancer', 'BRCANSCR_COD'),\n        ('cardiomyopathy', 'eFI2_HarmfulDrinking'),\n        ('cardiovascular disease', 'CVDRISKASS_COD'),\n        ('cervical cancer', 'CSDEC_COD'),\n        ('cholangiocarcinoma', 'eFI2_Cancer'),\n        ('chronic kidney disease', 'CKD_COD'),\n        ('chronic liver disease', 'eFI2_LiverProblems'),\n        ('chronic lymphocytic leukaemia', 'EPPHAEMCAN_COD'),\n        ('chronic myeloid leukaemia', 'EPPHAEMCAN_COD'),\n        ('chronic obstructive pulmonary disease', 'eFI2_COPD'),\n        ('colon cancer', 'eFI2_Cancer'),\n        ('colorectal cancer', 'GICANREF_COD'),\n        ('constipation', 'CHRONCONSTIP_COD'),\n        ('covid-19', 'POSSPOSTCOVID_COD'),\n        ('crohn''s disease', 'eFI2_InflammatoryBowelDisease'),\n        ('cutaneous t-cell lymphoma', 'C19HAEMCAN_COD'),\n        ('cystic fibrosis', 'CUST_ICB_CYSTIC_FIBROSIS'),\n        ('deep vein thrombosis', 'VTE_COD'),\n        ('depression', 'eFI2_Depression'),\n        ('diabetes', 'eFI2_DiabetesEver'),\n        ('diabetic retinopathy', 'DRSELIGIBILITY_COD'),\n        ('diffuse large b-cell lymphoma', 'C19HAEMCAN_COD'),\n        ('dravet syndrome', 'EPIL_COD'),\n        ('drug misuse', 'ILLSUBINT_COD'),\n        ('dyspepsia', 'eFI2_AbdominalPain'),\n        ('epilepsy', 'eFI2_Seizures'),\n        ('fallopian tube', 'STERIL_COD'),\n        ('follicular lymphoma', 'C19HAEMCAN_COD'),\n        ('gastric cancer', 'eFI2_Cancer'),\n        ('giant cell arteritis', 'GCA_COD'),\n        ('glioma', 'NHAEMCANMORPH_COD'),\n        ('gout', 'eFI2_InflammatoryArthritis'),\n        ('graft versus host disease', 'GVHD_COD'),\n        ('granulomatosis with polyangiitis', 'WEGENERVASC_COD'),\n        ('growth hormone deficiency', 'HYPOPITUITARY_COD'),\n        ('hand eczema', 'ECZEMA_COD'),\n        ('heart failure', 'eFI2_HeartFailure'),\n        ('hepatitis b', 'HEPBCVAC_COD'),\n        ('hepatocellular carcinoma', 'eFI2_Cancer'),\n        ('hiv', 'PREFLANG_COD'),\n        ('hodgkin lymphoma', 'HAEMCANMORPH_COD'),\n        ('hormone receptor', 'eFI2_ThyroidProblems'),\n        ('hypercholesterolaemia', 'CLASSFH_COD'),\n        ('immune thrombocytopenia', 'ITP_COD'),\n        ('influenza', 'FLUINVITE_COD'),\n        ('insomnia', 'eFI2_SleepProblems'),\n        ('irritable bowel syndrome', 'IBS_COD'),\n        ('ischaemic stroke', 'OSTR_COD'),\n        ('juvenile idiopathic arthritis', 'RARTHAD_COD'),\n        ('kidney transplant', 'RENALTRANSP_COD'),\n        ('leukaemia', 'eFI2_Cancer'),\n        ('lung cancer', 'FTCANREF_COD'),\n        ('lymphoma', 'C19HAEMCAN_COD'),\n        ('macular degeneration', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('macular oedema', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('major depressive episodes', 'eFI2_Depression'),\n        ('malignant melanoma', 'eFI2_Cancer'),\n        ('malignant pleural mesothelioma', 'LUNGCAN_COD'),\n        ('manic episode', 'MH_COD'),\n        ('mantle cell lymphoma', 'HAEMCANMORPH_COD'),\n        ('melanoma', 'eFI2_Cancer'),\n        ('merkel cell carcinoma', 'C19CAN_COD'),\n        ('migraine', 'eFI2_Headache'),\n        ('motor neurone disease', 'MND_COD'),\n        ('multiple myeloma', 'C19HAEMCAN_COD'),\n        ('multiple sclerosis', 'MS_COD'),\n        ('myelodysplastic', 'eFI2_AnaemiaEver'),\n        ('myelofibrosis', 'MDS_COD'),\n        ('myocardial infarction', 'eFI2_IschaemicHeartDisease'),\n        ('myotonia', 'CNDATRISK2_COD'),\n        ('narcolepsy', 'LD_COD'),\n        ('neuroendocrine tumour', 'LUNGCAN_COD'),\n        ('non-small cell lung cancer', 'LUNGCAN_COD'),\n        ('non-small-cell lung cancer', 'FTCANREF_COD'),\n        ('obesity', 'BMI30_COD'),\n        ('osteoarthritis', 'CUST_ICB_OSTEOARTHRITIS'),\n        ('osteoporosis', 'eFI2_Osteoporosis'),\n        ('osteosarcoma', 'NHAEMCANMORPH_COD'),\n        ('ovarian cancer', 'C19CAN_COD'),\n        ('peripheral arterial disease', 'PADEXC_COD'),\n        ('plaque psoriasis', 'PSORIASIS_COD'),\n        ('polycystic kidney disease', 'EPPCONGMALF_COD'),\n        ('polycythaemia vera', 'C19HAEMCAN_COD'),\n        ('pregnancy', 'C19PREG_COD'),\n        ('primary biliary cholangitis', 'eFI2_LiverProblems'),\n        ('primary hypercholesterolaemia', 'FNFHYP_COD'),\n        ('prostate cancer', 'EPPSOLIDCAN_COD'),\n        ('psoriasis', 'PSORIASIS_COD'),\n        ('psoriatic arthritis', 'RARTHAD_COD'),\n        ('pulmonary embolism', 'eFI2_RespiratoryDiseaseTimeSensitive'),\n        ('pulmonary fibrosis', 'ILD_COD'),\n        ('relapsing multiple sclerosis', 'MS_COD'),\n        ('renal cell carcinoma', 'C19CAN_COD'),\n        ('renal transplantation', 'RENALTRANSP_COD'),\n        ('retinal vein occlusion', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('rheumatoid arthritis', 'eFI2_InflammatoryArthritis'),\n        ('rivaroxaban', 'DOACCON_COD'),\n        ('schizophrenia', 'MH_COD'),\n        ('seizures', 'LSZFREQ_COD'),\n        ('sepsis', 'C19ACTIVITY_COD'),\n        ('asthma', 'SEVAST_COD'),\n        ('sickle cell disease', 'SICKLE_COD'),\n        ('sleep apnoea', 'CUST_ICB_NON_SEVERE_LDA'),\n        ('smoking cessation', 'SMOKINGINT_COD'),\n        ('soft tissue sarcoma', 'NHAEMCANMORPH_COD'),\n        ('spinal muscular atrophy', 'MND_COD'),\n        ('squamous cell', 'C19CAN_COD'),\n        ('squamous cell carcinoma', 'C19CAN_COD'),\n        ('stem cell transplant', 'ALLOTRANSP_COD'),\n        ('stroke', 'eFI2_Stroke'),\n        ('systemic lupus erythematosus', 'SLUPUS_COD'),\n        ('systemic mastocytosis', 'HAEMCANMORPH_COD'),\n        ('thrombocytopenic purpura', 'TTP_COD'),\n        ('thrombotic thrombocytopenic purpura', 'TTP_COD'),\n        ('thyroid cancer', 'C19CAN_COD'),\n        ('tophaceous gout', 'CUST_ICB_OSTEOARTHRITIS'),\n        ('transitional cell carcinoma', 'C19CAN_COD'),\n        ('type 1 diabetes', 'DMTYPE1_COD'),\n        ('type 2 diabetes', 'DMTYPE2_COD'),\n        ('ulcerative colitis', 'eFI2_InflammatoryBowelDisease'),\n        ('urothelial carcinoma', 'NHAEMCANMORPH_COD'),\n        ('urticaria', 'XSAL_COD'),\n        ('uveitis', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('vascular disease', 'CVDINVITE_COD'),\n        ('vasculitis', 'CRYOGLOBVASC_COD')\n    ) AS t(Search_Term, Cluster_ID)\n),\n\nClusterCodes AS (\n    SELECT\n        stc.Search_Term,\n        c.\"SNOMEDCode\",\n        c.\"SNOMEDDescription\"\n    FROM SearchTermClusters stc\n    JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" c\n        ON stc.Cluster_ID = c.\"Cluster_ID\"\n    WHERE c.\"SNOMEDCode\" IS NOT NULL\n),\n\nExplicitCodes AS (\n    SELECT Search_Term, SNOMEDCode, SNOMEDDescription FROM (VALUES\n        ('acute coronary syndrome', '837091000000100', 'Manual mapping'),\n        ('ankylosing spondylitis', '162930007', 'Manual mapping'),\n        ('ankylosing spondylitis', '239805001', 'Manual mapping'),\n        ('ankylosing spondylitis', '239810002', 'Manual mapping'),\n        ('ankylosing spondylitis', '239811003', 'Manual mapping'),\n        ('ankylosing spondylitis', '394990003', 'Manual mapping'),\n        ('ankylosing spondylitis', '429712009', 'Manual mapping'),\n        ('ankylosing spondylitis', '441562009', 'Manual mapping'),\n        ('ankylosing spondylitis', '441680005', 'Manual mapping'),\n        ('ankylosing spondylitis', '441930001', 'Manual mapping'),\n        ('axial spondyloarthritis', '723116002', 'Manual mapping'),\n        ('choroidal neovascularisation', '380621000000102', 'Manual mapping'),\n        ('choroidal neovascularisation', '733124000', 'Manual mapping')\n    ) AS t(Search_Term, SNOMEDCode, SNOMEDDescription)\n),\n\nAllIndicationCodes AS (\n    SELECT Search_Term, \"SNOMEDCode\" AS SNOMEDCode, \"SNOMEDDescription\" AS SNOMEDDescription\n    FROM ClusterCodes\n    UNION ALL\n    SELECT Search_Term, SNOMEDCode, SNOMEDDescription\n    FROM ExplicitCodes\n)\n\"\"\"\n\n\ndef get_patient_indication_groups(\n    patient_pseudonyms: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    batch_size: int = 500,\n    earliest_hcd_date: Optional[str] = None,\n) -> \"pd.DataFrame\":\n    \"\"\"\n    Batch lookup GP diagnosis-based indication groups using Snowflake cluster query.\n\n    Returns ALL matching Search_Terms per patient with code_frequency (count of\n    matching SNOMED codes). This enables drug-aware indication matching where\n    each drug is cross-referenced against the patient's GP diagnoses.\n\n    The query:\n    1. Uses the cluster mapping CTE to get all Search_Term -> SNOMED code mappings\n    2. Joins with PrimaryCareClinicalCoding to find patients with matching codes\n    3. Groups by patient + Search_Term and counts matching codes (code_frequency)\n    4. Optionally restricts to GP codes from earliest_hcd_date onwards\n\n    Args:\n        patient_pseudonyms: List of PseudoNHSNoLinked values (matches PatientPseudonym in GP records)\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        batch_size: Number of patients per Snowflake query batch (default 500)\n        earliest_hcd_date: Optional ISO date string (YYYY-MM-DD). If provided, only\n            counts GP codes from this date onwards. Should be MIN(Intervention Date)\n            from the HCD DataFrame to restrict to the HCD data window.\n\n    Returns:\n        DataFrame with columns:\n        - PatientPseudonym: The patient identifier (PseudoNHSNoLinked value)\n        - Search_Term: The matched indication (e.g., \"rheumatoid arthritis\")\n        - code_frequency: Count of matching SNOMED codes for this Search_Term\n\n        Multiple rows per patient (one per matched Search_Term).\n        Patients not found in results have no matching GP diagnosis.\n    \"\"\"\n    import pandas as pd\n\n    logger.info(f\"Starting Snowflake-direct indication lookup for {len(patient_pseudonyms)} patients...\")\n    if earliest_hcd_date:\n        logger.info(f\"  Restricting GP codes to >= {earliest_hcd_date}\")\n\n    # Handle edge case: empty patient list\n    if not patient_pseudonyms:\n        logger.warning(\"Empty patient list provided\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    # Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        logger.error(\"Snowflake connector not available - cannot lookup GP records\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    if not is_snowflake_configured():\n        logger.error(\"Snowflake not configured - cannot lookup GP records\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    if connector is None:\n        connector = get_connector()\n\n    # Results list to collect all matches\n    all_results: list[dict] = []\n\n    # Process patients in batches\n    total_patients = len(patient_pseudonyms)\n    for batch_start in range(0, total_patients, batch_size):\n        batch_end = min(batch_start + batch_size, total_patients)\n        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n        batch_num = batch_start // batch_size + 1\n        total_batches = (total_patients + batch_size - 1) // batch_size\n\n        logger.info(f\"Batch {batch_num}/{total_batches}: patients {batch_start + 1} to {batch_end}\")\n\n        # Build patient IN clause placeholders\n        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n\n        # Build WHERE clause with optional date filter\n        date_filter = \"\"\n        if earliest_hcd_date:\n            date_filter = f\"\\n    AND pc.\\\"EventDateTime\\\" >= %s\"\n\n        # Build the full query with cluster CTE\n        # Returns ALL matching Search_Terms per patient with code_frequency\n        # code_frequency = COUNT of matching SNOMED codes per Search_Term per patient\n        query = f\"\"\"\n{CLUSTER_MAPPING_SQL}\nSELECT\n    pc.\"PatientPseudonym\" AS \"PatientPseudonym\",\n    aic.Search_Term AS \"Search_Term\",\n    COUNT(*) AS \"code_frequency\"\nFROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\nINNER JOIN AllIndicationCodes aic\n    ON pc.\"SNOMEDCode\" = aic.SNOMEDCode\nWHERE pc.\"PatientPseudonym\" IN ({patient_placeholders}){date_filter}\nGROUP BY pc.\"PatientPseudonym\", aic.Search_Term\n\"\"\"\n\n        # Build params: patient pseudonyms + optional date\n        params = list(batch_pseudonyms)\n        if earliest_hcd_date:\n            params.append(earliest_hcd_date)\n\n        try:\n            results = connector.execute_dict(query, tuple(params))\n\n            for row in results:\n                all_results.append({\n                    'PatientPseudonym': row.get('PatientPseudonym'),\n                    'Search_Term': row.get('Search_Term'),\n                    'code_frequency': row.get('code_frequency', 0),\n                })\n\n            logger.debug(f\"Batch {batch_num}: found {len(results)} patient-indication matches\")\n\n        except Exception as e:\n            logger.error(f\"Error querying GP records for batch {batch_num}: {e}\")\n            # Continue with other batches - partial results are better than none\n\n    # Build result DataFrame\n    result_df = pd.DataFrame(all_results)\n\n    # Log summary statistics\n    if len(result_df) > 0:\n        unique_patients = result_df['PatientPseudonym'].nunique()\n        total_rows = len(result_df)\n        match_rate = 100 * unique_patients / total_patients\n        unique_terms = result_df['Search_Term'].nunique()\n        avg_indications = total_rows / unique_patients if unique_patients > 0 else 0\n        logger.info(f\"Indication lookup complete:\")\n        logger.info(f\"  Total patients queried: {total_patients}\")\n        logger.info(f\"  Patients with GP match: {unique_patients} ({match_rate:.1f}%)\")\n        logger.info(f\"  Total patient-indication rows: {total_rows} (avg {avg_indications:.1f} per patient)\")\n        logger.info(f\"  Unique Search_Terms found: {unique_terms}\")\n\n        # Log top Search_Terms\n        top_terms = result_df['Search_Term'].value_counts().head(5)\n        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n    else:\n        logger.info(f\"Indication lookup complete: 0 matches from {total_patients} patients\")\n\n    return result_df\n\n\n# Export public API\n__all__ = [\n    # Dataclasses\n    \"ClusterSnomedCodes\",\n    \"IndicationValidationResult\",\n    \"DrugIndicationMatchRate\",\n    \"DrugSnomedMapping\",\n    \"DirectSnomedMatchResult\",\n    \"DirectorateAssignment\",\n    # Cluster-based lookup functions (existing)\n    \"get_drug_clusters\",\n    \"get_drug_cluster_ids\",\n    \"get_cluster_snomed_codes\",\n    \"patient_has_indication\",\n    \"validate_indication\",\n    \"get_indication_match_rate\",\n    \"batch_validate_indications\",\n    \"get_available_clusters\",\n    # Direct SNOMED lookup functions (new)\n    \"get_drug_snomed_codes\",\n    \"patient_has_indication_direct\",\n    # Diagnosis-based directorate assignment\n    \"get_directorate_from_diagnosis\",\n    # Batch lookup for indication groups\n    \"batch_lookup_indication_groups\",\n    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Snowflake-direct indication lookup (new approach)\n    \"get_patient_indication_groups\",\n    \"CLUSTER_MAPPING_SQL\",\n]\n","numLines":1574,"startLine":1,"totalLines":1574}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01WH4YSKJEk2ajGUJJbkwifQ","type":"message","role":"assistant","content":[{"type":"text","text":"Good. I've read all the required files. Let me understand the current state:\n\n**Phase 1 is complete** (Tasks 1.1, 1.2, 1.3 all done). The next task is **Task 2.1: Create `assign_drug_indications()` function** ÔÇö this is the core drug-aware matching logic.\n\n**Why Task 2.1**: \n- All Phase 1 dependencies are complete (Snowflake query returns ALL matches with code_frequency, drug mapping loaded from CSV, asthma merged)\n- Task 2.1 is the foundation for Phase 2 ÔÇö Task 2.2 (tiebreaker logic) is naturally part of 2.1\n- This can be built and tested with mock data (no Snowflake needed)\n- Phase 3 (pipeline integration) depends on this function existing\n\nLet me read the current refresh_pathways.py to understand the integration point and how gp_matches_df is currently used."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":24242,"cache_read_input_tokens":82844,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":24242},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"d4bd4a94-98fa-4f76-8ef4-01b6dc6c0c1e"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01WH4YSKJEk2ajGUJJbkwifQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WViPVt2UTcRhMerW65Fe7P","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\cli\\refresh_pathways.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":24242,"cache_read_input_tokens":82844,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":24242},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"c05ea9b8-fab1-4745-bd1d-0992627435fb"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WViPVt2UTcRhMerW65Fe7P","type":"tool_result","content":"     1ÔåÆ\"\"\"\n     2ÔåÆCLI command for refreshing pathway data from Snowflake.\n     3ÔåÆ\n     4ÔåÆThis command fetches activity data from Snowflake, processes it through the\n     5ÔåÆpathway pipeline for all 6 date filter combinations, and stores the results\n     6ÔåÆin the SQLite pathway_nodes table. Supports two chart types:\n     7ÔåÆ- \"directory\": Trust ÔåÆ Directory ÔåÆ Drug ÔåÆ Pathway (default)\n     8ÔåÆ- \"indication\": Trust ÔåÆ Search_Term ÔåÆ Drug ÔåÆ Pathway (requires GP diagnosis lookup)\n     9ÔåÆ\n    10ÔåÆUsage:\n    11ÔåÆ    python -m cli.refresh_pathways\n    12ÔåÆ    python -m cli.refresh_pathways --minimum-patients 10\n    13ÔåÆ    python -m cli.refresh_pathways --provider-codes RGT,RM1\n    14ÔåÆ    python -m cli.refresh_pathways --chart-type all\n    15ÔåÆ    python -m cli.refresh_pathways --chart-type directory\n    16ÔåÆ    python -m cli.refresh_pathways --dry-run\n    17ÔåÆ\n    18ÔåÆRun `python -m cli.refresh_pathways --help` for full options.\n    19ÔåÆ\"\"\"\n    20ÔåÆ\n    21ÔåÆimport argparse\n    22ÔåÆimport json\n    23ÔåÆimport sqlite3\n    24ÔåÆimport sys\n    25ÔåÆimport time\n    26ÔåÆimport uuid\n    27ÔåÆfrom datetime import datetime\n    28ÔåÆfrom pathlib import Path\n    29ÔåÆfrom typing import Optional\n    30ÔåÆ\n    31ÔåÆfrom core import PathConfig, default_paths\n    32ÔåÆfrom core.logging_config import get_logger, setup_logging\n    33ÔåÆfrom data_processing.database import DatabaseManager, DatabaseConfig\n    34ÔåÆfrom data_processing.schema import (\n    35ÔåÆ    clear_pathway_nodes,\n    36ÔåÆ    get_pathway_table_counts,\n    37ÔåÆ    verify_pathway_tables_exist,\n    38ÔåÆ    create_pathway_tables,\n    39ÔåÆ)\n    40ÔåÆfrom data_processing.pathway_pipeline import (\n    41ÔåÆ    ChartType,\n    42ÔåÆ    DATE_FILTER_CONFIGS,\n    43ÔåÆ    fetch_and_transform_data,\n    44ÔåÆ    process_all_date_filters,\n    45ÔåÆ    process_pathway_for_date_filter,\n    46ÔåÆ    process_indication_pathway_for_date_filter,\n    47ÔåÆ    extract_denormalized_fields,\n    48ÔåÆ    extract_indication_fields,\n    49ÔåÆ    convert_to_records,\n    50ÔåÆ)\n    51ÔåÆfrom data_processing.diagnosis_lookup import get_patient_indication_groups\n    52ÔåÆ\n    53ÔåÆlogger = get_logger(__name__)\n    54ÔåÆ\n    55ÔåÆ\n    56ÔåÆdef get_default_filters(paths: PathConfig) -> tuple[list[str], list[str], list[str]]:\n    57ÔåÆ    \"\"\"\n    58ÔåÆ    Load default filter values from reference files.\n    59ÔåÆ\n    60ÔåÆ    Returns:\n    61ÔåÆ        Tuple of (trust_filter, drug_filter, directory_filter)\n    62ÔåÆ    \"\"\"\n    63ÔåÆ    import pandas as pd\n    64ÔåÆ\n    65ÔåÆ    # Load default trusts\n    66ÔåÆ    trust_filter = []\n    67ÔåÆ    if paths.default_trusts_csv.exists():\n    68ÔåÆ        try:\n    69ÔåÆ            trusts_df = pd.read_csv(paths.default_trusts_csv)\n    70ÔåÆ            # Use the \"Name\" column which contains trust names\n    71ÔåÆ            if 'Name' in trusts_df.columns:\n    72ÔåÆ                trust_filter = trusts_df['Name'].dropna().tolist()\n    73ÔåÆ            else:\n    74ÔåÆ                # Fallback to first column if no Name column\n    75ÔåÆ                trust_filter = trusts_df.iloc[:, 0].dropna().tolist()\n    76ÔåÆ            logger.info(f\"Loaded {len(trust_filter)} default trusts\")\n    77ÔåÆ        except Exception as e:\n    78ÔåÆ            logger.warning(f\"Could not load default trusts: {e}\")\n    79ÔåÆ\n    80ÔåÆ    # Load default drugs (Include=1 in include.csv)\n    81ÔåÆ    drug_filter = []\n    82ÔåÆ    if paths.include_csv.exists():\n    83ÔåÆ        try:\n    84ÔåÆ            drugs_df = pd.read_csv(paths.include_csv)\n    85ÔåÆ            if 'Include' in drugs_df.columns:\n    86ÔåÆ                drug_filter = drugs_df[drugs_df['Include'] == 1].iloc[:, 0].dropna().tolist()\n    87ÔåÆ            else:\n    88ÔåÆ                # Assume first column contains drug names if no Include column\n    89ÔåÆ                drug_filter = drugs_df.iloc[:, 0].dropna().tolist()\n    90ÔåÆ            logger.info(f\"Loaded {len(drug_filter)} default drugs\")\n    91ÔåÆ        except Exception as e:\n    92ÔåÆ            logger.warning(f\"Could not load default drugs: {e}\")\n    93ÔåÆ\n    94ÔåÆ    # Load default directories\n    95ÔåÆ    directory_filter = []\n    96ÔåÆ    if paths.directory_list_csv.exists():\n    97ÔåÆ        try:\n    98ÔåÆ            dirs_df = pd.read_csv(paths.directory_list_csv)\n    99ÔåÆ            # Assume first column contains directory names\n   100ÔåÆ            directory_filter = dirs_df.iloc[:, 0].dropna().tolist()\n   101ÔåÆ            logger.info(f\"Loaded {len(directory_filter)} default directories\")\n   102ÔåÆ        except Exception as e:\n   103ÔåÆ            logger.warning(f\"Could not load default directories: {e}\")\n   104ÔåÆ\n   105ÔåÆ    return trust_filter, drug_filter, directory_filter\n   106ÔåÆ\n   107ÔåÆ\n   108ÔåÆdef insert_pathway_records(\n   109ÔåÆ    conn: sqlite3.Connection,\n   110ÔåÆ    records: list[dict],\n   111ÔåÆ) -> int:\n   112ÔåÆ    \"\"\"\n   113ÔåÆ    Insert pathway records into pathway_nodes table.\n   114ÔåÆ\n   115ÔåÆ    Uses INSERT OR REPLACE to handle updates to existing records.\n   116ÔåÆ\n   117ÔåÆ    Args:\n   118ÔåÆ        conn: SQLite connection\n   119ÔåÆ        records: List of record dicts from convert_to_records()\n   120ÔåÆ\n   121ÔåÆ    Returns:\n   122ÔåÆ        Number of records inserted\n   123ÔåÆ    \"\"\"\n   124ÔåÆ    if not records:\n   125ÔåÆ        return 0\n   126ÔåÆ\n   127ÔåÆ    # Column order matching pathway_nodes schema (includes chart_type)\n   128ÔåÆ    columns = [\n   129ÔåÆ        'date_filter_id', 'chart_type', 'parents', 'ids', 'labels', 'level',\n   130ÔåÆ        'value', 'cost', 'costpp', 'cost_pp_pa', 'colour',\n   131ÔåÆ        'first_seen', 'last_seen', 'first_seen_parent', 'last_seen_parent',\n   132ÔåÆ        'average_spacing', 'average_administered', 'avg_days',\n   133ÔåÆ        'trust_name', 'directory', 'drug_sequence', 'data_refresh_id'\n   134ÔåÆ    ]\n   135ÔåÆ\n   136ÔåÆ    placeholders = ', '.join(['?' for _ in columns])\n   137ÔåÆ    column_names = ', '.join(columns)\n   138ÔåÆ\n   139ÔåÆ    insert_sql = f\"\"\"\n   140ÔåÆ        INSERT OR REPLACE INTO pathway_nodes ({column_names})\n   141ÔåÆ        VALUES ({placeholders})\n   142ÔåÆ    \"\"\"\n   143ÔåÆ\n   144ÔåÆ    # Convert records to tuples in column order\n   145ÔåÆ    rows = []\n   146ÔåÆ    for record in records:\n   147ÔåÆ        row = tuple(record.get(col) for col in columns)\n   148ÔåÆ        rows.append(row)\n   149ÔåÆ\n   150ÔåÆ    cursor = conn.executemany(insert_sql, rows)\n   151ÔåÆ    return cursor.rowcount\n   152ÔåÆ\n   153ÔåÆ\n   154ÔåÆdef log_refresh_start(\n   155ÔåÆ    conn: sqlite3.Connection,\n   156ÔåÆ    refresh_id: str,\n   157ÔåÆ    date_from: Optional[str] = None,\n   158ÔåÆ    date_to: Optional[str] = None,\n   159ÔåÆ) -> None:\n   160ÔåÆ    \"\"\"Log the start of a refresh operation.\"\"\"\n   161ÔåÆ    conn.execute(\"\"\"\n   162ÔåÆ        INSERT INTO pathway_refresh_log\n   163ÔåÆ        (refresh_id, started_at, status, snowflake_query_date_from, snowflake_query_date_to)\n   164ÔåÆ        VALUES (?, ?, 'running', ?, ?)\n   165ÔåÆ    \"\"\", (refresh_id, datetime.now().isoformat(), date_from, date_to))\n   166ÔåÆ    conn.commit()\n   167ÔåÆ\n   168ÔåÆ\n   169ÔåÆdef log_refresh_complete(\n   170ÔåÆ    conn: sqlite3.Connection,\n   171ÔåÆ    refresh_id: str,\n   172ÔåÆ    record_count: int,\n   173ÔåÆ    date_filter_counts: dict[str, int],\n   174ÔåÆ    duration_seconds: float,\n   175ÔåÆ) -> None:\n   176ÔåÆ    \"\"\"Log the successful completion of a refresh operation.\"\"\"\n   177ÔåÆ    conn.execute(\"\"\"\n   178ÔåÆ        UPDATE pathway_refresh_log\n   179ÔåÆ        SET completed_at = ?,\n   180ÔåÆ            status = 'completed',\n   181ÔåÆ            record_count = ?,\n   182ÔåÆ            date_filter_counts = ?,\n   183ÔåÆ            processing_duration_seconds = ?\n   184ÔåÆ        WHERE refresh_id = ?\n   185ÔåÆ    \"\"\", (\n   186ÔåÆ        datetime.now().isoformat(),\n   187ÔåÆ        record_count,\n   188ÔåÆ        json.dumps(date_filter_counts),\n   189ÔåÆ        duration_seconds,\n   190ÔåÆ        refresh_id,\n   191ÔåÆ    ))\n   192ÔåÆ    conn.commit()\n   193ÔåÆ\n   194ÔåÆ\n   195ÔåÆdef log_refresh_failed(\n   196ÔåÆ    conn: sqlite3.Connection,\n   197ÔåÆ    refresh_id: str,\n   198ÔåÆ    error_message: str,\n   199ÔåÆ    duration_seconds: float,\n   200ÔåÆ) -> None:\n   201ÔåÆ    \"\"\"Log a failed refresh operation.\"\"\"\n   202ÔåÆ    conn.execute(\"\"\"\n   203ÔåÆ        UPDATE pathway_refresh_log\n   204ÔåÆ        SET completed_at = ?,\n   205ÔåÆ            status = 'failed',\n   206ÔåÆ            error_message = ?,\n   207ÔåÆ            processing_duration_seconds = ?\n   208ÔåÆ        WHERE refresh_id = ?\n   209ÔåÆ    \"\"\", (\n   210ÔåÆ        datetime.now().isoformat(),\n   211ÔåÆ        error_message,\n   212ÔåÆ        duration_seconds,\n   213ÔåÆ        refresh_id,\n   214ÔåÆ    ))\n   215ÔåÆ    conn.commit()\n   216ÔåÆ\n   217ÔåÆ\n   218ÔåÆdef refresh_pathways(\n   219ÔåÆ    minimum_patients: int = 5,\n   220ÔåÆ    provider_codes: Optional[list[str]] = None,\n   221ÔåÆ    trust_filter: Optional[list[str]] = None,\n   222ÔåÆ    drug_filter: Optional[list[str]] = None,\n   223ÔåÆ    directory_filter: Optional[list[str]] = None,\n   224ÔåÆ    db_path: Optional[Path] = None,\n   225ÔåÆ    paths: Optional[PathConfig] = None,\n   226ÔåÆ    dry_run: bool = False,\n   227ÔåÆ    chart_type: str = \"directory\",\n   228ÔåÆ) -> tuple[bool, str, dict]:\n   229ÔåÆ    \"\"\"\n   230ÔåÆ    Main refresh function that orchestrates the full pipeline.\n   231ÔåÆ\n   232ÔåÆ    Args:\n   233ÔåÆ        minimum_patients: Minimum patients to include a pathway\n   234ÔåÆ        provider_codes: List of provider codes to filter Snowflake query\n   235ÔåÆ        trust_filter: List of trust names to include in pathways\n   236ÔåÆ        drug_filter: List of drug names to include in pathways\n   237ÔåÆ        directory_filter: List of directories to include in pathways\n   238ÔåÆ        db_path: Path to SQLite database (uses default if None)\n   239ÔåÆ        paths: PathConfig for file paths\n   240ÔåÆ        dry_run: If True, don't actually insert records\n   241ÔåÆ        chart_type: Which chart type to process: \"directory\", \"indication\", or \"all\"\n   242ÔåÆ\n   243ÔåÆ    Returns:\n   244ÔåÆ        Tuple of (success: bool, message: str, stats: dict)\n   245ÔåÆ    \"\"\"\n   246ÔåÆ    if paths is None:\n   247ÔåÆ        paths = default_paths\n   248ÔåÆ\n   249ÔåÆ    # Set up database connection\n   250ÔåÆ    if db_path:\n   251ÔåÆ        db_config = DatabaseConfig(db_path=db_path)\n   252ÔåÆ    else:\n   253ÔåÆ        db_config = DatabaseConfig(data_dir=paths.data_dir)\n   254ÔåÆ\n   255ÔåÆ    db_manager = DatabaseManager(db_config)\n   256ÔåÆ\n   257ÔåÆ    # Load default filters if not provided\n   258ÔåÆ    default_trusts, default_drugs, default_dirs = get_default_filters(paths)\n   259ÔåÆ\n   260ÔåÆ    if trust_filter is None:\n   261ÔåÆ        trust_filter = default_trusts\n   262ÔåÆ    if drug_filter is None:\n   263ÔåÆ        drug_filter = default_drugs\n   264ÔåÆ    if directory_filter is None:\n   265ÔåÆ        directory_filter = default_dirs\n   266ÔåÆ\n   267ÔåÆ    # Ensure we have some filters\n   268ÔåÆ    if not drug_filter:\n   269ÔåÆ        return False, \"No drugs specified and could not load defaults\", {}\n   270ÔåÆ\n   271ÔåÆ    # Determine which chart types to process\n   272ÔåÆ    if chart_type == \"all\":\n   273ÔåÆ        chart_types_to_process: list[ChartType] = [\"directory\", \"indication\"]\n   274ÔåÆ    else:\n   275ÔåÆ        chart_types_to_process = [chart_type]  # type: ignore\n   276ÔåÆ\n   277ÔåÆ    logger.info(\"=\" * 60)\n   278ÔåÆ    logger.info(\"Pathway Data Refresh Starting\")\n   279ÔåÆ    logger.info(\"=\" * 60)\n   280ÔåÆ    logger.info(f\"Minimum patients: {minimum_patients}\")\n   281ÔåÆ    logger.info(f\"Trust filter: {len(trust_filter)} trusts\")\n   282ÔåÆ    logger.info(f\"Drug filter: {len(drug_filter)} drugs\")\n   283ÔåÆ    logger.info(f\"Directory filter: {len(directory_filter)} directories\")\n   284ÔåÆ    logger.info(f\"Provider codes: {provider_codes or 'All'}\")\n   285ÔåÆ    logger.info(f\"Chart type(s): {', '.join(chart_types_to_process)}\")\n   286ÔåÆ    logger.info(f\"Database: {db_manager.db_path}\")\n   287ÔåÆ    logger.info(f\"Dry run: {dry_run}\")\n   288ÔåÆ    logger.info(\"=\" * 60)\n   289ÔåÆ\n   290ÔåÆ    start_time = time.time()\n   291ÔåÆ    refresh_id = str(uuid.uuid4())[:8]\n   292ÔåÆ    stats = {\n   293ÔåÆ        \"refresh_id\": refresh_id,\n   294ÔåÆ        \"date_filter_counts\": {},\n   295ÔåÆ        \"total_records\": 0,\n   296ÔåÆ        \"snowflake_rows\": 0,\n   297ÔåÆ    }\n   298ÔåÆ\n   299ÔåÆ    try:\n   300ÔåÆ        # Verify database and tables\n   301ÔåÆ        with db_manager.get_connection() as conn:\n   302ÔåÆ            missing_tables = verify_pathway_tables_exist(conn)\n   303ÔåÆ            if missing_tables:\n   304ÔåÆ                logger.info(f\"Creating missing tables: {missing_tables}\")\n   305ÔåÆ                create_pathway_tables(conn)\n   306ÔåÆ\n   307ÔåÆ            # Log refresh start\n   308ÔåÆ            if not dry_run:\n   309ÔåÆ                log_refresh_start(conn, refresh_id)\n   310ÔåÆ\n   311ÔåÆ        # Step 1: Fetch data from Snowflake\n   312ÔåÆ        logger.info(\"\")\n   313ÔåÆ        logger.info(\"Step 1/4: Fetching data from Snowflake...\")\n   314ÔåÆ        df = fetch_and_transform_data(\n   315ÔåÆ            provider_codes=provider_codes,\n   316ÔåÆ            paths=paths,\n   317ÔåÆ        )\n   318ÔåÆ\n   319ÔåÆ        if df.empty:\n   320ÔåÆ            msg = \"No data returned from Snowflake\"\n   321ÔåÆ            logger.error(msg)\n   322ÔåÆ            with db_manager.get_connection() as conn:\n   323ÔåÆ                log_refresh_failed(conn, refresh_id, msg, time.time() - start_time)\n   324ÔåÆ            return False, msg, stats\n   325ÔåÆ\n   326ÔåÆ        stats[\"snowflake_rows\"] = len(df)\n   327ÔåÆ        logger.info(f\"Fetched {len(df)} records from Snowflake\")\n   328ÔåÆ\n   329ÔåÆ        # Step 2: Process all date filters for each chart type\n   330ÔåÆ        num_date_filters = len(DATE_FILTER_CONFIGS)\n   331ÔåÆ        num_chart_types = len(chart_types_to_process)\n   332ÔåÆ        total_datasets = num_date_filters * num_chart_types\n   333ÔåÆ\n   334ÔåÆ        logger.info(\"\")\n   335ÔåÆ        logger.info(f\"Step 2/4: Processing pathway data for {total_datasets} datasets \"\n   336ÔåÆ                   f\"({num_date_filters} date filters x {num_chart_types} chart types)...\")\n   337ÔåÆ\n   338ÔåÆ        # Store results keyed by \"date_filter_id:chart_type\"\n   339ÔåÆ        results: dict[str, list[dict]] = {}\n   340ÔåÆ\n   341ÔåÆ        for current_chart_type in chart_types_to_process:\n   342ÔåÆ            logger.info(\"\")\n   343ÔåÆ            logger.info(f\"Processing chart type: {current_chart_type}\")\n   344ÔåÆ\n   345ÔåÆ            if current_chart_type == \"directory\":\n   346ÔåÆ                # Use existing process_all_date_filters for directory charts\n   347ÔåÆ                dir_results = process_all_date_filters(\n   348ÔåÆ                    df=df,\n   349ÔåÆ                    trust_filter=trust_filter,\n   350ÔåÆ                    drug_filter=drug_filter,\n   351ÔåÆ                    directory_filter=directory_filter,\n   352ÔåÆ                    minimum_patients=minimum_patients,\n   353ÔåÆ                    refresh_id=refresh_id,\n   354ÔåÆ                    paths=paths,\n   355ÔåÆ                )\n   356ÔåÆ                # Add results with chart_type suffix\n   357ÔåÆ                for filter_id, records in dir_results.items():\n   358ÔåÆ                    # Records already have chart_type set by convert_to_records\n   359ÔåÆ                    results[f\"{filter_id}:directory\"] = records\n   360ÔåÆ\n   361ÔåÆ            elif current_chart_type == \"indication\":\n   362ÔåÆ                # For indication charts, we need to look up GP diagnoses for all patients\n   363ÔåÆ                # using the new Snowflake-direct approach via get_patient_indication_groups()\n   364ÔåÆ                logger.info(\"Building indication groups from GP diagnosis lookups (Snowflake-direct)...\")\n   365ÔåÆ\n   366ÔåÆ                # Check Snowflake availability\n   367ÔåÆ                from data_processing.snowflake_connector import get_connector, is_snowflake_available\n   368ÔåÆ\n   369ÔåÆ                if not is_snowflake_available():\n   370ÔåÆ                    logger.warning(\"Snowflake not available - cannot process indication charts\")\n   371ÔåÆ                    for config in DATE_FILTER_CONFIGS:\n   372ÔåÆ                        results[f\"{config.id}:indication\"] = []\n   373ÔåÆ                    continue\n   374ÔåÆ\n   375ÔåÆ                try:\n   376ÔåÆ                    import pandas as pd\n   377ÔåÆ                    connector = get_connector()\n   378ÔåÆ\n   379ÔåÆ                    # Step 1: Extract unique PseudoNHSNoLinked values from df\n   380ÔåÆ                    # This is the patient identifier that matches PatientPseudonym in GP records\n   381ÔåÆ                    if 'PseudoNHSNoLinked' not in df.columns:\n   382ÔåÆ                        logger.error(\"DataFrame missing 'PseudoNHSNoLinked' column - cannot lookup GP records\")\n   383ÔåÆ                        for config in DATE_FILTER_CONFIGS:\n   384ÔåÆ                            results[f\"{config.id}:indication\"] = []\n   385ÔåÆ                        continue\n   386ÔåÆ\n   387ÔåÆ                    # Get unique patient pseudonyms for GP lookup (avoid redundant queries)\n   388ÔåÆ                    patient_pseudonyms = df['PseudoNHSNoLinked'].dropna().unique().tolist()\n   389ÔåÆ\n   390ÔåÆ                    logger.info(f\"Looking up GP diagnoses for {len(patient_pseudonyms)} unique patients...\")\n   391ÔåÆ\n   392ÔåÆ                    # Step 2: Call the new Snowflake-direct indication lookup\n   393ÔåÆ                    gp_matches_df = get_patient_indication_groups(\n   394ÔåÆ                        patient_pseudonyms=patient_pseudonyms,\n   395ÔåÆ                        connector=connector,\n   396ÔåÆ                        batch_size=500,\n   397ÔåÆ                    )\n   398ÔåÆ\n   399ÔåÆ                    # Step 3: Build indication_df mapping UPID -> Indication_Group\n   400ÔåÆ                    # For matched patients: Indication_Group = Search_Term\n   401ÔåÆ                    # For unmatched patients: Indication_Group = Directory + \" (no GP dx)\"\n   402ÔåÆ                    #\n   403ÔåÆ                    # IMPORTANT: We need ALL unique UPIDs, not just unique PseudoNHSNoLinked.\n   404ÔåÆ                    # A patient can have multiple UPIDs if they visited multiple providers.\n   405ÔåÆ\n   406ÔåÆ                    # Get all unique UPID records with their PseudoNHSNoLinked and Directory\n   407ÔåÆ                    upid_lookup = df[['UPID', 'PseudoNHSNoLinked', 'Directory']].drop_duplicates(\n   408ÔåÆ                        subset=['UPID']\n   409ÔåÆ                    ).copy()\n   410ÔåÆ\n   411ÔåÆ                    if gp_matches_df.empty:\n   412ÔåÆ                        logger.warning(\"No GP matches found - all patients will use fallback directory\")\n   413ÔåÆ                        # All patients use fallback\n   414ÔåÆ                        indication_records = []\n   415ÔåÆ                        for _, row in upid_lookup.iterrows():\n   416ÔåÆ                            directory = row['Directory']\n   417ÔåÆ                            indication_records.append({\n   418ÔåÆ                                'UPID': row['UPID'],\n   419ÔåÆ                                'Indication_Group': str(directory) + \" (no GP dx)\" if pd.notna(directory) else \"UNKNOWN (no GP dx)\",\n   420ÔåÆ                                'Source': 'FALLBACK',\n   421ÔåÆ                            })\n   422ÔåÆ                        indication_df = pd.DataFrame(indication_records)\n   423ÔåÆ                    else:\n   424ÔåÆ                        # Create lookup dict: PseudoNHSNoLinked -> Search_Term\n   425ÔåÆ                        match_lookup = dict(zip(\n   426ÔåÆ                            gp_matches_df['PatientPseudonym'],\n   427ÔåÆ                            gp_matches_df['Search_Term']\n   428ÔåÆ                        ))\n   429ÔåÆ\n   430ÔåÆ                        # Build indication records for each unique UPID\n   431ÔåÆ                        indication_records = []\n   432ÔåÆ                        for _, row in upid_lookup.iterrows():\n   433ÔåÆ                            pseudo = row['PseudoNHSNoLinked']\n   434ÔåÆ                            upid = row['UPID']\n   435ÔåÆ                            directory = row['Directory']\n   436ÔåÆ\n   437ÔåÆ                            if pd.notna(pseudo) and pseudo in match_lookup:\n   438ÔåÆ                                indication_records.append({\n   439ÔåÆ                                    'UPID': upid,\n   440ÔåÆ                                    'Indication_Group': match_lookup[pseudo],\n   441ÔåÆ                                    'Source': 'DIAGNOSIS',\n   442ÔåÆ                                })\n   443ÔåÆ                            else:\n   444ÔåÆ                                # Use fallback: Directory + \" (no GP dx)\"\n   445ÔåÆ                                fallback_label = str(directory) + \" (no GP dx)\" if pd.notna(directory) else \"UNKNOWN (no GP dx)\"\n   446ÔåÆ                                indication_records.append({\n   447ÔåÆ                                    'UPID': upid,\n   448ÔåÆ                                    'Indication_Group': fallback_label,\n   449ÔåÆ                                    'Source': 'FALLBACK',\n   450ÔåÆ                                })\n   451ÔåÆ\n   452ÔåÆ                        indication_df = pd.DataFrame(indication_records)\n   453ÔåÆ\n   454ÔåÆ                    # Log coverage statistics\n   455ÔåÆ                    if not indication_df.empty:\n   456ÔåÆ                        diagnosis_count = (indication_df['Source'] == 'DIAGNOSIS').sum()\n   457ÔåÆ                        fallback_count = (indication_df['Source'] == 'FALLBACK').sum()\n   458ÔåÆ                        total = len(indication_df)\n   459ÔåÆ                        stats[\"diagnosis_coverage\"] = {\n   460ÔåÆ                            \"diagnosis\": int(diagnosis_count),\n   461ÔåÆ                            \"fallback\": int(fallback_count),\n   462ÔåÆ                            \"total\": total,\n   463ÔåÆ                            \"diagnosis_pct\": round(100 * diagnosis_count / total, 1) if total > 0 else 0,\n   464ÔåÆ                        }\n   465ÔåÆ                        logger.info(f\"Indication coverage: {diagnosis_count}/{total} ({stats['diagnosis_coverage']['diagnosis_pct']}%) diagnosis-matched\")\n   466ÔåÆ\n   467ÔåÆ                        # Log top indication groups\n   468ÔåÆ                        top_indications = indication_df[indication_df['Source'] == 'DIAGNOSIS']['Indication_Group'].value_counts().head(5)\n   469ÔåÆ                        if len(top_indications) > 0:\n   470ÔåÆ                            logger.info(f\"Top 5 indications: {dict(top_indications)}\")\n   471ÔåÆ\n   472ÔåÆ                        # Rename column for compatibility with generate_icicle_chart_indication\n   473ÔåÆ                        # It expects indication_df to have 'Directory' column (mapped from Indication_Group)\n   474ÔåÆ                        indication_df_for_chart = indication_df[['UPID', 'Indication_Group']].copy()\n   475ÔåÆ                        indication_df_for_chart = indication_df_for_chart.rename(columns={'Indication_Group': 'Directory'})\n   476ÔåÆ                        # Ensure unique UPID index (build_hierarchy requires uniquely valued Index)\n   477ÔåÆ                        # Keep first occurrence - DIAGNOSIS entries should come before FALLBACK\n   478ÔåÆ                        indication_df_for_chart = indication_df_for_chart.drop_duplicates(subset=['UPID'], keep='first')\n   479ÔåÆ                        indication_df_for_chart = indication_df_for_chart.set_index('UPID')\n   480ÔåÆ\n   481ÔåÆ                        # Process each date filter with indication grouping\n   482ÔåÆ                        for config in DATE_FILTER_CONFIGS:\n   483ÔåÆ                            logger.info(f\"Processing indication pathway for {config.id}\")\n   484ÔåÆ\n   485ÔåÆ                            ice_df = process_indication_pathway_for_date_filter(\n   486ÔåÆ                                df=df,\n   487ÔåÆ                                indication_df=indication_df_for_chart,\n   488ÔåÆ                                config=config,\n   489ÔåÆ                                trust_filter=trust_filter,\n   490ÔåÆ                                drug_filter=drug_filter,\n   491ÔåÆ                                directory_filter=directory_filter,\n   492ÔåÆ                                minimum_patients=minimum_patients,\n   493ÔåÆ                                paths=paths,\n   494ÔåÆ                            )\n   495ÔåÆ\n   496ÔåÆ                            if ice_df is None:\n   497ÔåÆ                                logger.warning(f\"No indication pathway data for {config.id}\")\n   498ÔåÆ                                results[f\"{config.id}:indication\"] = []\n   499ÔåÆ                                continue\n   500ÔåÆ\n   501ÔåÆ                            # Extract denormalized fields (using indication variant)\n   502ÔåÆ                            ice_df = extract_indication_fields(ice_df)\n   503ÔåÆ\n   504ÔåÆ                            # Convert to records with chart_type=\"indication\"\n   505ÔåÆ                            records = convert_to_records(ice_df, config.id, refresh_id, chart_type=\"indication\")\n   506ÔåÆ                            results[f\"{config.id}:indication\"] = records\n   507ÔåÆ\n   508ÔåÆ                            logger.info(f\"Completed {config.id}:indication: {len(records)} nodes\")\n   509ÔåÆ                    else:\n   510ÔåÆ                        logger.warning(\"Empty indication_df - skipping indication charts\")\n   511ÔåÆ                        for config in DATE_FILTER_CONFIGS:\n   512ÔåÆ                            results[f\"{config.id}:indication\"] = []\n   513ÔåÆ\n   514ÔåÆ                except Exception as e:\n   515ÔåÆ                    logger.error(f\"Error processing indication charts: {e}\")\n   516ÔåÆ                    logger.exception(e)\n   517ÔåÆ                    for config in DATE_FILTER_CONFIGS:\n   518ÔåÆ                        results[f\"{config.id}:indication\"] = []\n   519ÔåÆ\n   520ÔåÆ        # Count records per filter and chart type\n   521ÔåÆ        stats[\"chart_type_counts\"] = {}\n   522ÔåÆ        for key, records in results.items():\n   523ÔåÆ            stats[\"date_filter_counts\"][key] = len(records)\n   524ÔåÆ            stats[\"total_records\"] += len(records)\n   525ÔåÆ            # Also track by chart type\n   526ÔåÆ            _, ct = key.split(\":\")\n   527ÔåÆ            stats[\"chart_type_counts\"][ct] = stats[\"chart_type_counts\"].get(ct, 0) + len(records)\n   528ÔåÆ\n   529ÔåÆ        logger.info(\"\")\n   530ÔåÆ        logger.info(f\"Processed {stats['total_records']} total pathway nodes\")\n   531ÔåÆ        for chart_type_name, count in stats.get(\"chart_type_counts\", {}).items():\n   532ÔåÆ            logger.info(f\"  {chart_type_name}: {count} nodes total\")\n   533ÔåÆ        for key, count in sorted(stats[\"date_filter_counts\"].items()):\n   534ÔåÆ            if count > 0:\n   535ÔåÆ                logger.info(f\"    {key}: {count} nodes\")\n   536ÔåÆ\n   537ÔåÆ        if dry_run:\n   538ÔåÆ            logger.info(\"\")\n   539ÔåÆ            logger.info(\"DRY RUN - Skipping database insertion\")\n   540ÔåÆ            elapsed = time.time() - start_time\n   541ÔåÆ            return True, f\"Dry run complete: {stats['total_records']} records would be inserted\", stats\n   542ÔåÆ\n   543ÔåÆ        # Step 3: Clear existing data and insert new records\n   544ÔåÆ        logger.info(\"\")\n   545ÔåÆ        logger.info(\"Step 3/4: Clearing existing pathway data and inserting new records...\")\n   546ÔåÆ\n   547ÔåÆ        with db_manager.get_transaction() as conn:\n   548ÔåÆ            # Clear all existing pathway nodes\n   549ÔåÆ            deleted = clear_pathway_nodes(conn)\n   550ÔåÆ            logger.info(f\"Cleared {deleted} existing pathway nodes\")\n   551ÔåÆ\n   552ÔåÆ            # Insert new records for each date filter + chart type combination\n   553ÔåÆ            total_inserted = 0\n   554ÔåÆ            for key, records in results.items():\n   555ÔåÆ                if records:\n   556ÔåÆ                    inserted = insert_pathway_records(conn, records)\n   557ÔåÆ                    total_inserted += len(records)\n   558ÔåÆ                    logger.info(f\"  Inserted {len(records)} records for {key}\")\n   559ÔåÆ\n   560ÔåÆ        # Step 4: Log completion\n   561ÔåÆ        logger.info(\"\")\n   562ÔåÆ        logger.info(\"Step 4/4: Logging refresh completion...\")\n   563ÔåÆ\n   564ÔåÆ        elapsed = time.time() - start_time\n   565ÔåÆ\n   566ÔåÆ        with db_manager.get_connection() as conn:\n   567ÔåÆ            log_refresh_complete(\n   568ÔåÆ                conn=conn,\n   569ÔåÆ                refresh_id=refresh_id,\n   570ÔåÆ                record_count=stats[\"total_records\"],\n   571ÔåÆ                date_filter_counts=stats[\"date_filter_counts\"],\n   572ÔåÆ                duration_seconds=elapsed,\n   573ÔåÆ            )\n   574ÔåÆ\n   575ÔåÆ            # Verify final counts\n   576ÔåÆ            counts = get_pathway_table_counts(conn)\n   577ÔåÆ            logger.info(f\"Final table counts: {counts}\")\n   578ÔåÆ\n   579ÔåÆ        logger.info(\"\")\n   580ÔåÆ        logger.info(\"=\" * 60)\n   581ÔåÆ        logger.info(f\"Refresh completed successfully in {elapsed:.1f} seconds\")\n   582ÔåÆ        logger.info(f\"Total records: {stats['total_records']}\")\n   583ÔåÆ        logger.info(f\"Refresh ID: {refresh_id}\")\n   584ÔåÆ        logger.info(\"=\" * 60)\n   585ÔåÆ\n   586ÔåÆ        return True, f\"Refresh complete: {stats['total_records']} records in {elapsed:.1f}s\", stats\n   587ÔåÆ\n   588ÔåÆ    except Exception as e:\n   589ÔåÆ        elapsed = time.time() - start_time\n   590ÔåÆ        error_msg = f\"Refresh failed: {e}\"\n   591ÔåÆ        logger.error(error_msg, exc_info=True)\n   592ÔåÆ\n   593ÔåÆ        try:\n   594ÔåÆ            with db_manager.get_connection() as conn:\n   595ÔåÆ                log_refresh_failed(conn, refresh_id, str(e), elapsed)\n   596ÔåÆ        except Exception:\n   597ÔåÆ            pass  # Don't fail the error handling\n   598ÔåÆ\n   599ÔåÆ        return False, error_msg, stats\n   600ÔåÆ\n   601ÔåÆ\n   602ÔåÆdef main() -> int:\n   603ÔåÆ    \"\"\"CLI entry point.\"\"\"\n   604ÔåÆ    parser = argparse.ArgumentParser(\n   605ÔåÆ        description=\"Refresh pathway data from Snowflake\",\n   606ÔåÆ        formatter_class=argparse.RawDescriptionHelpFormatter,\n   607ÔåÆ        epilog=\"\"\"\n   608ÔåÆExamples:\n   609ÔåÆ    # Basic refresh with defaults (directory chart only)\n   610ÔåÆ    python -m cli.refresh_pathways\n   611ÔåÆ\n   612ÔåÆ    # Refresh both chart types (directory and indication)\n   613ÔåÆ    python -m cli.refresh_pathways --chart-type all\n   614ÔåÆ\n   615ÔåÆ    # Refresh only indication-based charts\n   616ÔåÆ    python -m cli.refresh_pathways --chart-type indication\n   617ÔåÆ\n   618ÔåÆ    # Refresh with custom minimum patients\n   619ÔåÆ    python -m cli.refresh_pathways --minimum-patients 10\n   620ÔåÆ\n   621ÔåÆ    # Refresh specific providers only\n   622ÔåÆ    python -m cli.refresh_pathways --provider-codes RGT,RM1\n   623ÔåÆ\n   624ÔåÆ    # Dry run to see what would be processed\n   625ÔåÆ    python -m cli.refresh_pathways --dry-run\n   626ÔåÆ\n   627ÔåÆ    # Verbose output\n   628ÔåÆ    python -m cli.refresh_pathways --verbose\n   629ÔåÆ        \"\"\"\n   630ÔåÆ    )\n   631ÔåÆ\n   632ÔåÆ    parser.add_argument(\n   633ÔåÆ        \"--minimum-patients\",\n   634ÔåÆ        type=int,\n   635ÔåÆ        default=5,\n   636ÔåÆ        help=\"Minimum patients to include a pathway (default: 5)\"\n   637ÔåÆ    )\n   638ÔåÆ\n   639ÔåÆ    parser.add_argument(\n   640ÔåÆ        \"--provider-codes\",\n   641ÔåÆ        type=str,\n   642ÔåÆ        default=None,\n   643ÔåÆ        help=\"Comma-separated list of provider codes to filter (default: all)\"\n   644ÔåÆ    )\n   645ÔåÆ\n   646ÔåÆ    parser.add_argument(\n   647ÔåÆ        \"--db-path\",\n   648ÔåÆ        type=str,\n   649ÔåÆ        default=None,\n   650ÔåÆ        help=\"Path to SQLite database (default: data/pathways.db)\"\n   651ÔåÆ    )\n   652ÔåÆ\n   653ÔåÆ    parser.add_argument(\n   654ÔåÆ        \"--dry-run\",\n   655ÔåÆ        action=\"store_true\",\n   656ÔåÆ        help=\"Process data but don't insert into database\"\n   657ÔåÆ    )\n   658ÔåÆ\n   659ÔåÆ    parser.add_argument(\n   660ÔåÆ        \"--chart-type\",\n   661ÔåÆ        type=str,\n   662ÔåÆ        choices=[\"directory\", \"indication\", \"all\"],\n   663ÔåÆ        default=\"directory\",\n   664ÔåÆ        help=\"Chart type to process: 'directory' (default), 'indication', or 'all'\"\n   665ÔåÆ    )\n   666ÔåÆ\n   667ÔåÆ    parser.add_argument(\n   668ÔåÆ        \"--verbose\", \"-v\",\n   669ÔåÆ        action=\"store_true\",\n   670ÔåÆ        help=\"Enable verbose logging\"\n   671ÔåÆ    )\n   672ÔåÆ\n   673ÔåÆ    args = parser.parse_args()\n   674ÔåÆ\n   675ÔåÆ    # Configure logging\n   676ÔåÆ    import logging\n   677ÔåÆ    log_level = logging.DEBUG if args.verbose else logging.INFO\n   678ÔåÆ    setup_logging(level=log_level)\n   679ÔåÆ\n   680ÔåÆ    # Parse provider codes\n   681ÔåÆ    provider_codes = None\n   682ÔåÆ    if args.provider_codes:\n   683ÔåÆ        provider_codes = [code.strip() for code in args.provider_codes.split(\",\")]\n   684ÔåÆ\n   685ÔåÆ    # Parse db path\n   686ÔåÆ    db_path = Path(args.db_path) if args.db_path else None\n   687ÔåÆ\n   688ÔåÆ    # Run the refresh\n   689ÔåÆ    success, message, stats = refresh_pathways(\n   690ÔåÆ        minimum_patients=args.minimum_patients,\n   691ÔåÆ        provider_codes=provider_codes,\n   692ÔåÆ        db_path=db_path,\n   693ÔåÆ        dry_run=args.dry_run,\n   694ÔåÆ        chart_type=args.chart_type,\n   695ÔåÆ    )\n   696ÔåÆ\n   697ÔåÆ    if success:\n   698ÔåÆ        print(f\"\\n[OK] {message}\")\n   699ÔåÆ        return 0\n   700ÔåÆ    else:\n   701ÔåÆ        print(f\"\\n[FAILED] {message}\", file=sys.stderr)\n   702ÔåÆ        return 1\n   703ÔåÆ\n   704ÔåÆ\n   705ÔåÆif __name__ == \"__main__\":\n   706ÔåÆ    sys.exit(main())\n   707ÔåÆ\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"97dda50f-76e8-47b7-9db4-29886c16b3d9","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\cli\\refresh_pathways.py","content":"\"\"\"\nCLI command for refreshing pathway data from Snowflake.\n\nThis command fetches activity data from Snowflake, processes it through the\npathway pipeline for all 6 date filter combinations, and stores the results\nin the SQLite pathway_nodes table. Supports two chart types:\n- \"directory\": Trust ÔåÆ Directory ÔåÆ Drug ÔåÆ Pathway (default)\n- \"indication\": Trust ÔåÆ Search_Term ÔåÆ Drug ÔåÆ Pathway (requires GP diagnosis lookup)\n\nUsage:\n    python -m cli.refresh_pathways\n    python -m cli.refresh_pathways --minimum-patients 10\n    python -m cli.refresh_pathways --provider-codes RGT,RM1\n    python -m cli.refresh_pathways --chart-type all\n    python -m cli.refresh_pathways --chart-type directory\n    python -m cli.refresh_pathways --dry-run\n\nRun `python -m cli.refresh_pathways --help` for full options.\n\"\"\"\n\nimport argparse\nimport json\nimport sqlite3\nimport sys\nimport time\nimport uuid\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom core import PathConfig, default_paths\nfrom core.logging_config import get_logger, setup_logging\nfrom data_processing.database import DatabaseManager, DatabaseConfig\nfrom data_processing.schema import (\n    clear_pathway_nodes,\n    get_pathway_table_counts,\n    verify_pathway_tables_exist,\n    create_pathway_tables,\n)\nfrom data_processing.pathway_pipeline import (\n    ChartType,\n    DATE_FILTER_CONFIGS,\n    fetch_and_transform_data,\n    process_all_date_filters,\n    process_pathway_for_date_filter,\n    process_indication_pathway_for_date_filter,\n    extract_denormalized_fields,\n    extract_indication_fields,\n    convert_to_records,\n)\nfrom data_processing.diagnosis_lookup import get_patient_indication_groups\n\nlogger = get_logger(__name__)\n\n\ndef get_default_filters(paths: PathConfig) -> tuple[list[str], list[str], list[str]]:\n    \"\"\"\n    Load default filter values from reference files.\n\n    Returns:\n        Tuple of (trust_filter, drug_filter, directory_filter)\n    \"\"\"\n    import pandas as pd\n\n    # Load default trusts\n    trust_filter = []\n    if paths.default_trusts_csv.exists():\n        try:\n            trusts_df = pd.read_csv(paths.default_trusts_csv)\n            # Use the \"Name\" column which contains trust names\n            if 'Name' in trusts_df.columns:\n                trust_filter = trusts_df['Name'].dropna().tolist()\n            else:\n                # Fallback to first column if no Name column\n                trust_filter = trusts_df.iloc[:, 0].dropna().tolist()\n            logger.info(f\"Loaded {len(trust_filter)} default trusts\")\n        except Exception as e:\n            logger.warning(f\"Could not load default trusts: {e}\")\n\n    # Load default drugs (Include=1 in include.csv)\n    drug_filter = []\n    if paths.include_csv.exists():\n        try:\n            drugs_df = pd.read_csv(paths.include_csv)\n            if 'Include' in drugs_df.columns:\n                drug_filter = drugs_df[drugs_df['Include'] == 1].iloc[:, 0].dropna().tolist()\n            else:\n                # Assume first column contains drug names if no Include column\n                drug_filter = drugs_df.iloc[:, 0].dropna().tolist()\n            logger.info(f\"Loaded {len(drug_filter)} default drugs\")\n        except Exception as e:\n            logger.warning(f\"Could not load default drugs: {e}\")\n\n    # Load default directories\n    directory_filter = []\n    if paths.directory_list_csv.exists():\n        try:\n            dirs_df = pd.read_csv(paths.directory_list_csv)\n            # Assume first column contains directory names\n            directory_filter = dirs_df.iloc[:, 0].dropna().tolist()\n            logger.info(f\"Loaded {len(directory_filter)} default directories\")\n        except Exception as e:\n            logger.warning(f\"Could not load default directories: {e}\")\n\n    return trust_filter, drug_filter, directory_filter\n\n\ndef insert_pathway_records(\n    conn: sqlite3.Connection,\n    records: list[dict],\n) -> int:\n    \"\"\"\n    Insert pathway records into pathway_nodes table.\n\n    Uses INSERT OR REPLACE to handle updates to existing records.\n\n    Args:\n        conn: SQLite connection\n        records: List of record dicts from convert_to_records()\n\n    Returns:\n        Number of records inserted\n    \"\"\"\n    if not records:\n        return 0\n\n    # Column order matching pathway_nodes schema (includes chart_type)\n    columns = [\n        'date_filter_id', 'chart_type', 'parents', 'ids', 'labels', 'level',\n        'value', 'cost', 'costpp', 'cost_pp_pa', 'colour',\n        'first_seen', 'last_seen', 'first_seen_parent', 'last_seen_parent',\n        'average_spacing', 'average_administered', 'avg_days',\n        'trust_name', 'directory', 'drug_sequence', 'data_refresh_id'\n    ]\n\n    placeholders = ', '.join(['?' for _ in columns])\n    column_names = ', '.join(columns)\n\n    insert_sql = f\"\"\"\n        INSERT OR REPLACE INTO pathway_nodes ({column_names})\n        VALUES ({placeholders})\n    \"\"\"\n\n    # Convert records to tuples in column order\n    rows = []\n    for record in records:\n        row = tuple(record.get(col) for col in columns)\n        rows.append(row)\n\n    cursor = conn.executemany(insert_sql, rows)\n    return cursor.rowcount\n\n\ndef log_refresh_start(\n    conn: sqlite3.Connection,\n    refresh_id: str,\n    date_from: Optional[str] = None,\n    date_to: Optional[str] = None,\n) -> None:\n    \"\"\"Log the start of a refresh operation.\"\"\"\n    conn.execute(\"\"\"\n        INSERT INTO pathway_refresh_log\n        (refresh_id, started_at, status, snowflake_query_date_from, snowflake_query_date_to)\n        VALUES (?, ?, 'running', ?, ?)\n    \"\"\", (refresh_id, datetime.now().isoformat(), date_from, date_to))\n    conn.commit()\n\n\ndef log_refresh_complete(\n    conn: sqlite3.Connection,\n    refresh_id: str,\n    record_count: int,\n    date_filter_counts: dict[str, int],\n    duration_seconds: float,\n) -> None:\n    \"\"\"Log the successful completion of a refresh operation.\"\"\"\n    conn.execute(\"\"\"\n        UPDATE pathway_refresh_log\n        SET completed_at = ?,\n            status = 'completed',\n            record_count = ?,\n            date_filter_counts = ?,\n            processing_duration_seconds = ?\n        WHERE refresh_id = ?\n    \"\"\", (\n        datetime.now().isoformat(),\n        record_count,\n        json.dumps(date_filter_counts),\n        duration_seconds,\n        refresh_id,\n    ))\n    conn.commit()\n\n\ndef log_refresh_failed(\n    conn: sqlite3.Connection,\n    refresh_id: str,\n    error_message: str,\n    duration_seconds: float,\n) -> None:\n    \"\"\"Log a failed refresh operation.\"\"\"\n    conn.execute(\"\"\"\n        UPDATE pathway_refresh_log\n        SET completed_at = ?,\n            status = 'failed',\n            error_message = ?,\n            processing_duration_seconds = ?\n        WHERE refresh_id = ?\n    \"\"\", (\n        datetime.now().isoformat(),\n        error_message,\n        duration_seconds,\n        refresh_id,\n    ))\n    conn.commit()\n\n\ndef refresh_pathways(\n    minimum_patients: int = 5,\n    provider_codes: Optional[list[str]] = None,\n    trust_filter: Optional[list[str]] = None,\n    drug_filter: Optional[list[str]] = None,\n    directory_filter: Optional[list[str]] = None,\n    db_path: Optional[Path] = None,\n    paths: Optional[PathConfig] = None,\n    dry_run: bool = False,\n    chart_type: str = \"directory\",\n) -> tuple[bool, str, dict]:\n    \"\"\"\n    Main refresh function that orchestrates the full pipeline.\n\n    Args:\n        minimum_patients: Minimum patients to include a pathway\n        provider_codes: List of provider codes to filter Snowflake query\n        trust_filter: List of trust names to include in pathways\n        drug_filter: List of drug names to include in pathways\n        directory_filter: List of directories to include in pathways\n        db_path: Path to SQLite database (uses default if None)\n        paths: PathConfig for file paths\n        dry_run: If True, don't actually insert records\n        chart_type: Which chart type to process: \"directory\", \"indication\", or \"all\"\n\n    Returns:\n        Tuple of (success: bool, message: str, stats: dict)\n    \"\"\"\n    if paths is None:\n        paths = default_paths\n\n    # Set up database connection\n    if db_path:\n        db_config = DatabaseConfig(db_path=db_path)\n    else:\n        db_config = DatabaseConfig(data_dir=paths.data_dir)\n\n    db_manager = DatabaseManager(db_config)\n\n    # Load default filters if not provided\n    default_trusts, default_drugs, default_dirs = get_default_filters(paths)\n\n    if trust_filter is None:\n        trust_filter = default_trusts\n    if drug_filter is None:\n        drug_filter = default_drugs\n    if directory_filter is None:\n        directory_filter = default_dirs\n\n    # Ensure we have some filters\n    if not drug_filter:\n        return False, \"No drugs specified and could not load defaults\", {}\n\n    # Determine which chart types to process\n    if chart_type == \"all\":\n        chart_types_to_process: list[ChartType] = [\"directory\", \"indication\"]\n    else:\n        chart_types_to_process = [chart_type]  # type: ignore\n\n    logger.info(\"=\" * 60)\n    logger.info(\"Pathway Data Refresh Starting\")\n    logger.info(\"=\" * 60)\n    logger.info(f\"Minimum patients: {minimum_patients}\")\n    logger.info(f\"Trust filter: {len(trust_filter)} trusts\")\n    logger.info(f\"Drug filter: {len(drug_filter)} drugs\")\n    logger.info(f\"Directory filter: {len(directory_filter)} directories\")\n    logger.info(f\"Provider codes: {provider_codes or 'All'}\")\n    logger.info(f\"Chart type(s): {', '.join(chart_types_to_process)}\")\n    logger.info(f\"Database: {db_manager.db_path}\")\n    logger.info(f\"Dry run: {dry_run}\")\n    logger.info(\"=\" * 60)\n\n    start_time = time.time()\n    refresh_id = str(uuid.uuid4())[:8]\n    stats = {\n        \"refresh_id\": refresh_id,\n        \"date_filter_counts\": {},\n        \"total_records\": 0,\n        \"snowflake_rows\": 0,\n    }\n\n    try:\n        # Verify database and tables\n        with db_manager.get_connection() as conn:\n            missing_tables = verify_pathway_tables_exist(conn)\n            if missing_tables:\n                logger.info(f\"Creating missing tables: {missing_tables}\")\n                create_pathway_tables(conn)\n\n            # Log refresh start\n            if not dry_run:\n                log_refresh_start(conn, refresh_id)\n\n        # Step 1: Fetch data from Snowflake\n        logger.info(\"\")\n        logger.info(\"Step 1/4: Fetching data from Snowflake...\")\n        df = fetch_and_transform_data(\n            provider_codes=provider_codes,\n            paths=paths,\n        )\n\n        if df.empty:\n            msg = \"No data returned from Snowflake\"\n            logger.error(msg)\n            with db_manager.get_connection() as conn:\n                log_refresh_failed(conn, refresh_id, msg, time.time() - start_time)\n            return False, msg, stats\n\n        stats[\"snowflake_rows\"] = len(df)\n        logger.info(f\"Fetched {len(df)} records from Snowflake\")\n\n        # Step 2: Process all date filters for each chart type\n        num_date_filters = len(DATE_FILTER_CONFIGS)\n        num_chart_types = len(chart_types_to_process)\n        total_datasets = num_date_filters * num_chart_types\n\n        logger.info(\"\")\n        logger.info(f\"Step 2/4: Processing pathway data for {total_datasets} datasets \"\n                   f\"({num_date_filters} date filters x {num_chart_types} chart types)...\")\n\n        # Store results keyed by \"date_filter_id:chart_type\"\n        results: dict[str, list[dict]] = {}\n\n        for current_chart_type in chart_types_to_process:\n            logger.info(\"\")\n            logger.info(f\"Processing chart type: {current_chart_type}\")\n\n            if current_chart_type == \"directory\":\n                # Use existing process_all_date_filters for directory charts\n                dir_results = process_all_date_filters(\n                    df=df,\n                    trust_filter=trust_filter,\n                    drug_filter=drug_filter,\n                    directory_filter=directory_filter,\n                    minimum_patients=minimum_patients,\n                    refresh_id=refresh_id,\n                    paths=paths,\n                )\n                # Add results with chart_type suffix\n                for filter_id, records in dir_results.items():\n                    # Records already have chart_type set by convert_to_records\n                    results[f\"{filter_id}:directory\"] = records\n\n            elif current_chart_type == \"indication\":\n                # For indication charts, we need to look up GP diagnoses for all patients\n                # using the new Snowflake-direct approach via get_patient_indication_groups()\n                logger.info(\"Building indication groups from GP diagnosis lookups (Snowflake-direct)...\")\n\n                # Check Snowflake availability\n                from data_processing.snowflake_connector import get_connector, is_snowflake_available\n\n                if not is_snowflake_available():\n                    logger.warning(\"Snowflake not available - cannot process indication charts\")\n                    for config in DATE_FILTER_CONFIGS:\n                        results[f\"{config.id}:indication\"] = []\n                    continue\n\n                try:\n                    import pandas as pd\n                    connector = get_connector()\n\n                    # Step 1: Extract unique PseudoNHSNoLinked values from df\n                    # This is the patient identifier that matches PatientPseudonym in GP records\n                    if 'PseudoNHSNoLinked' not in df.columns:\n                        logger.error(\"DataFrame missing 'PseudoNHSNoLinked' column - cannot lookup GP records\")\n                        for config in DATE_FILTER_CONFIGS:\n                            results[f\"{config.id}:indication\"] = []\n                        continue\n\n                    # Get unique patient pseudonyms for GP lookup (avoid redundant queries)\n                    patient_pseudonyms = df['PseudoNHSNoLinked'].dropna().unique().tolist()\n\n                    logger.info(f\"Looking up GP diagnoses for {len(patient_pseudonyms)} unique patients...\")\n\n                    # Step 2: Call the new Snowflake-direct indication lookup\n                    gp_matches_df = get_patient_indication_groups(\n                        patient_pseudonyms=patient_pseudonyms,\n                        connector=connector,\n                        batch_size=500,\n                    )\n\n                    # Step 3: Build indication_df mapping UPID -> Indication_Group\n                    # For matched patients: Indication_Group = Search_Term\n                    # For unmatched patients: Indication_Group = Directory + \" (no GP dx)\"\n                    #\n                    # IMPORTANT: We need ALL unique UPIDs, not just unique PseudoNHSNoLinked.\n                    # A patient can have multiple UPIDs if they visited multiple providers.\n\n                    # Get all unique UPID records with their PseudoNHSNoLinked and Directory\n                    upid_lookup = df[['UPID', 'PseudoNHSNoLinked', 'Directory']].drop_duplicates(\n                        subset=['UPID']\n                    ).copy()\n\n                    if gp_matches_df.empty:\n                        logger.warning(\"No GP matches found - all patients will use fallback directory\")\n                        # All patients use fallback\n                        indication_records = []\n                        for _, row in upid_lookup.iterrows():\n                            directory = row['Directory']\n                            indication_records.append({\n                                'UPID': row['UPID'],\n                                'Indication_Group': str(directory) + \" (no GP dx)\" if pd.notna(directory) else \"UNKNOWN (no GP dx)\",\n                                'Source': 'FALLBACK',\n                            })\n                        indication_df = pd.DataFrame(indication_records)\n                    else:\n                        # Create lookup dict: PseudoNHSNoLinked -> Search_Term\n                        match_lookup = dict(zip(\n                            gp_matches_df['PatientPseudonym'],\n                            gp_matches_df['Search_Term']\n                        ))\n\n                        # Build indication records for each unique UPID\n                        indication_records = []\n                        for _, row in upid_lookup.iterrows():\n                            pseudo = row['PseudoNHSNoLinked']\n                            upid = row['UPID']\n                            directory = row['Directory']\n\n                            if pd.notna(pseudo) and pseudo in match_lookup:\n                                indication_records.append({\n                                    'UPID': upid,\n                                    'Indication_Group': match_lookup[pseudo],\n                                    'Source': 'DIAGNOSIS',\n                                })\n                            else:\n                                # Use fallback: Directory + \" (no GP dx)\"\n                                fallback_label = str(directory) + \" (no GP dx)\" if pd.notna(directory) else \"UNKNOWN (no GP dx)\"\n                                indication_records.append({\n                                    'UPID': upid,\n                                    'Indication_Group': fallback_label,\n                                    'Source': 'FALLBACK',\n                                })\n\n                        indication_df = pd.DataFrame(indication_records)\n\n                    # Log coverage statistics\n                    if not indication_df.empty:\n                        diagnosis_count = (indication_df['Source'] == 'DIAGNOSIS').sum()\n                        fallback_count = (indication_df['Source'] == 'FALLBACK').sum()\n                        total = len(indication_df)\n                        stats[\"diagnosis_coverage\"] = {\n                            \"diagnosis\": int(diagnosis_count),\n                            \"fallback\": int(fallback_count),\n                            \"total\": total,\n                            \"diagnosis_pct\": round(100 * diagnosis_count / total, 1) if total > 0 else 0,\n                        }\n                        logger.info(f\"Indication coverage: {diagnosis_count}/{total} ({stats['diagnosis_coverage']['diagnosis_pct']}%) diagnosis-matched\")\n\n                        # Log top indication groups\n                        top_indications = indication_df[indication_df['Source'] == 'DIAGNOSIS']['Indication_Group'].value_counts().head(5)\n                        if len(top_indications) > 0:\n                            logger.info(f\"Top 5 indications: {dict(top_indications)}\")\n\n                        # Rename column for compatibility with generate_icicle_chart_indication\n                        # It expects indication_df to have 'Directory' column (mapped from Indication_Group)\n                        indication_df_for_chart = indication_df[['UPID', 'Indication_Group']].copy()\n                        indication_df_for_chart = indication_df_for_chart.rename(columns={'Indication_Group': 'Directory'})\n                        # Ensure unique UPID index (build_hierarchy requires uniquely valued Index)\n                        # Keep first occurrence - DIAGNOSIS entries should come before FALLBACK\n                        indication_df_for_chart = indication_df_for_chart.drop_duplicates(subset=['UPID'], keep='first')\n                        indication_df_for_chart = indication_df_for_chart.set_index('UPID')\n\n                        # Process each date filter with indication grouping\n                        for config in DATE_FILTER_CONFIGS:\n                            logger.info(f\"Processing indication pathway for {config.id}\")\n\n                            ice_df = process_indication_pathway_for_date_filter(\n                                df=df,\n                                indication_df=indication_df_for_chart,\n                                config=config,\n                                trust_filter=trust_filter,\n                                drug_filter=drug_filter,\n                                directory_filter=directory_filter,\n                                minimum_patients=minimum_patients,\n                                paths=paths,\n                            )\n\n                            if ice_df is None:\n                                logger.warning(f\"No indication pathway data for {config.id}\")\n                                results[f\"{config.id}:indication\"] = []\n                                continue\n\n                            # Extract denormalized fields (using indication variant)\n                            ice_df = extract_indication_fields(ice_df)\n\n                            # Convert to records with chart_type=\"indication\"\n                            records = convert_to_records(ice_df, config.id, refresh_id, chart_type=\"indication\")\n                            results[f\"{config.id}:indication\"] = records\n\n                            logger.info(f\"Completed {config.id}:indication: {len(records)} nodes\")\n                    else:\n                        logger.warning(\"Empty indication_df - skipping indication charts\")\n                        for config in DATE_FILTER_CONFIGS:\n                            results[f\"{config.id}:indication\"] = []\n\n                except Exception as e:\n                    logger.error(f\"Error processing indication charts: {e}\")\n                    logger.exception(e)\n                    for config in DATE_FILTER_CONFIGS:\n                        results[f\"{config.id}:indication\"] = []\n\n        # Count records per filter and chart type\n        stats[\"chart_type_counts\"] = {}\n        for key, records in results.items():\n            stats[\"date_filter_counts\"][key] = len(records)\n            stats[\"total_records\"] += len(records)\n            # Also track by chart type\n            _, ct = key.split(\":\")\n            stats[\"chart_type_counts\"][ct] = stats[\"chart_type_counts\"].get(ct, 0) + len(records)\n\n        logger.info(\"\")\n        logger.info(f\"Processed {stats['total_records']} total pathway nodes\")\n        for chart_type_name, count in stats.get(\"chart_type_counts\", {}).items():\n            logger.info(f\"  {chart_type_name}: {count} nodes total\")\n        for key, count in sorted(stats[\"date_filter_counts\"].items()):\n            if count > 0:\n                logger.info(f\"    {key}: {count} nodes\")\n\n        if dry_run:\n            logger.info(\"\")\n            logger.info(\"DRY RUN - Skipping database insertion\")\n            elapsed = time.time() - start_time\n            return True, f\"Dry run complete: {stats['total_records']} records would be inserted\", stats\n\n        # Step 3: Clear existing data and insert new records\n        logger.info(\"\")\n        logger.info(\"Step 3/4: Clearing existing pathway data and inserting new records...\")\n\n        with db_manager.get_transaction() as conn:\n            # Clear all existing pathway nodes\n            deleted = clear_pathway_nodes(conn)\n            logger.info(f\"Cleared {deleted} existing pathway nodes\")\n\n            # Insert new records for each date filter + chart type combination\n            total_inserted = 0\n            for key, records in results.items():\n                if records:\n                    inserted = insert_pathway_records(conn, records)\n                    total_inserted += len(records)\n                    logger.info(f\"  Inserted {len(records)} records for {key}\")\n\n        # Step 4: Log completion\n        logger.info(\"\")\n        logger.info(\"Step 4/4: Logging refresh completion...\")\n\n        elapsed = time.time() - start_time\n\n        with db_manager.get_connection() as conn:\n            log_refresh_complete(\n                conn=conn,\n                refresh_id=refresh_id,\n                record_count=stats[\"total_records\"],\n                date_filter_counts=stats[\"date_filter_counts\"],\n                duration_seconds=elapsed,\n            )\n\n            # Verify final counts\n            counts = get_pathway_table_counts(conn)\n            logger.info(f\"Final table counts: {counts}\")\n\n        logger.info(\"\")\n        logger.info(\"=\" * 60)\n        logger.info(f\"Refresh completed successfully in {elapsed:.1f} seconds\")\n        logger.info(f\"Total records: {stats['total_records']}\")\n        logger.info(f\"Refresh ID: {refresh_id}\")\n        logger.info(\"=\" * 60)\n\n        return True, f\"Refresh complete: {stats['total_records']} records in {elapsed:.1f}s\", stats\n\n    except Exception as e:\n        elapsed = time.time() - start_time\n        error_msg = f\"Refresh failed: {e}\"\n        logger.error(error_msg, exc_info=True)\n\n        try:\n            with db_manager.get_connection() as conn:\n                log_refresh_failed(conn, refresh_id, str(e), elapsed)\n        except Exception:\n            pass  # Don't fail the error handling\n\n        return False, error_msg, stats\n\n\ndef main() -> int:\n    \"\"\"CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Refresh pathway data from Snowflake\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n    # Basic refresh with defaults (directory chart only)\n    python -m cli.refresh_pathways\n\n    # Refresh both chart types (directory and indication)\n    python -m cli.refresh_pathways --chart-type all\n\n    # Refresh only indication-based charts\n    python -m cli.refresh_pathways --chart-type indication\n\n    # Refresh with custom minimum patients\n    python -m cli.refresh_pathways --minimum-patients 10\n\n    # Refresh specific providers only\n    python -m cli.refresh_pathways --provider-codes RGT,RM1\n\n    # Dry run to see what would be processed\n    python -m cli.refresh_pathways --dry-run\n\n    # Verbose output\n    python -m cli.refresh_pathways --verbose\n        \"\"\"\n    )\n\n    parser.add_argument(\n        \"--minimum-patients\",\n        type=int,\n        default=5,\n        help=\"Minimum patients to include a pathway (default: 5)\"\n    )\n\n    parser.add_argument(\n        \"--provider-codes\",\n        type=str,\n        default=None,\n        help=\"Comma-separated list of provider codes to filter (default: all)\"\n    )\n\n    parser.add_argument(\n        \"--db-path\",\n        type=str,\n        default=None,\n        help=\"Path to SQLite database (default: data/pathways.db)\"\n    )\n\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"Process data but don't insert into database\"\n    )\n\n    parser.add_argument(\n        \"--chart-type\",\n        type=str,\n        choices=[\"directory\", \"indication\", \"all\"],\n        default=\"directory\",\n        help=\"Chart type to process: 'directory' (default), 'indication', or 'all'\"\n    )\n\n    parser.add_argument(\n        \"--verbose\", \"-v\",\n        action=\"store_true\",\n        help=\"Enable verbose logging\"\n    )\n\n    args = parser.parse_args()\n\n    # Configure logging\n    import logging\n    log_level = logging.DEBUG if args.verbose else logging.INFO\n    setup_logging(level=log_level)\n\n    # Parse provider codes\n    provider_codes = None\n    if args.provider_codes:\n        provider_codes = [code.strip() for code in args.provider_codes.split(\",\")]\n\n    # Parse db path\n    db_path = Path(args.db_path) if args.db_path else None\n\n    # Run the refresh\n    success, message, stats = refresh_pathways(\n        minimum_patients=args.minimum_patients,\n        provider_codes=provider_codes,\n        db_path=db_path,\n        dry_run=args.dry_run,\n        chart_type=args.chart_type,\n    )\n\n    if success:\n        print(f\"\\n[OK] {message}\")\n        return 0\n    else:\n        print(f\"\\n[FAILED] {message}\", file=sys.stderr)\n        return 1\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n","numLines":707,"startLine":1,"totalLines":707}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01SPtt3ThiBTCY77ANd815S5","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have the full picture. I can see the critical issue in `refresh_pathways.py` lines 424-428 ÔÇö `dict(zip(...))` which only keeps the LAST match per patient (since the new query returns multiple rows per patient). This will be fixed in Task 3.1.\n\nNow I'm ready to build Task 2.1: `assign_drug_indications()`. Let me mark it in-progress and start implementing."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":9879,"cache_read_input_tokens":107086,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":9879},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"b6e01980-bf82-407e-a081-407f533bef54"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01SPtt3ThiBTCY77ANd815S5","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Tvxan4PLS5bb3jjqPQPpwQ","name":"TodoWrite","input":{"todos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"in_progress","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"pending","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"pending","activeForm":"Committing and updating progress"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":9879,"cache_read_input_tokens":107086,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":9879},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"85172ba8-1530-4707-a2f6-423891f06a4b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Tvxan4PLS5bb3jjqPQPpwQ","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"b3855b0b-b9b7-485d-b067-af82b023f2e8","tool_use_result":{"oldTodos":[],"newTodos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"in_progress","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"pending","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"pending","activeForm":"Committing and updating progress"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_016fakBqsAyPbU7bQnVJ3sqL","type":"message","role":"assistant","content":[{"type":"text","text":"Now implementing the `assign_drug_indications()` function. The key logic:\n\n1. For each UPID + Drug Name pair in the HCD data:\n   - Look up patient's GP-matched Search_Terms (from gp_matches_df via PseudoNHSNoLinked)\n   - Look up which Search_Terms list this drug (from drug mapping)\n   - Intersect ÔåÆ valid indications\n   - Pick highest code_frequency as tiebreaker (or alphabetical if tied)\n   - Modify UPID: `{UPID}|{search_term}` or fallback `{UPID}|{Directory} (no GP dx)`\n\n2. Build indication_df mapping modified UPID ÔåÆ Search_Term\n\nI'll add this function to `diagnosis_lookup.py` since that's where all the related functions live."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":354,"cache_read_input_tokens":116965,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":354},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"135bcce8-94d3-4364-a08d-accbdb1b543f"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_016fakBqsAyPbU7bQnVJ3sqL","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0178KQTzfJyHJHykaxL7Tsg5","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","offset":1090,"limit":50}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":354,"cache_read_input_tokens":116965,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":354},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"26e2507e-9b60-471e-abc2-39bd5e100936"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0178KQTzfJyHJHykaxL7Tsg5","type":"tool_result","content":"  1090ÔåÆ# === Drug-to-indication mapping from DimSearchTerm.csv ===\n  1091ÔåÆ\n  1092ÔåÆ\n  1093ÔåÆ# Merge related Search_Terms into canonical names.\n  1094ÔåÆ# Asthma variants are clinically the same condition at different severity levels.\n  1095ÔåÆ# Urticaria is a separate condition ÔÇö do NOT merge with asthma.\n  1096ÔåÆSEARCH_TERM_MERGE_MAP: dict[str, str] = {\n  1097ÔåÆ    \"allergic asthma\": \"asthma\",\n  1098ÔåÆ    \"severe persistent allergic asthma\": \"asthma\",\n  1099ÔåÆ}\n  1100ÔåÆ\n  1101ÔåÆ\n  1102ÔåÆdef load_drug_indication_mapping(\n  1103ÔåÆ    csv_path: Optional[str] = None,\n  1104ÔåÆ) -> tuple[dict[str, list[str]], dict[str, list[str]]]:\n  1105ÔåÆ    \"\"\"\n  1106ÔåÆ    Load the drug-to-Search_Term mapping from DimSearchTerm.csv.\n  1107ÔåÆ\n  1108ÔåÆ    Builds two lookup dicts:\n  1109ÔåÆ    - fragment_to_search_terms: drug fragment (UPPERCASE) -> list of Search_Terms containing it\n  1110ÔåÆ    - search_term_to_fragments: search_term -> list of drug fragments (UPPERCASE)\n  1111ÔåÆ\n  1112ÔåÆ    DimSearchTerm.csv has columns: Search_Term, CleanedDrugName, PrimaryDirectorate\n  1113ÔåÆ    CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\").\n  1114ÔåÆ\n  1115ÔåÆ    Note: A Search_Term can appear multiple times with different PrimaryDirectorates\n  1116ÔåÆ    (e.g., \"diabetes\" appears under both DIABETIC MEDICINE and OPHTHALMOLOGY).\n  1117ÔåÆ    Drug fragments from all rows for the same Search_Term are combined.\n  1118ÔåÆ\n  1119ÔåÆ    Asthma-related Search_Terms (\"allergic asthma\", \"severe persistent allergic asthma\")\n  1120ÔåÆ    are merged into \"asthma\" to match the CLUSTER_MAPPING_SQL normalization.\n  1121ÔåÆ    \"urticaria\" stays separate.\n  1122ÔåÆ\n  1123ÔåÆ    Args:\n  1124ÔåÆ        csv_path: Path to DimSearchTerm.csv. Defaults to data/DimSearchTerm.csv.\n  1125ÔåÆ\n  1126ÔåÆ    Returns:\n  1127ÔåÆ        Tuple of (fragment_to_search_terms, search_term_to_fragments)\n  1128ÔåÆ    \"\"\"\n  1129ÔåÆ    if csv_path is None:\n  1130ÔåÆ        csv_path = str(Path(__file__).parent.parent / \"data\" / \"DimSearchTerm.csv\")\n  1131ÔåÆ\n  1132ÔåÆ    fragment_to_search_terms: dict[str, list[str]] = {}\n  1133ÔåÆ    search_term_to_fragments: dict[str, list[str]] = {}\n  1134ÔåÆ\n  1135ÔåÆ    try:\n  1136ÔåÆ        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n  1137ÔåÆ            reader = csv.DictReader(f)\n  1138ÔåÆ            for row in reader:\n  1139ÔåÆ                search_term = row.get(\"Search_Term\", \"\").strip()\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"2101dcfa-c9cf-4039-adf1-80c15606767f","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","content":"# === Drug-to-indication mapping from DimSearchTerm.csv ===\n\n\n# Merge related Search_Terms into canonical names.\n# Asthma variants are clinically the same condition at different severity levels.\n# Urticaria is a separate condition ÔÇö do NOT merge with asthma.\nSEARCH_TERM_MERGE_MAP: dict[str, str] = {\n    \"allergic asthma\": \"asthma\",\n    \"severe persistent allergic asthma\": \"asthma\",\n}\n\n\ndef load_drug_indication_mapping(\n    csv_path: Optional[str] = None,\n) -> tuple[dict[str, list[str]], dict[str, list[str]]]:\n    \"\"\"\n    Load the drug-to-Search_Term mapping from DimSearchTerm.csv.\n\n    Builds two lookup dicts:\n    - fragment_to_search_terms: drug fragment (UPPERCASE) -> list of Search_Terms containing it\n    - search_term_to_fragments: search_term -> list of drug fragments (UPPERCASE)\n\n    DimSearchTerm.csv has columns: Search_Term, CleanedDrugName, PrimaryDirectorate\n    CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\").\n\n    Note: A Search_Term can appear multiple times with different PrimaryDirectorates\n    (e.g., \"diabetes\" appears under both DIABETIC MEDICINE and OPHTHALMOLOGY).\n    Drug fragments from all rows for the same Search_Term are combined.\n\n    Asthma-related Search_Terms (\"allergic asthma\", \"severe persistent allergic asthma\")\n    are merged into \"asthma\" to match the CLUSTER_MAPPING_SQL normalization.\n    \"urticaria\" stays separate.\n\n    Args:\n        csv_path: Path to DimSearchTerm.csv. Defaults to data/DimSearchTerm.csv.\n\n    Returns:\n        Tuple of (fragment_to_search_terms, search_term_to_fragments)\n    \"\"\"\n    if csv_path is None:\n        csv_path = str(Path(__file__).parent.parent / \"data\" / \"DimSearchTerm.csv\")\n\n    fragment_to_search_terms: dict[str, list[str]] = {}\n    search_term_to_fragments: dict[str, list[str]] = {}\n\n    try:\n        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                search_term = row.get(\"Search_Term\", \"\").strip()","numLines":50,"startLine":1090,"totalLines":1574}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_011qD5mM6NtHkXkRkCqMvMJj","type":"message","role":"assistant","content":[{"type":"text","text":"Now I'll add the `assign_drug_indications()` function. I'll insert it after `get_search_terms_for_drug()` (line 1204) and before the `CLUSTER_MAPPING_SQL` section (line 1207)."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1251,"cache_read_input_tokens":117319,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1251},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"a7751ce2-c8d0-4c9f-9803-930a7b142b4d"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_011qD5mM6NtHkXkRkCqMvMJj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019G9MeAokjyqZgA1REbfsS4","name":"Edit","input":{"replace_all":false,"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","old_string":"    return matched_terms\n\n\n# === NEW APPROACH: Query Snowflake directly using cluster CTE ===","new_string":"    return matched_terms\n\n\ndef assign_drug_indications(\n    df: \"pd.DataFrame\",\n    gp_matches_df: \"pd.DataFrame\",\n    search_term_to_fragments: dict[str, list[str]],\n) -> \"tuple[pd.DataFrame, pd.DataFrame]\":\n    \"\"\"\n    Assign drug-aware indications by cross-referencing GP diagnoses with drug mappings.\n\n    For each UPID + Drug Name pair in the HCD data:\n    1. Look up patient's GP-matched Search_Terms (via PseudoNHSNoLinked ÔåÆ gp_matches_df)\n    2. Look up which Search_Terms list this drug (via search_term_to_fragments)\n    3. Intersect = valid indications for this drug-patient pair\n    4. If 1 match: use it\n    5. If multiple: pick highest code_frequency (most GP coding = most likely indication)\n    6. If tied frequency: alphabetical Search_Term for determinism\n    7. If 0 matches: fallback to \"{Directory} (no GP dx)\"\n\n    Modifies UPID to include indication: \"{UPID}|{search_term}\" or \"{UPID}|{Directory} (no GP dx)\".\n    This makes drugs under different indications appear as separate pathways.\n\n    Args:\n        df: HCD DataFrame with columns: UPID, Drug Name, PseudoNHSNoLinked, Directory\n        gp_matches_df: GP matches from get_patient_indication_groups() with columns:\n            PatientPseudonym, Search_Term, code_frequency\n            (multiple rows per patient, one per matched Search_Term)\n        search_term_to_fragments: From load_drug_indication_mapping()[1].\n            Maps search_term -> list of drug fragments (UPPERCASE).\n\n    Returns:\n        Tuple of (modified_df, indication_df):\n        - modified_df: Copy of df with UPID replaced by \"{UPID}|{indication}\"\n        - indication_df: DataFrame indexed by modified UPID with 'Directory' column\n            containing the Search_Term (or fallback label) for pathway hierarchy\n    \"\"\"\n    import pandas as pd\n\n    modified_df = df.copy()\n\n    # Build GP match lookup: PseudoNHSNoLinked -> {Search_Term: code_frequency}\n    gp_lookup: dict[str, dict[str, int]] = {}\n    if not gp_matches_df.empty:\n        for _, row in gp_matches_df.iterrows():\n            pseudo = row['PatientPseudonym']\n            term = row['Search_Term']\n            freq = int(row.get('code_frequency', 0))\n            if pseudo not in gp_lookup:\n                gp_lookup[pseudo] = {}\n            gp_lookup[pseudo][term] = freq\n\n    logger.info(f\"GP lookup built: {len(gp_lookup)} patients with GP matches\")\n\n    # Cache drug -> Search_Terms lookups to avoid recomputing per row\n    drug_search_terms_cache: dict[str, list[str]] = {}\n\n    def get_drug_terms(drug_name: str) -> list[str]:\n        if drug_name not in drug_search_terms_cache:\n            drug_search_terms_cache[drug_name] = get_search_terms_for_drug(\n                drug_name, search_term_to_fragments\n            )\n        return drug_search_terms_cache[drug_name]\n\n    # Process each row: determine indication for each UPID + Drug Name\n    # We work at the (UPID, Drug Name) pair level, then apply to all rows\n    # Key: (UPID, Drug Name) -> (matched_search_term_or_fallback, is_diagnosis)\n    pair_indication_cache: dict[tuple[str, str], tuple[str, bool]] = {}\n\n    # Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combos\n    required_cols = ['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']\n    unique_pairs = modified_df[required_cols].drop_duplicates(\n        subset=['UPID', 'Drug Name']\n    )\n\n    match_count = 0\n    fallback_count = 0\n    tiebreak_count = 0\n\n    for _, pair_row in unique_pairs.iterrows():\n        upid = pair_row['UPID']\n        drug_name = pair_row['Drug Name']\n        pseudo = pair_row['PseudoNHSNoLinked']\n        directory = pair_row['Directory']\n\n        # Get Search_Terms this drug maps to\n        drug_terms = get_drug_terms(drug_name)\n\n        # Get patient's GP-matched Search_Terms\n        patient_gp_terms = gp_lookup.get(pseudo, {}) if pd.notna(pseudo) else {}\n\n        # Intersect: Search_Terms that list this drug AND patient has GP dx for\n        valid_indications = {\n            term: patient_gp_terms[term]\n            for term in drug_terms\n            if term in patient_gp_terms\n        }\n\n        if len(valid_indications) == 1:\n            matched_term = next(iter(valid_indications))\n            pair_indication_cache[(upid, drug_name)] = (matched_term, True)\n            match_count += 1\n\n        elif len(valid_indications) > 1:\n            # Tiebreaker: highest code_frequency, then alphabetical\n            sorted_terms = sorted(\n                valid_indications.items(),\n                key=lambda x: (-x[1], x[0]),\n            )\n            matched_term = sorted_terms[0][0]\n            pair_indication_cache[(upid, drug_name)] = (matched_term, True)\n            match_count += 1\n            tiebreak_count += 1\n\n        else:\n            # No intersection: fallback to directory\n            if pd.notna(directory):\n                fallback_label = f\"{directory} (no GP dx)\"\n            else:\n                fallback_label = \"UNKNOWN (no GP dx)\"\n            pair_indication_cache[(upid, drug_name)] = (fallback_label, False)\n            fallback_count += 1\n\n    total_pairs = len(unique_pairs)\n    logger.info(f\"Drug-indication matching complete:\")\n    logger.info(f\"  Total UPID-Drug pairs: {total_pairs}\")\n    logger.info(f\"  Matched (GP dx + drug mapping): {match_count} ({100*match_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Matched: 0\")\n    logger.info(f\"  Tiebreaker used: {tiebreak_count}\")\n    logger.info(f\"  Fallback (no match): {fallback_count} ({100*fallback_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Fallback: 0\")\n\n    # Apply modified UPIDs to all rows\n    # Build vectorized lookup: original UPID + Drug Name -> modified UPID\n    def build_modified_upid(row: \"pd.Series\") -> str:\n        upid = row['UPID']\n        drug_name = row['Drug Name']\n        key = (upid, drug_name)\n        if key in pair_indication_cache:\n            indication, _ = pair_indication_cache[key]\n            return f\"{upid}|{indication}\"\n        # Shouldn't happen, but fallback\n        return f\"{upid}|UNKNOWN (no GP dx)\"\n\n    modified_df['UPID'] = modified_df.apply(build_modified_upid, axis=1)\n\n    # Build indication_df: modified UPID -> Search_Term/fallback label\n    # This maps each unique modified UPID to its indication for the pathway hierarchy\n    indication_records: list[dict[str, str]] = []\n    seen_upids: set[str] = set()\n\n    for (original_upid, drug_name), (indication, is_diagnosis) in pair_indication_cache.items():\n        modified_upid = f\"{original_upid}|{indication}\"\n        if modified_upid not in seen_upids:\n            indication_records.append({\n                'UPID': modified_upid,\n                'Directory': indication,\n            })\n            seen_upids.add(modified_upid)\n\n    indication_df = pd.DataFrame(indication_records)\n    if not indication_df.empty:\n        indication_df = indication_df.set_index('UPID')\n\n    logger.info(f\"  Unique modified UPIDs: {len(indication_df)}\")\n\n    # Log top indications\n    if not indication_df.empty:\n        top_terms = indication_df['Directory'].value_counts().head(5)\n        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n\n    return modified_df, indication_df\n\n\n# === NEW APPROACH: Query Snowflake directly using cluster CTE ==="}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1251,"cache_read_input_tokens":117319,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1251},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"d2363220-a815-458d-a73e-7bc439603b23"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019G9MeAokjyqZgA1REbfsS4","type":"tool_result","content":"The file C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"83befc2a-34db-46a9-85f9-51c36c1276c5","tool_use_result":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","oldString":"    return matched_terms\n\n\n# === NEW APPROACH: Query Snowflake directly using cluster CTE ===","newString":"    return matched_terms\n\n\ndef assign_drug_indications(\n    df: \"pd.DataFrame\",\n    gp_matches_df: \"pd.DataFrame\",\n    search_term_to_fragments: dict[str, list[str]],\n) -> \"tuple[pd.DataFrame, pd.DataFrame]\":\n    \"\"\"\n    Assign drug-aware indications by cross-referencing GP diagnoses with drug mappings.\n\n    For each UPID + Drug Name pair in the HCD data:\n    1. Look up patient's GP-matched Search_Terms (via PseudoNHSNoLinked ÔåÆ gp_matches_df)\n    2. Look up which Search_Terms list this drug (via search_term_to_fragments)\n    3. Intersect = valid indications for this drug-patient pair\n    4. If 1 match: use it\n    5. If multiple: pick highest code_frequency (most GP coding = most likely indication)\n    6. If tied frequency: alphabetical Search_Term for determinism\n    7. If 0 matches: fallback to \"{Directory} (no GP dx)\"\n\n    Modifies UPID to include indication: \"{UPID}|{search_term}\" or \"{UPID}|{Directory} (no GP dx)\".\n    This makes drugs under different indications appear as separate pathways.\n\n    Args:\n        df: HCD DataFrame with columns: UPID, Drug Name, PseudoNHSNoLinked, Directory\n        gp_matches_df: GP matches from get_patient_indication_groups() with columns:\n            PatientPseudonym, Search_Term, code_frequency\n            (multiple rows per patient, one per matched Search_Term)\n        search_term_to_fragments: From load_drug_indication_mapping()[1].\n            Maps search_term -> list of drug fragments (UPPERCASE).\n\n    Returns:\n        Tuple of (modified_df, indication_df):\n        - modified_df: Copy of df with UPID replaced by \"{UPID}|{indication}\"\n        - indication_df: DataFrame indexed by modified UPID with 'Directory' column\n            containing the Search_Term (or fallback label) for pathway hierarchy\n    \"\"\"\n    import pandas as pd\n\n    modified_df = df.copy()\n\n    # Build GP match lookup: PseudoNHSNoLinked -> {Search_Term: code_frequency}\n    gp_lookup: dict[str, dict[str, int]] = {}\n    if not gp_matches_df.empty:\n        for _, row in gp_matches_df.iterrows():\n            pseudo = row['PatientPseudonym']\n            term = row['Search_Term']\n            freq = int(row.get('code_frequency', 0))\n            if pseudo not in gp_lookup:\n                gp_lookup[pseudo] = {}\n            gp_lookup[pseudo][term] = freq\n\n    logger.info(f\"GP lookup built: {len(gp_lookup)} patients with GP matches\")\n\n    # Cache drug -> Search_Terms lookups to avoid recomputing per row\n    drug_search_terms_cache: dict[str, list[str]] = {}\n\n    def get_drug_terms(drug_name: str) -> list[str]:\n        if drug_name not in drug_search_terms_cache:\n            drug_search_terms_cache[drug_name] = get_search_terms_for_drug(\n                drug_name, search_term_to_fragments\n            )\n        return drug_search_terms_cache[drug_name]\n\n    # Process each row: determine indication for each UPID + Drug Name\n    # We work at the (UPID, Drug Name) pair level, then apply to all rows\n    # Key: (UPID, Drug Name) -> (matched_search_term_or_fallback, is_diagnosis)\n    pair_indication_cache: dict[tuple[str, str], tuple[str, bool]] = {}\n\n    # Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combos\n    required_cols = ['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']\n    unique_pairs = modified_df[required_cols].drop_duplicates(\n        subset=['UPID', 'Drug Name']\n    )\n\n    match_count = 0\n    fallback_count = 0\n    tiebreak_count = 0\n\n    for _, pair_row in unique_pairs.iterrows():\n        upid = pair_row['UPID']\n        drug_name = pair_row['Drug Name']\n        pseudo = pair_row['PseudoNHSNoLinked']\n        directory = pair_row['Directory']\n\n        # Get Search_Terms this drug maps to\n        drug_terms = get_drug_terms(drug_name)\n\n        # Get patient's GP-matched Search_Terms\n        patient_gp_terms = gp_lookup.get(pseudo, {}) if pd.notna(pseudo) else {}\n\n        # Intersect: Search_Terms that list this drug AND patient has GP dx for\n        valid_indications = {\n            term: patient_gp_terms[term]\n            for term in drug_terms\n            if term in patient_gp_terms\n        }\n\n        if len(valid_indications) == 1:\n            matched_term = next(iter(valid_indications))\n            pair_indication_cache[(upid, drug_name)] = (matched_term, True)\n            match_count += 1\n\n        elif len(valid_indications) > 1:\n            # Tiebreaker: highest code_frequency, then alphabetical\n            sorted_terms = sorted(\n                valid_indications.items(),\n                key=lambda x: (-x[1], x[0]),\n            )\n            matched_term = sorted_terms[0][0]\n            pair_indication_cache[(upid, drug_name)] = (matched_term, True)\n            match_count += 1\n            tiebreak_count += 1\n\n        else:\n            # No intersection: fallback to directory\n            if pd.notna(directory):\n                fallback_label = f\"{directory} (no GP dx)\"\n            else:\n                fallback_label = \"UNKNOWN (no GP dx)\"\n            pair_indication_cache[(upid, drug_name)] = (fallback_label, False)\n            fallback_count += 1\n\n    total_pairs = len(unique_pairs)\n    logger.info(f\"Drug-indication matching complete:\")\n    logger.info(f\"  Total UPID-Drug pairs: {total_pairs}\")\n    logger.info(f\"  Matched (GP dx + drug mapping): {match_count} ({100*match_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Matched: 0\")\n    logger.info(f\"  Tiebreaker used: {tiebreak_count}\")\n    logger.info(f\"  Fallback (no match): {fallback_count} ({100*fallback_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Fallback: 0\")\n\n    # Apply modified UPIDs to all rows\n    # Build vectorized lookup: original UPID + Drug Name -> modified UPID\n    def build_modified_upid(row: \"pd.Series\") -> str:\n        upid = row['UPID']\n        drug_name = row['Drug Name']\n        key = (upid, drug_name)\n        if key in pair_indication_cache:\n            indication, _ = pair_indication_cache[key]\n            return f\"{upid}|{indication}\"\n        # Shouldn't happen, but fallback\n        return f\"{upid}|UNKNOWN (no GP dx)\"\n\n    modified_df['UPID'] = modified_df.apply(build_modified_upid, axis=1)\n\n    # Build indication_df: modified UPID -> Search_Term/fallback label\n    # This maps each unique modified UPID to its indication for the pathway hierarchy\n    indication_records: list[dict[str, str]] = []\n    seen_upids: set[str] = set()\n\n    for (original_upid, drug_name), (indication, is_diagnosis) in pair_indication_cache.items():\n        modified_upid = f\"{original_upid}|{indication}\"\n        if modified_upid not in seen_upids:\n            indication_records.append({\n                'UPID': modified_upid,\n                'Directory': indication,\n            })\n            seen_upids.add(modified_upid)\n\n    indication_df = pd.DataFrame(indication_records)\n    if not indication_df.empty:\n        indication_df = indication_df.set_index('UPID')\n\n    logger.info(f\"  Unique modified UPIDs: {len(indication_df)}\")\n\n    # Log top indications\n    if not indication_df.empty:\n        top_terms = indication_df['Directory'].value_counts().head(5)\n        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n\n    return modified_df, indication_df\n\n\n# === NEW APPROACH: Query Snowflake directly using cluster CTE ===","originalFile":"\"\"\"\nDiagnosis lookup module for NHS Patient Pathway Analysis.\n\nProvides functions to validate patient indications by checking GP diagnosis records\nagainst SNOMED cluster codes. Uses the drug-to-cluster mapping from\ndrug_indication_clusters.csv and queries Snowflake for SNOMED codes and GP records.\n\nKey workflow:\n1. Get drug's valid indication clusters from local mapping\n2. Get all SNOMED codes for those clusters from Snowflake\n3. Check if patient has any of those SNOMED codes in GP records\n4. Report indication validation status\n\nIMPORTANT: HCD activity data indication codes are UNRELIABLE. This module uses\nGP/Primary Care data (PrimaryCareClinicalCoding) as the authoritative source.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom datetime import date, datetime\nfrom pathlib import Path\nfrom typing import Optional, Callable, Any, cast, TYPE_CHECKING\nimport csv\n\nif TYPE_CHECKING:\n    import pandas as pd\n\nfrom core.logging_config import get_logger\nfrom data_processing.database import DatabaseManager, default_db_manager\nfrom data_processing.snowflake_connector import (\n    SnowflakeConnector,\n    get_connector,\n    is_snowflake_available,\n    is_snowflake_configured,\n    SNOWFLAKE_AVAILABLE,\n)\nfrom data_processing.cache import get_cache, is_cache_enabled\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass ClusterSnomedCodes:\n    \"\"\"SNOMED codes for a clinical coding cluster.\"\"\"\n    cluster_id: str\n    cluster_description: str\n    snomed_codes: list[str] = field(default_factory=list)\n    snomed_descriptions: dict[str, str] = field(default_factory=dict)\n\n    @property\n    def code_count(self) -> int:\n        return len(self.snomed_codes)\n\n\n@dataclass\nclass IndicationValidationResult:\n    \"\"\"Result of validating a patient's indication for a drug.\"\"\"\n    patient_pseudonym: str\n    drug_name: str\n    has_valid_indication: bool\n    matched_cluster_id: Optional[str] = None\n    matched_snomed_code: Optional[str] = None\n    matched_snomed_description: Optional[str] = None\n    checked_clusters: list[str] = field(default_factory=list)\n    total_codes_checked: int = 0\n    source: str = \"GP_SNOMED\"  # GP_SNOMED | NONE\n    error_message: Optional[str] = None\n\n\n@dataclass\nclass DrugIndicationMatchRate:\n    \"\"\"Match rate statistics for a drug's indication validation.\"\"\"\n    drug_name: str\n    total_patients: int\n    patients_with_indication: int\n    patients_without_indication: int\n    match_rate: float  # 0.0 to 1.0\n    clusters_checked: list[str] = field(default_factory=list)\n    sample_unmatched: list[str] = field(default_factory=list)  # Sample patient IDs\n\n\n@dataclass\nclass DrugSnomedMapping:\n    \"\"\"SNOMED code mapping for a drug from ref_drug_snomed_mapping.\"\"\"\n    snomed_code: str\n    snomed_description: str\n    search_term: str\n    primary_directorate: str\n    indication: str = \"\"\n    ta_id: str = \"\"\n\n\n@dataclass\nclass DirectSnomedMatchResult:\n    \"\"\"Result of direct SNOMED code lookup in GP records.\"\"\"\n    patient_pseudonym: str\n    matched: bool\n    snomed_code: Optional[str] = None\n    snomed_description: Optional[str] = None\n    search_term: Optional[str] = None\n    primary_directorate: Optional[str] = None\n    event_date: Optional[datetime] = None\n    source: str = \"DIRECT_SNOMED\"  # DIRECT_SNOMED | NONE\n\n\n@dataclass\nclass DirectorateAssignment:\n    \"\"\"Result of directorate assignment for a patient-drug combination.\"\"\"\n    upid: str\n    drug_name: str\n    directorate: Optional[str]\n    search_term: Optional[str] = None\n    source: str = \"FALLBACK\"  # DIAGNOSIS | FALLBACK\n    snomed_code: Optional[str] = None\n    event_date: Optional[datetime] = None\n\n\ndef get_drug_clusters(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[dict]:\n    \"\"\"\n    Get all SNOMED cluster mappings for a drug from local SQLite.\n\n    Args:\n        drug_name: Drug name to look up (case-insensitive)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n\n    Returns:\n        List of dicts with keys: drug_name, indication, cluster_id,\n        cluster_description, nice_ta_reference\n    \"\"\"\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    query = \"\"\"\n        SELECT drug_name, indication, cluster_id, cluster_description, nice_ta_reference\n        FROM ref_drug_indication_clusters\n        WHERE UPPER(drug_name) = UPPER(?)\n        ORDER BY indication, cluster_id\n    \"\"\"\n\n    try:\n        with db_manager.get_connection() as conn:\n            cursor = conn.execute(query, (drug_name,))\n            rows = cursor.fetchall()\n\n            results = []\n            for row in rows:\n                results.append({\n                    \"drug_name\": row[\"drug_name\"],\n                    \"indication\": row[\"indication\"],\n                    \"cluster_id\": row[\"cluster_id\"],\n                    \"cluster_description\": row[\"cluster_description\"],\n                    \"nice_ta_reference\": row[\"nice_ta_reference\"],\n                })\n\n            logger.debug(f\"Found {len(results)} cluster mappings for drug '{drug_name}'\")\n            return results\n\n    except Exception as e:\n        logger.error(f\"Error getting clusters for drug '{drug_name}': {e}\")\n        return []\n\n\ndef get_drug_cluster_ids(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[str]:\n    \"\"\"\n    Get unique cluster IDs for a drug.\n\n    Args:\n        drug_name: Drug name to look up\n        db_manager: Optional DatabaseManager\n\n    Returns:\n        List of unique cluster IDs\n    \"\"\"\n    clusters = get_drug_clusters(drug_name, db_manager)\n    return list(set(c[\"cluster_id\"] for c in clusters))\n\n\ndef get_drug_snomed_codes(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[DrugSnomedMapping]:\n    \"\"\"\n    Get all SNOMED codes for a drug from local ref_drug_snomed_mapping table.\n\n    This uses the enriched mapping CSV data loaded into SQLite, which provides\n    direct SNOMED-to-drug mappings with Search_Term and PrimaryDirectorate.\n\n    Args:\n        drug_name: Drug name to look up (case-insensitive, matches cleaned_drug_name)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n\n    Returns:\n        List of DrugSnomedMapping with snomed_code, snomed_description,\n        search_term, primary_directorate, indication, ta_id\n    \"\"\"\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    query = \"\"\"\n        SELECT DISTINCT\n            snomed_code,\n            snomed_description,\n            search_term,\n            primary_directorate,\n            indication,\n            ta_id\n        FROM ref_drug_snomed_mapping\n        WHERE UPPER(cleaned_drug_name) = UPPER(?)\n           OR UPPER(drug_name) = UPPER(?)\n        ORDER BY search_term, snomed_code\n    \"\"\"\n\n    try:\n        with db_manager.get_connection() as conn:\n            cursor = conn.execute(query, (drug_name, drug_name))\n            rows = cursor.fetchall()\n\n            results = []\n            for row in rows:\n                results.append(DrugSnomedMapping(\n                    snomed_code=row[\"snomed_code\"],\n                    snomed_description=row[\"snomed_description\"] or \"\",\n                    search_term=row[\"search_term\"] or \"\",\n                    primary_directorate=row[\"primary_directorate\"] or \"\",\n                    indication=row[\"indication\"] or \"\",\n                    ta_id=row[\"ta_id\"] or \"\",\n                ))\n\n            logger.debug(f\"Found {len(results)} SNOMED mappings for drug '{drug_name}'\")\n            return results\n\n    except Exception as e:\n        logger.error(f\"Error getting SNOMED codes for drug '{drug_name}': {e}\")\n        return []\n\n\ndef patient_has_indication_direct(\n    patient_pseudonym: str,\n    drug_snomed_mappings: list[DrugSnomedMapping],\n    connector: Optional[SnowflakeConnector] = None,\n    before_date: Optional[date] = None,\n) -> DirectSnomedMatchResult:\n    \"\"\"\n    Check if patient has any of the SNOMED codes in their GP records.\n\n    This is the direct SNOMED lookup - it queries PrimaryCareClinicalCoding\n    for exact SNOMED code matches (not via cluster). Returns the most recent\n    match by EventDateTime if multiple matches exist.\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        drug_snomed_mappings: List of DrugSnomedMapping from get_drug_snomed_codes()\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        DirectSnomedMatchResult with match details (most recent by EventDateTime)\n    \"\"\"\n    result = DirectSnomedMatchResult(\n        patient_pseudonym=patient_pseudonym,\n        matched=False,\n        source=\"NONE\",\n    )\n\n    if not drug_snomed_mappings:\n        return result\n\n    if not SNOWFLAKE_AVAILABLE:\n        logger.warning(\"Snowflake connector not available\")\n        return result\n\n    if not is_snowflake_configured():\n        logger.warning(\"Snowflake not configured - cannot check GP records\")\n        return result\n\n    if connector is None:\n        connector = get_connector()\n\n    # Build lookup dict for mapping snomed_code -> (search_term, primary_directorate, snomed_description)\n    snomed_lookup = {\n        m.snomed_code: (m.search_term, m.primary_directorate, m.snomed_description)\n        for m in drug_snomed_mappings\n    }\n\n    # Get unique SNOMED codes\n    snomed_codes = list(snomed_lookup.keys())\n\n    # Build placeholders for SNOMED codes\n    placeholders = \", \".join([\"%s\"] * len(snomed_codes))\n\n    # Query to find most recent matching SNOMED code in GP records\n    query = f'''\n        SELECT\n            \"SNOMEDCode\",\n            \"EventDateTime\"\n        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n        WHERE \"PatientPseudonym\" = %s\n            AND \"SNOMEDCode\" IN ({placeholders})\n    '''\n\n    params: list = [patient_pseudonym] + snomed_codes\n\n    if before_date:\n        query += ' AND \"EventDateTime\" < %s'\n        params.append(before_date.isoformat())\n\n    query += ' ORDER BY \"EventDateTime\" DESC LIMIT 1'\n\n    try:\n        results = connector.execute_dict(query, tuple(params))\n\n        if results:\n            row = results[0]\n            matched_code = row.get(\"SNOMEDCode\")\n            event_dt = row.get(\"EventDateTime\")\n\n            if matched_code and matched_code in snomed_lookup:\n                search_term, primary_dir, snomed_desc = snomed_lookup[matched_code]\n\n                return DirectSnomedMatchResult(\n                    patient_pseudonym=patient_pseudonym,\n                    matched=True,\n                    snomed_code=matched_code,\n                    snomed_description=snomed_desc,\n                    search_term=search_term,\n                    primary_directorate=primary_dir,\n                    event_date=event_dt,\n                    source=\"DIRECT_SNOMED\",\n                )\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Error checking direct SNOMED for patient '{patient_pseudonym}': {e}\")\n        return result\n\n\ndef get_directorate_from_diagnosis(\n    upid: str,\n    drug_name: str,\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    before_date: Optional[date] = None,\n) -> DirectorateAssignment:\n    \"\"\"\n    Get directorate assignment for a patient-drug combination using diagnosis-based lookup.\n\n    This function attempts to assign a directorate based on the patient's GP records\n    (direct SNOMED code matching). If no match is found, it returns a FALLBACK result\n    indicating that the caller should use alternative assignment methods (e.g.,\n    department_identification() from tools/data.py).\n\n    Workflow:\n    1. Get all SNOMED codes for the drug from ref_drug_snomed_mapping\n    2. Query patient's GP records for matching SNOMED codes\n    3. If match found ÔåÆ return diagnosis-based directorate and search_term\n    4. If no match ÔåÆ return FALLBACK result (caller handles fallback logic)\n\n    Args:\n        upid: Patient's unique patient ID (Provider Code[:3] + PersonKey)\n        drug_name: Drug name to look up\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        DirectorateAssignment with directorate, search_term, and source\n    \"\"\"\n    result = DirectorateAssignment(\n        upid=upid,\n        drug_name=drug_name,\n        directorate=None,\n        source=\"FALLBACK\",\n    )\n\n    # Step 1: Get SNOMED codes for the drug\n    drug_snomed_mappings = get_drug_snomed_codes(drug_name, db_manager)\n\n    if not drug_snomed_mappings:\n        logger.debug(f\"No SNOMED mappings found for drug '{drug_name}' - using fallback\")\n        return result\n\n    # Step 2: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        logger.debug(\"Snowflake not available - using fallback\")\n        return result\n\n    if not is_snowflake_configured():\n        logger.debug(\"Snowflake not configured - using fallback\")\n        return result\n\n    # Step 3: Get patient pseudonym from UPID\n    # UPID format is Provider Code (3 chars) + PersonKey\n    # We need to query Snowflake to get the PatientPseudonym for this PersonKey\n    # However, patient_has_indication_direct expects PatientPseudonym, not UPID\n    # For now, we'll use UPID as the identifier - the actual integration\n    # will need to happen at the DataFrame level where we have PersonKey\n    #\n    # NOTE: This function will be called from the pipeline where we have\n    # access to PatientPseudonym. The UPID is passed for logging/tracking.\n\n    # Actually, looking at the pipeline, we need PatientPseudonym, not UPID.\n    # The caller should pass the PatientPseudonym or we need to look it up.\n    # For now, let's assume the caller will use this in a batch context\n    # where they can map UPID -> PatientPseudonym.\n\n    # Let me reconsider: the function signature takes UPID but we need\n    # PatientPseudonym for Snowflake. In the pipeline context (fetch_and_transform_data),\n    # we'll have the PersonKey column which IS the PatientPseudonym.\n    # So UPID = ProviderCode[:3] + PersonKey, and PersonKey = PatientPseudonym.\n    #\n    # We can extract PatientPseudonym from UPID by removing the first 3 chars.\n    patient_pseudonym = upid[3:] if len(upid) > 3 else upid\n\n    # Step 4: Check patient's GP records for matching SNOMED codes\n    match_result = patient_has_indication_direct(\n        patient_pseudonym=patient_pseudonym,\n        drug_snomed_mappings=drug_snomed_mappings,\n        connector=connector,\n        before_date=before_date,\n    )\n\n    if match_result.matched and match_result.primary_directorate:\n        return DirectorateAssignment(\n            upid=upid,\n            drug_name=drug_name,\n            directorate=match_result.primary_directorate,\n            search_term=match_result.search_term,\n            source=\"DIAGNOSIS\",\n            snomed_code=match_result.snomed_code,\n            event_date=match_result.event_date,\n        )\n\n    # No match found - return fallback result\n    return result\n\n\ndef get_cluster_snomed_codes(\n    cluster_id: str,\n    connector: Optional[SnowflakeConnector] = None,\n    use_cache: bool = True,\n) -> ClusterSnomedCodes:\n    \"\"\"\n    Get all SNOMED codes for a cluster from Snowflake.\n\n    Queries the ClinicalCodingClusterSnomedCodes table to get all SNOMED codes\n    that belong to the specified cluster.\n\n    Args:\n        cluster_id: Cluster ID to look up (e.g., 'RARTH_COD', 'PSORIASIS_COD')\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        use_cache: Whether to use cached results (default True)\n\n    Returns:\n        ClusterSnomedCodes with list of SNOMED codes and descriptions\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE:\n        logger.warning(\"Snowflake connector not available\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n    if not is_snowflake_configured():\n        logger.warning(\"Snowflake not configured - cannot get cluster codes\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n    # Check cache first\n    cache_key = f\"cluster_snomed_{cluster_id}\"\n    if use_cache and is_cache_enabled():\n        cache = get_cache()\n        cached = cache.get(cache_key)\n        if cached is not None and len(cached) > 0:\n            logger.debug(f\"Using cached SNOMED codes for cluster '{cluster_id}'\")\n            cached_dict = cached[0]  # First element is our data dict\n            return ClusterSnomedCodes(\n                cluster_id=cluster_id,\n                cluster_description=str(cached_dict.get(\"description\", \"\")),\n                snomed_codes=list(cached_dict.get(\"codes\", [])),\n                snomed_descriptions=dict(cached_dict.get(\"descriptions\", {})),\n            )\n\n    if connector is None:\n        connector = get_connector()\n\n    query = '''\n        SELECT DISTINCT\n            \"Cluster_ID\",\n            \"Cluster_Description\",\n            \"SNOMEDCode\",\n            \"SNOMEDDescription\"\n        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n        WHERE \"Cluster_ID\" = %s\n        ORDER BY \"SNOMEDCode\"\n    '''\n\n    try:\n        results = connector.execute_dict(query, (cluster_id,))\n\n        if not results:\n            logger.warning(f\"No SNOMED codes found for cluster '{cluster_id}'\")\n            return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n        codes = []\n        descriptions = {}\n        description = results[0].get(\"Cluster_Description\", \"\") if results else \"\"\n\n        for row in results:\n            code = row.get(\"SNOMEDCode\")\n            if code:\n                codes.append(code)\n                descriptions[code] = row.get(\"SNOMEDDescription\", \"\")\n\n        logger.info(f\"Found {len(codes)} SNOMED codes for cluster '{cluster_id}'\")\n\n        # Cache the results (using query-based cache with fake params)\n        if use_cache and is_cache_enabled():\n            cache = get_cache()\n            cache_data = [{\n                \"description\": description,\n                \"codes\": codes,\n                \"descriptions\": descriptions,\n            }]\n            cache.set(cache_key, None, cache_data)  # type: ignore[arg-type]\n\n        return ClusterSnomedCodes(\n            cluster_id=cluster_id,\n            cluster_description=description,\n            snomed_codes=codes,\n            snomed_descriptions=descriptions,\n        )\n\n    except Exception as e:\n        logger.error(f\"Error getting SNOMED codes for cluster '{cluster_id}': {e}\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n\ndef patient_has_indication(\n    patient_pseudonym: str,\n    cluster_ids: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    before_date: Optional[date] = None,\n) -> tuple[bool, Optional[str], Optional[str], Optional[str]]:\n    \"\"\"\n    Check if a patient has any SNOMED codes from the specified clusters in GP records.\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        cluster_ids: List of cluster IDs to check against\n        connector: Optional SnowflakeConnector\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        Tuple of (has_indication, matched_cluster_id, matched_snomed_code, matched_description)\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        return False, None, None, None\n\n    if not cluster_ids:\n        return False, None, None, None\n\n    if connector is None:\n        connector = get_connector()\n\n    # Build placeholders for cluster IDs\n    placeholders = \", \".join([\"%s\"] * len(cluster_ids))\n\n    # Query to check if patient has any matching SNOMED code\n    query = f'''\n        SELECT\n            pc.\"SNOMEDCode\",\n            cc.\"Cluster_ID\",\n            cc.\"SNOMEDDescription\"\n        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\n        INNER JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" cc\n            ON pc.\"SNOMEDCode\" = cc.\"SNOMEDCode\"\n        WHERE pc.\"PatientPseudonym\" = %s\n            AND cc.\"Cluster_ID\" IN ({placeholders})\n    '''\n\n    params = [patient_pseudonym] + cluster_ids\n\n    if before_date:\n        query += ' AND pc.\"EventDateTime\" < %s'\n        params.append(before_date.isoformat())\n\n    query += ' LIMIT 1'\n\n    try:\n        results = connector.execute_dict(query, tuple(params))\n\n        if results:\n            row = results[0]\n            return (\n                True,\n                row.get(\"Cluster_ID\"),\n                row.get(\"SNOMEDCode\"),\n                row.get(\"SNOMEDDescription\"),\n            )\n\n        return False, None, None, None\n\n    except Exception as e:\n        logger.error(f\"Error checking indication for patient '{patient_pseudonym}': {e}\")\n        return False, None, None, None\n\n\ndef validate_indication(\n    patient_pseudonym: str,\n    drug_name: str,\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    before_date: Optional[date] = None,\n) -> IndicationValidationResult:\n    \"\"\"\n    Validate that a patient has an appropriate indication for a drug.\n\n    Full validation workflow:\n    1. Get drug's valid indication clusters from local mapping\n    2. Check if patient has any matching SNOMED codes in GP records\n    3. Return detailed validation result\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        drug_name: Drug name to validate indication for\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        IndicationValidationResult with validation details\n    \"\"\"\n    result = IndicationValidationResult(\n        patient_pseudonym=patient_pseudonym,\n        drug_name=drug_name,\n        has_valid_indication=False,\n    )\n\n    # Step 1: Get drug's cluster mappings\n    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n\n    if not cluster_ids:\n        result.error_message = f\"No cluster mappings found for drug '{drug_name}'\"\n        result.source = \"NONE\"\n        return result\n\n    result.checked_clusters = cluster_ids\n\n    # Step 2: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        result.error_message = \"Snowflake connector not installed\"\n        result.source = \"NONE\"\n        return result\n\n    if not is_snowflake_configured():\n        result.error_message = \"Snowflake not configured\"\n        result.source = \"NONE\"\n        return result\n\n    # Step 3: Check patient GP records\n    has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n        patient_pseudonym=patient_pseudonym,\n        cluster_ids=cluster_ids,\n        connector=connector,\n        before_date=before_date,\n    )\n\n    result.has_valid_indication = has_indication\n    result.matched_cluster_id = matched_cluster\n    result.matched_snomed_code = matched_code\n    result.matched_snomed_description = matched_desc\n    result.source = \"GP_SNOMED\" if has_indication else \"NONE\"\n\n    return result\n\n\ndef get_indication_match_rate(\n    drug_name: str,\n    patient_pseudonyms: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    sample_unmatched_count: int = 10,\n) -> DrugIndicationMatchRate:\n    \"\"\"\n    Calculate indication match rate for a drug across a list of patients.\n\n    Args:\n        drug_name: Drug name to check\n        patient_pseudonyms: List of patient pseudonymised NHS numbers\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        sample_unmatched_count: Number of unmatched patient IDs to include in sample\n\n    Returns:\n        DrugIndicationMatchRate with match statistics\n    \"\"\"\n    if connector is None and SNOWFLAKE_AVAILABLE and is_snowflake_configured():\n        connector = get_connector()\n\n    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n\n    total = len(patient_pseudonyms)\n    matched = 0\n    unmatched = 0\n    sample_unmatched: list[str] = []\n\n    if not cluster_ids:\n        logger.warning(f\"No cluster mappings for drug '{drug_name}' - all patients will be unmatched\")\n        return DrugIndicationMatchRate(\n            drug_name=drug_name,\n            total_patients=total,\n            patients_with_indication=0,\n            patients_without_indication=total,\n            match_rate=0.0,\n            clusters_checked=[],\n            sample_unmatched=patient_pseudonyms[:sample_unmatched_count],\n        )\n\n    for i, pseudonym in enumerate(patient_pseudonyms):\n        if i > 0 and i % 100 == 0:\n            logger.info(f\"Validating indications: {i}/{total} ({100*i/total:.1f}%)\")\n\n        has_indication, _, _, _ = patient_has_indication(\n            patient_pseudonym=pseudonym,\n            cluster_ids=cluster_ids,\n            connector=connector,\n        )\n\n        if has_indication:\n            matched += 1\n        else:\n            unmatched += 1\n            if len(sample_unmatched) < sample_unmatched_count:\n                sample_unmatched.append(pseudonym)\n\n    match_rate = matched / total if total > 0 else 0.0\n\n    logger.info(f\"Indication match rate for '{drug_name}': {100*match_rate:.1f}% ({matched}/{total})\")\n\n    return DrugIndicationMatchRate(\n        drug_name=drug_name,\n        total_patients=total,\n        patients_with_indication=matched,\n        patients_without_indication=unmatched,\n        match_rate=match_rate,\n        clusters_checked=cluster_ids,\n        sample_unmatched=sample_unmatched,\n    )\n\n\ndef batch_validate_indications(\n    patient_drug_pairs: list[tuple[str, str]],\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -> list[IndicationValidationResult]:\n    \"\"\"\n    Validate indications for multiple patient-drug pairs efficiently.\n\n    Args:\n        patient_drug_pairs: List of (patient_pseudonym, drug_name) tuples\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        progress_callback: Optional callback(current, total) for progress updates\n\n    Returns:\n        List of IndicationValidationResult for each pair\n    \"\"\"\n    results = []\n    total = len(patient_drug_pairs)\n\n    # Cache cluster lookups by drug\n    drug_clusters_cache = {}\n\n    for i, (pseudonym, drug_name) in enumerate(patient_drug_pairs):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # Get clusters from cache or lookup\n        drug_upper = drug_name.upper()\n        if drug_upper not in drug_clusters_cache:\n            drug_clusters_cache[drug_upper] = get_drug_cluster_ids(drug_name, db_manager)\n\n        cluster_ids = drug_clusters_cache[drug_upper]\n\n        if not cluster_ids:\n            results.append(IndicationValidationResult(\n                patient_pseudonym=pseudonym,\n                drug_name=drug_name,\n                has_valid_indication=False,\n                source=\"NONE\",\n                error_message=f\"No cluster mappings for drug '{drug_name}'\",\n            ))\n            continue\n\n        # Check patient indication\n        has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n            patient_pseudonym=pseudonym,\n            cluster_ids=cluster_ids,\n            connector=connector,\n        )\n\n        results.append(IndicationValidationResult(\n            patient_pseudonym=pseudonym,\n            drug_name=drug_name,\n            has_valid_indication=has_indication,\n            matched_cluster_id=matched_cluster,\n            matched_snomed_code=matched_code,\n            matched_snomed_description=matched_desc,\n            checked_clusters=cluster_ids,\n            source=\"GP_SNOMED\" if has_indication else \"NONE\",\n        ))\n\n    matched_count = sum(1 for r in results if r.has_valid_indication)\n    logger.info(f\"Batch validation complete: {matched_count}/{total} ({100*matched_count/total:.1f}%) with valid indications\")\n\n    return results\n\n\ndef get_available_clusters(\n    connector: Optional[SnowflakeConnector] = None,\n) -> list[dict]:\n    \"\"\"\n    Get list of all available SNOMED clusters from Snowflake.\n\n    Returns:\n        List of dicts with cluster_id, cluster_description, code_count\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        logger.warning(\"Snowflake not available - cannot list clusters\")\n        return []\n\n    if connector is None:\n        connector = get_connector()\n\n    query = '''\n        SELECT\n            \"Cluster_ID\",\n            \"Cluster_Description\",\n            COUNT(DISTINCT \"SNOMEDCode\") as code_count\n        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n        GROUP BY \"Cluster_ID\", \"Cluster_Description\"\n        ORDER BY \"Cluster_ID\"\n    '''\n\n    try:\n        results = connector.execute_dict(query)\n\n        clusters = []\n        for row in results:\n            clusters.append({\n                \"cluster_id\": row.get(\"Cluster_ID\"),\n                \"cluster_description\": row.get(\"Cluster_Description\"),\n                \"code_count\": row.get(\"code_count\", 0),\n            })\n\n        logger.info(f\"Found {len(clusters)} available SNOMED clusters\")\n        return clusters\n\n    except Exception as e:\n        logger.error(f\"Error getting available clusters: {e}\")\n        return []\n\n\ndef batch_lookup_indication_groups(\n    df: \"pd.DataFrame\",\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    batch_size: int = 500,\n) -> \"pd.DataFrame\":\n    \"\"\"\n    Batch lookup GP diagnosis-based indication groups for a DataFrame of patients.\n\n    This is the efficient batch version of get_directorate_from_diagnosis().\n    Instead of querying Snowflake per patient, it batches the lookups for performance.\n\n    Strategy:\n    1. Get all unique (PersonKey, Drug Name) pairs from DataFrame\n    2. For each unique drug, get all SNOMED codes from local SQLite\n    3. Build batched Snowflake queries to check GP records\n    4. Return indication_df mapping UPID ÔåÆ Indication_Group\n\n    For unmatched patients, Indication_Group will be their Directory (with suffix).\n\n    Args:\n        df: DataFrame with columns: UPID, Drug Name, Directory, PersonKey\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n        batch_size: Number of patients per Snowflake query batch\n\n    Returns:\n        DataFrame with columns: UPID, Indication_Group, Source\n        - Indication_Group: Search_Term (if matched) or \"Directory (no GP dx)\" (if not)\n        - Source: \"DIAGNOSIS\" or \"FALLBACK\"\n    \"\"\"\n    import pandas as pd\n\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    logger.info(f\"Starting batch indication lookup for {len(df)} records...\")\n\n    # Step 1: Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combinations\n    # We need PseudoNHSNoLinked to query Snowflake - this matches PatientPseudonym in GP records\n    # Note: PersonKey is LocalPatientID which is provider-specific and does NOT match GP records\n    if 'PseudoNHSNoLinked' not in df.columns:\n        logger.error(\"DataFrame missing 'PseudoNHSNoLinked' column - cannot lookup GP records\")\n        # Return fallback for all patients\n        result_df = df[['UPID', 'Directory']].drop_duplicates().copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']]\n\n    # Get unique patient-drug combinations (we need one lookup per patient-drug pair)\n    unique_pairs = df[['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']].drop_duplicates()\n    logger.info(f\"Found {len(unique_pairs)} unique patient-drug combinations\")\n\n    # Step 2: Get all unique drugs and their SNOMED codes\n    unique_drugs = unique_pairs['Drug Name'].unique()\n    logger.info(f\"Building SNOMED lookup for {len(unique_drugs)} unique drugs...\")\n\n    # Build drug -> list of DrugSnomedMapping dict\n    drug_snomed_map: dict[str, list[DrugSnomedMapping]] = {}\n    all_snomed_codes: set[str] = set()\n    snomed_to_drug_searchterm: dict[str, list[tuple[str, str, str]]] = {}  # snomed -> [(drug, search_term, primary_dir), ...]\n\n    for drug_name in unique_drugs:\n        mappings = get_drug_snomed_codes(drug_name, db_manager)\n        drug_snomed_map[drug_name] = mappings\n\n        for m in mappings:\n            all_snomed_codes.add(m.snomed_code)\n            if m.snomed_code not in snomed_to_drug_searchterm:\n                snomed_to_drug_searchterm[m.snomed_code] = []\n            snomed_to_drug_searchterm[m.snomed_code].append(\n                (drug_name, m.search_term, m.primary_directorate)\n            )\n\n    logger.info(f\"Total SNOMED codes to check: {len(all_snomed_codes)}\")\n\n    # Step 3: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        logger.warning(\"Snowflake not available - returning fallback for all patients\")\n        result_df = unique_pairs[['UPID', 'Directory']].copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n\n    if connector is None:\n        connector = get_connector()\n\n    # Step 4: Query GP records for all patients in batches\n    # The query finds the most recent matching SNOMED code for each patient\n\n    # Get unique PseudoNHSNoLinked values (each = one patient in GP records)\n    unique_patients = unique_pairs[['PseudoNHSNoLinked', 'UPID', 'Directory']].drop_duplicates(subset=['PseudoNHSNoLinked'])\n    patient_pseudonyms = unique_patients['PseudoNHSNoLinked'].tolist()\n\n    logger.info(f\"Querying GP records for {len(patient_pseudonyms)} unique patients in batches of {batch_size}...\")\n\n    # Results dict: PersonKey -> (snomed_code, event_date)\n    gp_matches: dict[str, tuple[str, Any]] = {}\n\n    # Convert SNOMED codes to list for query\n    snomed_list = list(all_snomed_codes)\n\n    if not snomed_list:\n        logger.warning(\"No SNOMED codes to check - returning fallback for all patients\")\n        result_df = unique_pairs[['UPID', 'Directory']].copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n\n    # Build SNOMED IN clause (reused across batches)\n    snomed_placeholders = \", \".join([\"%s\"] * len(snomed_list))\n\n    # Process patients in batches\n    for batch_start in range(0, len(patient_pseudonyms), batch_size):\n        batch_end = min(batch_start + batch_size, len(patient_pseudonyms))\n        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n\n        logger.info(f\"Batch {batch_start//batch_size + 1}: patients {batch_start} to {batch_end}\")\n\n        # Build patient IN clause\n        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n\n        # Query to find all matching SNOMED codes for these patients\n        # We'll get all matches and pick the most recent per patient in Python\n        query = f'''\n            SELECT\n                \"PatientPseudonym\",\n                \"SNOMEDCode\",\n                \"EventDateTime\"\n            FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n            WHERE \"PatientPseudonym\" IN ({patient_placeholders})\n              AND \"SNOMEDCode\" IN ({snomed_placeholders})\n            ORDER BY \"PatientPseudonym\", \"EventDateTime\" DESC\n        '''\n\n        params = tuple(batch_pseudonyms) + tuple(snomed_list)\n\n        try:\n            results = connector.execute_dict(query, params)\n\n            # Process results - pick most recent per patient\n            for row in results:\n                person_key = row.get(\"PatientPseudonym\")\n                snomed_code = row.get(\"SNOMEDCode\")\n                event_date = row.get(\"EventDateTime\")\n\n                if person_key and snomed_code:\n                    # Keep only if we haven't seen this patient yet (first = most recent due to ORDER BY)\n                    if person_key not in gp_matches:\n                        gp_matches[person_key] = (snomed_code, event_date)\n\n        except Exception as e:\n            logger.error(f\"Error querying GP records for batch: {e}\")\n            # Continue with other batches\n\n    logger.info(f\"Found GP matches for {len(gp_matches)} patients\")\n\n    # Step 5: Build result DataFrame\n    # For each unique_pair, determine Indication_Group based on match status\n    results_list = []\n\n    # We need to dedupe by UPID - a patient might be on multiple drugs\n    # Strategy: For each UPID, use the most recent match (if any)\n    upid_to_match: dict[str, tuple[str, str]] = {}  # UPID -> (Indication_Group, Source)\n\n    for _, row in unique_pairs.iterrows():\n        upid = row['UPID']\n        drug_name = row['Drug Name']\n        patient_pseudonym = row['PseudoNHSNoLinked']\n        directory = row['Directory']\n\n        # Check if patient has GP match (using PseudoNHSNoLinked which matches PatientPseudonym in GP)\n        if patient_pseudonym in gp_matches:\n            matched_snomed, event_date = gp_matches[patient_pseudonym]\n\n            # Find the search_term for this SNOMED code and drug\n            # (A SNOMED code might map to multiple drugs with different search_terms)\n            if matched_snomed in snomed_to_drug_searchterm:\n                # Look for match with current drug first\n                search_term = None\n                for drug, st, pd in snomed_to_drug_searchterm[matched_snomed]:\n                    if drug.upper() == drug_name.upper():\n                        search_term = st\n                        break\n                # If no drug-specific match, use any match\n                if search_term is None:\n                    search_term = snomed_to_drug_searchterm[matched_snomed][0][1]\n\n                # Only update if we don't have a match for this UPID yet\n                if upid not in upid_to_match:\n                    upid_to_match[upid] = (search_term, \"DIAGNOSIS\")\n            else:\n                # Shouldn't happen but fallback just in case\n                if upid not in upid_to_match:\n                    upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n        else:\n            # No GP match - use fallback\n            if upid not in upid_to_match:\n                upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n\n    # Build result DataFrame\n    for upid, (indication_group, source) in upid_to_match.items():\n        results_list.append({\n            'UPID': upid,\n            'Indication_Group': indication_group,\n            'Source': source,\n        })\n\n    result_df = pd.DataFrame(results_list)\n\n    # Log statistics\n    diagnosis_count = len([s for s in result_df['Source'] if s == \"DIAGNOSIS\"])\n    fallback_count = len([s for s in result_df['Source'] if s == \"FALLBACK\"])\n    total = len(result_df)\n\n    logger.info(f\"Indication lookup complete:\")\n    logger.info(f\"  Total unique patients: {total}\")\n    logger.info(f\"  DIAGNOSIS matches: {diagnosis_count} ({100*diagnosis_count/total:.1f}%)\")\n    logger.info(f\"  FALLBACK (no GP match): {fallback_count} ({100*fallback_count/total:.1f}%)\")\n\n    return result_df\n\n\n# === Drug-to-indication mapping from DimSearchTerm.csv ===\n\n\n# Merge related Search_Terms into canonical names.\n# Asthma variants are clinically the same condition at different severity levels.\n# Urticaria is a separate condition ÔÇö do NOT merge with asthma.\nSEARCH_TERM_MERGE_MAP: dict[str, str] = {\n    \"allergic asthma\": \"asthma\",\n    \"severe persistent allergic asthma\": \"asthma\",\n}\n\n\ndef load_drug_indication_mapping(\n    csv_path: Optional[str] = None,\n) -> tuple[dict[str, list[str]], dict[str, list[str]]]:\n    \"\"\"\n    Load the drug-to-Search_Term mapping from DimSearchTerm.csv.\n\n    Builds two lookup dicts:\n    - fragment_to_search_terms: drug fragment (UPPERCASE) -> list of Search_Terms containing it\n    - search_term_to_fragments: search_term -> list of drug fragments (UPPERCASE)\n\n    DimSearchTerm.csv has columns: Search_Term, CleanedDrugName, PrimaryDirectorate\n    CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\").\n\n    Note: A Search_Term can appear multiple times with different PrimaryDirectorates\n    (e.g., \"diabetes\" appears under both DIABETIC MEDICINE and OPHTHALMOLOGY).\n    Drug fragments from all rows for the same Search_Term are combined.\n\n    Asthma-related Search_Terms (\"allergic asthma\", \"severe persistent allergic asthma\")\n    are merged into \"asthma\" to match the CLUSTER_MAPPING_SQL normalization.\n    \"urticaria\" stays separate.\n\n    Args:\n        csv_path: Path to DimSearchTerm.csv. Defaults to data/DimSearchTerm.csv.\n\n    Returns:\n        Tuple of (fragment_to_search_terms, search_term_to_fragments)\n    \"\"\"\n    if csv_path is None:\n        csv_path = str(Path(__file__).parent.parent / \"data\" / \"DimSearchTerm.csv\")\n\n    fragment_to_search_terms: dict[str, list[str]] = {}\n    search_term_to_fragments: dict[str, list[str]] = {}\n\n    try:\n        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                search_term = row.get(\"Search_Term\", \"\").strip()\n                drug_names_raw = row.get(\"CleanedDrugName\", \"\").strip()\n\n                # Normalize asthma variants to canonical \"asthma\"\n                search_term = SEARCH_TERM_MERGE_MAP.get(search_term, search_term)\n\n                if not search_term or not drug_names_raw:\n                    continue\n\n                fragments = [frag.strip().upper() for frag in drug_names_raw.split(\"|\") if frag.strip()]\n\n                # Build search_term -> fragments (accumulate for duplicate Search_Terms)\n                if search_term not in search_term_to_fragments:\n                    search_term_to_fragments[search_term] = []\n                for frag in fragments:\n                    if frag not in search_term_to_fragments[search_term]:\n                        search_term_to_fragments[search_term].append(frag)\n\n                # Build fragment -> search_terms\n                for frag in fragments:\n                    if frag not in fragment_to_search_terms:\n                        fragment_to_search_terms[frag] = []\n                    if search_term not in fragment_to_search_terms[frag]:\n                        fragment_to_search_terms[frag].append(search_term)\n\n        logger.info(\n            f\"Loaded drug-indication mapping: {len(search_term_to_fragments)} Search_Terms, \"\n            f\"{len(fragment_to_search_terms)} drug fragments\"\n        )\n\n    except FileNotFoundError:\n        logger.error(f\"DimSearchTerm.csv not found at {csv_path}\")\n    except Exception as e:\n        logger.error(f\"Error loading DimSearchTerm.csv: {e}\")\n\n    return fragment_to_search_terms, search_term_to_fragments\n\n\ndef get_search_terms_for_drug(\n    drug_name: str,\n    search_term_to_fragments: dict[str, list[str]],\n) -> list[str]:\n    \"\"\"\n    Get all Search_Terms that list a given drug using substring matching.\n\n    Checks if any drug fragment from DimSearchTerm is a SUBSTRING of the given\n    drug name (case-insensitive). This handles both exact matches (ADALIMUMAB)\n    and partial fragments (PEGYLATED, INHALED).\n\n    Args:\n        drug_name: HCD drug name (e.g., \"ADALIMUMAB 40MG\", \"PEGYLATED LIPOSOMAL DOXORUBICIN\")\n        search_term_to_fragments: Mapping of search_term -> list of drug fragments\n\n    Returns:\n        List of Search_Terms whose drug fragments match the drug name\n    \"\"\"\n    drug_name_upper = drug_name.upper()\n    matched_terms: list[str] = []\n\n    for search_term, fragments in search_term_to_fragments.items():\n        for frag in fragments:\n            if frag in drug_name_upper:\n                matched_terms.append(search_term)\n                break  # One matching fragment is enough for this Search_Term\n\n    return matched_terms\n\n\n# === NEW APPROACH: Query Snowflake directly using cluster CTE ===\n\n# The cluster query mapping (embedded from snomed_indication_mapping_query.sql)\n# This maps Search_Term -> Cluster_ID for ~148 clinical conditions\nCLUSTER_MAPPING_SQL = \"\"\"\nWITH SearchTermClusters AS (\n    SELECT Search_Term, Cluster_ID FROM (VALUES\n        ('acute lymphoblastic leukaemia', 'HAEMCANMORPH_COD'),\n        ('acute myeloid leukaemia', 'C19HAEMCAN_COD'),\n        ('acute promyelocytic leukaemia', 'HAEMCANMORPH_COD'),\n        ('asthma', 'AST_COD'),\n        ('allergic rhinitis', 'MILDINTAST_COD'),\n        ('alzheimer''s disease', 'DEMALZ_COD'),\n        ('amyloidosis', 'AMYLOID_COD'),\n        ('anaemia', 'eFI2_AnaemiaTimeSensitive'),\n        ('anaplastic large cell lymphoma', 'C19HAEMCAN_COD'),\n        ('apixaban', 'DOACCON_COD'),\n        ('aplastic anaemia', 'eFI2_AnaemiaEver'),\n        ('arthritis', 'eFI2_InflammatoryArthritis'),\n        ('asthma', 'eFI2_Asthma'),\n        ('atopic dermatitis', 'ATOPDERM_COD'),\n        ('atrial fibrillation', 'eFI2_AtrialFibrillation'),\n        ('attention deficit hyperactivity disorder', 'ADHD_COD'),\n        ('bipolar disorder', 'MH_COD'),\n        ('bladder', 'eFI2_UrinaryIncontinence'),\n        ('breast cancer', 'BRCANSCR_COD'),\n        ('cardiomyopathy', 'eFI2_HarmfulDrinking'),\n        ('cardiovascular disease', 'CVDRISKASS_COD'),\n        ('cervical cancer', 'CSDEC_COD'),\n        ('cholangiocarcinoma', 'eFI2_Cancer'),\n        ('chronic kidney disease', 'CKD_COD'),\n        ('chronic liver disease', 'eFI2_LiverProblems'),\n        ('chronic lymphocytic leukaemia', 'EPPHAEMCAN_COD'),\n        ('chronic myeloid leukaemia', 'EPPHAEMCAN_COD'),\n        ('chronic obstructive pulmonary disease', 'eFI2_COPD'),\n        ('colon cancer', 'eFI2_Cancer'),\n        ('colorectal cancer', 'GICANREF_COD'),\n        ('constipation', 'CHRONCONSTIP_COD'),\n        ('covid-19', 'POSSPOSTCOVID_COD'),\n        ('crohn''s disease', 'eFI2_InflammatoryBowelDisease'),\n        ('cutaneous t-cell lymphoma', 'C19HAEMCAN_COD'),\n        ('cystic fibrosis', 'CUST_ICB_CYSTIC_FIBROSIS'),\n        ('deep vein thrombosis', 'VTE_COD'),\n        ('depression', 'eFI2_Depression'),\n        ('diabetes', 'eFI2_DiabetesEver'),\n        ('diabetic retinopathy', 'DRSELIGIBILITY_COD'),\n        ('diffuse large b-cell lymphoma', 'C19HAEMCAN_COD'),\n        ('dravet syndrome', 'EPIL_COD'),\n        ('drug misuse', 'ILLSUBINT_COD'),\n        ('dyspepsia', 'eFI2_AbdominalPain'),\n        ('epilepsy', 'eFI2_Seizures'),\n        ('fallopian tube', 'STERIL_COD'),\n        ('follicular lymphoma', 'C19HAEMCAN_COD'),\n        ('gastric cancer', 'eFI2_Cancer'),\n        ('giant cell arteritis', 'GCA_COD'),\n        ('glioma', 'NHAEMCANMORPH_COD'),\n        ('gout', 'eFI2_InflammatoryArthritis'),\n        ('graft versus host disease', 'GVHD_COD'),\n        ('granulomatosis with polyangiitis', 'WEGENERVASC_COD'),\n        ('growth hormone deficiency', 'HYPOPITUITARY_COD'),\n        ('hand eczema', 'ECZEMA_COD'),\n        ('heart failure', 'eFI2_HeartFailure'),\n        ('hepatitis b', 'HEPBCVAC_COD'),\n        ('hepatocellular carcinoma', 'eFI2_Cancer'),\n        ('hiv', 'PREFLANG_COD'),\n        ('hodgkin lymphoma', 'HAEMCANMORPH_COD'),\n        ('hormone receptor', 'eFI2_ThyroidProblems'),\n        ('hypercholesterolaemia', 'CLASSFH_COD'),\n        ('immune thrombocytopenia', 'ITP_COD'),\n        ('influenza', 'FLUINVITE_COD'),\n        ('insomnia', 'eFI2_SleepProblems'),\n        ('irritable bowel syndrome', 'IBS_COD'),\n        ('ischaemic stroke', 'OSTR_COD'),\n        ('juvenile idiopathic arthritis', 'RARTHAD_COD'),\n        ('kidney transplant', 'RENALTRANSP_COD'),\n        ('leukaemia', 'eFI2_Cancer'),\n        ('lung cancer', 'FTCANREF_COD'),\n        ('lymphoma', 'C19HAEMCAN_COD'),\n        ('macular degeneration', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('macular oedema', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('major depressive episodes', 'eFI2_Depression'),\n        ('malignant melanoma', 'eFI2_Cancer'),\n        ('malignant pleural mesothelioma', 'LUNGCAN_COD'),\n        ('manic episode', 'MH_COD'),\n        ('mantle cell lymphoma', 'HAEMCANMORPH_COD'),\n        ('melanoma', 'eFI2_Cancer'),\n        ('merkel cell carcinoma', 'C19CAN_COD'),\n        ('migraine', 'eFI2_Headache'),\n        ('motor neurone disease', 'MND_COD'),\n        ('multiple myeloma', 'C19HAEMCAN_COD'),\n        ('multiple sclerosis', 'MS_COD'),\n        ('myelodysplastic', 'eFI2_AnaemiaEver'),\n        ('myelofibrosis', 'MDS_COD'),\n        ('myocardial infarction', 'eFI2_IschaemicHeartDisease'),\n        ('myotonia', 'CNDATRISK2_COD'),\n        ('narcolepsy', 'LD_COD'),\n        ('neuroendocrine tumour', 'LUNGCAN_COD'),\n        ('non-small cell lung cancer', 'LUNGCAN_COD'),\n        ('non-small-cell lung cancer', 'FTCANREF_COD'),\n        ('obesity', 'BMI30_COD'),\n        ('osteoarthritis', 'CUST_ICB_OSTEOARTHRITIS'),\n        ('osteoporosis', 'eFI2_Osteoporosis'),\n        ('osteosarcoma', 'NHAEMCANMORPH_COD'),\n        ('ovarian cancer', 'C19CAN_COD'),\n        ('peripheral arterial disease', 'PADEXC_COD'),\n        ('plaque psoriasis', 'PSORIASIS_COD'),\n        ('polycystic kidney disease', 'EPPCONGMALF_COD'),\n        ('polycythaemia vera', 'C19HAEMCAN_COD'),\n        ('pregnancy', 'C19PREG_COD'),\n        ('primary biliary cholangitis', 'eFI2_LiverProblems'),\n        ('primary hypercholesterolaemia', 'FNFHYP_COD'),\n        ('prostate cancer', 'EPPSOLIDCAN_COD'),\n        ('psoriasis', 'PSORIASIS_COD'),\n        ('psoriatic arthritis', 'RARTHAD_COD'),\n        ('pulmonary embolism', 'eFI2_RespiratoryDiseaseTimeSensitive'),\n        ('pulmonary fibrosis', 'ILD_COD'),\n        ('relapsing multiple sclerosis', 'MS_COD'),\n        ('renal cell carcinoma', 'C19CAN_COD'),\n        ('renal transplantation', 'RENALTRANSP_COD'),\n        ('retinal vein occlusion', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('rheumatoid arthritis', 'eFI2_InflammatoryArthritis'),\n        ('rivaroxaban', 'DOACCON_COD'),\n        ('schizophrenia', 'MH_COD'),\n        ('seizures', 'LSZFREQ_COD'),\n        ('sepsis', 'C19ACTIVITY_COD'),\n        ('asthma', 'SEVAST_COD'),\n        ('sickle cell disease', 'SICKLE_COD'),\n        ('sleep apnoea', 'CUST_ICB_NON_SEVERE_LDA'),\n        ('smoking cessation', 'SMOKINGINT_COD'),\n        ('soft tissue sarcoma', 'NHAEMCANMORPH_COD'),\n        ('spinal muscular atrophy', 'MND_COD'),\n        ('squamous cell', 'C19CAN_COD'),\n        ('squamous cell carcinoma', 'C19CAN_COD'),\n        ('stem cell transplant', 'ALLOTRANSP_COD'),\n        ('stroke', 'eFI2_Stroke'),\n        ('systemic lupus erythematosus', 'SLUPUS_COD'),\n        ('systemic mastocytosis', 'HAEMCANMORPH_COD'),\n        ('thrombocytopenic purpura', 'TTP_COD'),\n        ('thrombotic thrombocytopenic purpura', 'TTP_COD'),\n        ('thyroid cancer', 'C19CAN_COD'),\n        ('tophaceous gout', 'CUST_ICB_OSTEOARTHRITIS'),\n        ('transitional cell carcinoma', 'C19CAN_COD'),\n        ('type 1 diabetes', 'DMTYPE1_COD'),\n        ('type 2 diabetes', 'DMTYPE2_COD'),\n        ('ulcerative colitis', 'eFI2_InflammatoryBowelDisease'),\n        ('urothelial carcinoma', 'NHAEMCANMORPH_COD'),\n        ('urticaria', 'XSAL_COD'),\n        ('uveitis', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('vascular disease', 'CVDINVITE_COD'),\n        ('vasculitis', 'CRYOGLOBVASC_COD')\n    ) AS t(Search_Term, Cluster_ID)\n),\n\nClusterCodes AS (\n    SELECT\n        stc.Search_Term,\n        c.\"SNOMEDCode\",\n        c.\"SNOMEDDescription\"\n    FROM SearchTermClusters stc\n    JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" c\n        ON stc.Cluster_ID = c.\"Cluster_ID\"\n    WHERE c.\"SNOMEDCode\" IS NOT NULL\n),\n\nExplicitCodes AS (\n    SELECT Search_Term, SNOMEDCode, SNOMEDDescription FROM (VALUES\n        ('acute coronary syndrome', '837091000000100', 'Manual mapping'),\n        ('ankylosing spondylitis', '162930007', 'Manual mapping'),\n        ('ankylosing spondylitis', '239805001', 'Manual mapping'),\n        ('ankylosing spondylitis', '239810002', 'Manual mapping'),\n        ('ankylosing spondylitis', '239811003', 'Manual mapping'),\n        ('ankylosing spondylitis', '394990003', 'Manual mapping'),\n        ('ankylosing spondylitis', '429712009', 'Manual mapping'),\n        ('ankylosing spondylitis', '441562009', 'Manual mapping'),\n        ('ankylosing spondylitis', '441680005', 'Manual mapping'),\n        ('ankylosing spondylitis', '441930001', 'Manual mapping'),\n        ('axial spondyloarthritis', '723116002', 'Manual mapping'),\n        ('choroidal neovascularisation', '380621000000102', 'Manual mapping'),\n        ('choroidal neovascularisation', '733124000', 'Manual mapping')\n    ) AS t(Search_Term, SNOMEDCode, SNOMEDDescription)\n),\n\nAllIndicationCodes AS (\n    SELECT Search_Term, \"SNOMEDCode\" AS SNOMEDCode, \"SNOMEDDescription\" AS SNOMEDDescription\n    FROM ClusterCodes\n    UNION ALL\n    SELECT Search_Term, SNOMEDCode, SNOMEDDescription\n    FROM ExplicitCodes\n)\n\"\"\"\n\n\ndef get_patient_indication_groups(\n    patient_pseudonyms: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    batch_size: int = 500,\n    earliest_hcd_date: Optional[str] = None,\n) -> \"pd.DataFrame\":\n    \"\"\"\n    Batch lookup GP diagnosis-based indication groups using Snowflake cluster query.\n\n    Returns ALL matching Search_Terms per patient with code_frequency (count of\n    matching SNOMED codes). This enables drug-aware indication matching where\n    each drug is cross-referenced against the patient's GP diagnoses.\n\n    The query:\n    1. Uses the cluster mapping CTE to get all Search_Term -> SNOMED code mappings\n    2. Joins with PrimaryCareClinicalCoding to find patients with matching codes\n    3. Groups by patient + Search_Term and counts matching codes (code_frequency)\n    4. Optionally restricts to GP codes from earliest_hcd_date onwards\n\n    Args:\n        patient_pseudonyms: List of PseudoNHSNoLinked values (matches PatientPseudonym in GP records)\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        batch_size: Number of patients per Snowflake query batch (default 500)\n        earliest_hcd_date: Optional ISO date string (YYYY-MM-DD). If provided, only\n            counts GP codes from this date onwards. Should be MIN(Intervention Date)\n            from the HCD DataFrame to restrict to the HCD data window.\n\n    Returns:\n        DataFrame with columns:\n        - PatientPseudonym: The patient identifier (PseudoNHSNoLinked value)\n        - Search_Term: The matched indication (e.g., \"rheumatoid arthritis\")\n        - code_frequency: Count of matching SNOMED codes for this Search_Term\n\n        Multiple rows per patient (one per matched Search_Term).\n        Patients not found in results have no matching GP diagnosis.\n    \"\"\"\n    import pandas as pd\n\n    logger.info(f\"Starting Snowflake-direct indication lookup for {len(patient_pseudonyms)} patients...\")\n    if earliest_hcd_date:\n        logger.info(f\"  Restricting GP codes to >= {earliest_hcd_date}\")\n\n    # Handle edge case: empty patient list\n    if not patient_pseudonyms:\n        logger.warning(\"Empty patient list provided\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    # Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        logger.error(\"Snowflake connector not available - cannot lookup GP records\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    if not is_snowflake_configured():\n        logger.error(\"Snowflake not configured - cannot lookup GP records\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    if connector is None:\n        connector = get_connector()\n\n    # Results list to collect all matches\n    all_results: list[dict] = []\n\n    # Process patients in batches\n    total_patients = len(patient_pseudonyms)\n    for batch_start in range(0, total_patients, batch_size):\n        batch_end = min(batch_start + batch_size, total_patients)\n        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n        batch_num = batch_start // batch_size + 1\n        total_batches = (total_patients + batch_size - 1) // batch_size\n\n        logger.info(f\"Batch {batch_num}/{total_batches}: patients {batch_start + 1} to {batch_end}\")\n\n        # Build patient IN clause placeholders\n        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n\n        # Build WHERE clause with optional date filter\n        date_filter = \"\"\n        if earliest_hcd_date:\n            date_filter = f\"\\n    AND pc.\\\"EventDateTime\\\" >= %s\"\n\n        # Build the full query with cluster CTE\n        # Returns ALL matching Search_Terms per patient with code_frequency\n        # code_frequency = COUNT of matching SNOMED codes per Search_Term per patient\n        query = f\"\"\"\n{CLUSTER_MAPPING_SQL}\nSELECT\n    pc.\"PatientPseudonym\" AS \"PatientPseudonym\",\n    aic.Search_Term AS \"Search_Term\",\n    COUNT(*) AS \"code_frequency\"\nFROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\nINNER JOIN AllIndicationCodes aic\n    ON pc.\"SNOMEDCode\" = aic.SNOMEDCode\nWHERE pc.\"PatientPseudonym\" IN ({patient_placeholders}){date_filter}\nGROUP BY pc.\"PatientPseudonym\", aic.Search_Term\n\"\"\"\n\n        # Build params: patient pseudonyms + optional date\n        params = list(batch_pseudonyms)\n        if earliest_hcd_date:\n            params.append(earliest_hcd_date)\n\n        try:\n            results = connector.execute_dict(query, tuple(params))\n\n            for row in results:\n                all_results.append({\n                    'PatientPseudonym': row.get('PatientPseudonym'),\n                    'Search_Term': row.get('Search_Term'),\n                    'code_frequency': row.get('code_frequency', 0),\n                })\n\n            logger.debug(f\"Batch {batch_num}: found {len(results)} patient-indication matches\")\n\n        except Exception as e:\n            logger.error(f\"Error querying GP records for batch {batch_num}: {e}\")\n            # Continue with other batches - partial results are better than none\n\n    # Build result DataFrame\n    result_df = pd.DataFrame(all_results)\n\n    # Log summary statistics\n    if len(result_df) > 0:\n        unique_patients = result_df['PatientPseudonym'].nunique()\n        total_rows = len(result_df)\n        match_rate = 100 * unique_patients / total_patients\n        unique_terms = result_df['Search_Term'].nunique()\n        avg_indications = total_rows / unique_patients if unique_patients > 0 else 0\n        logger.info(f\"Indication lookup complete:\")\n        logger.info(f\"  Total patients queried: {total_patients}\")\n        logger.info(f\"  Patients with GP match: {unique_patients} ({match_rate:.1f}%)\")\n        logger.info(f\"  Total patient-indication rows: {total_rows} (avg {avg_indications:.1f} per patient)\")\n        logger.info(f\"  Unique Search_Terms found: {unique_terms}\")\n\n        # Log top Search_Terms\n        top_terms = result_df['Search_Term'].value_counts().head(5)\n        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n    else:\n        logger.info(f\"Indication lookup complete: 0 matches from {total_patients} patients\")\n\n    return result_df\n\n\n# Export public API\n__all__ = [\n    # Dataclasses\n    \"ClusterSnomedCodes\",\n    \"IndicationValidationResult\",\n    \"DrugIndicationMatchRate\",\n    \"DrugSnomedMapping\",\n    \"DirectSnomedMatchResult\",\n    \"DirectorateAssignment\",\n    # Cluster-based lookup functions (existing)\n    \"get_drug_clusters\",\n    \"get_drug_cluster_ids\",\n    \"get_cluster_snomed_codes\",\n    \"patient_has_indication\",\n    \"validate_indication\",\n    \"get_indication_match_rate\",\n    \"batch_validate_indications\",\n    \"get_available_clusters\",\n    # Direct SNOMED lookup functions (new)\n    \"get_drug_snomed_codes\",\n    \"patient_has_indication_direct\",\n    # Diagnosis-based directorate assignment\n    \"get_directorate_from_diagnosis\",\n    # Batch lookup for indication groups\n    \"batch_lookup_indication_groups\",\n    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Snowflake-direct indication lookup (new approach)\n    \"get_patient_indication_groups\",\n    \"CLUSTER_MAPPING_SQL\",\n]\n","structuredPatch":[{"oldStart":1204,"oldLines":6,"newStart":1204,"newLines":175,"lines":["     return matched_terms"," "," ","+def assign_drug_indications(","+    df: \"pd.DataFrame\",","+    gp_matches_df: \"pd.DataFrame\",","+    search_term_to_fragments: dict[str, list[str]],","+) -> \"tuple[pd.DataFrame, pd.DataFrame]\":","+    \"\"\"","+    Assign drug-aware indications by cross-referencing GP diagnoses with drug mappings.","+","+    For each UPID + Drug Name pair in the HCD data:","+    1. Look up patient's GP-matched Search_Terms (via PseudoNHSNoLinked ÔåÆ gp_matches_df)","+    2. Look up which Search_Terms list this drug (via search_term_to_fragments)","+    3. Intersect = valid indications for this drug-patient pair","+    4. If 1 match: use it","+    5. If multiple: pick highest code_frequency (most GP coding = most likely indication)","+    6. If tied frequency: alphabetical Search_Term for determinism","+    7. If 0 matches: fallback to \"{Directory} (no GP dx)\"","+","+    Modifies UPID to include indication: \"{UPID}|{search_term}\" or \"{UPID}|{Directory} (no GP dx)\".","+    This makes drugs under different indications appear as separate pathways.","+","+    Args:","+        df: HCD DataFrame with columns: UPID, Drug Name, PseudoNHSNoLinked, Directory","+        gp_matches_df: GP matches from get_patient_indication_groups() with columns:","+            PatientPseudonym, Search_Term, code_frequency","+            (multiple rows per patient, one per matched Search_Term)","+        search_term_to_fragments: From load_drug_indication_mapping()[1].","+            Maps search_term -> list of drug fragments (UPPERCASE).","+","+    Returns:","+        Tuple of (modified_df, indication_df):","+        - modified_df: Copy of df with UPID replaced by \"{UPID}|{indication}\"","+        - indication_df: DataFrame indexed by modified UPID with 'Directory' column","+            containing the Search_Term (or fallback label) for pathway hierarchy","+    \"\"\"","+    import pandas as pd","+","+    modified_df = df.copy()","+","+    # Build GP match lookup: PseudoNHSNoLinked -> {Search_Term: code_frequency}","+    gp_lookup: dict[str, dict[str, int]] = {}","+    if not gp_matches_df.empty:","+        for _, row in gp_matches_df.iterrows():","+            pseudo = row['PatientPseudonym']","+            term = row['Search_Term']","+            freq = int(row.get('code_frequency', 0))","+            if pseudo not in gp_lookup:","+                gp_lookup[pseudo] = {}","+            gp_lookup[pseudo][term] = freq","+","+    logger.info(f\"GP lookup built: {len(gp_lookup)} patients with GP matches\")","+","+    # Cache drug -> Search_Terms lookups to avoid recomputing per row","+    drug_search_terms_cache: dict[str, list[str]] = {}","+","+    def get_drug_terms(drug_name: str) -> list[str]:","+        if drug_name not in drug_search_terms_cache:","+            drug_search_terms_cache[drug_name] = get_search_terms_for_drug(","+                drug_name, search_term_to_fragments","+            )","+        return drug_search_terms_cache[drug_name]","+","+    # Process each row: determine indication for each UPID + Drug Name","+    # We work at the (UPID, Drug Name) pair level, then apply to all rows","+    # Key: (UPID, Drug Name) -> (matched_search_term_or_fallback, is_diagnosis)","+    pair_indication_cache: dict[tuple[str, str], tuple[str, bool]] = {}","+","+    # Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combos","+    required_cols = ['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']","+    unique_pairs = modified_df[required_cols].drop_duplicates(","+        subset=['UPID', 'Drug Name']","+    )","+","+    match_count = 0","+    fallback_count = 0","+    tiebreak_count = 0","+","+    for _, pair_row in unique_pairs.iterrows():","+        upid = pair_row['UPID']","+        drug_name = pair_row['Drug Name']","+        pseudo = pair_row['PseudoNHSNoLinked']","+        directory = pair_row['Directory']","+","+        # Get Search_Terms this drug maps to","+        drug_terms = get_drug_terms(drug_name)","+","+        # Get patient's GP-matched Search_Terms","+        patient_gp_terms = gp_lookup.get(pseudo, {}) if pd.notna(pseudo) else {}","+","+        # Intersect: Search_Terms that list this drug AND patient has GP dx for","+        valid_indications = {","+            term: patient_gp_terms[term]","+            for term in drug_terms","+            if term in patient_gp_terms","+        }","+","+        if len(valid_indications) == 1:","+            matched_term = next(iter(valid_indications))","+            pair_indication_cache[(upid, drug_name)] = (matched_term, True)","+            match_count += 1","+","+        elif len(valid_indications) > 1:","+            # Tiebreaker: highest code_frequency, then alphabetical","+            sorted_terms = sorted(","+                valid_indications.items(),","+                key=lambda x: (-x[1], x[0]),","+            )","+            matched_term = sorted_terms[0][0]","+            pair_indication_cache[(upid, drug_name)] = (matched_term, True)","+            match_count += 1","+            tiebreak_count += 1","+","+        else:","+            # No intersection: fallback to directory","+            if pd.notna(directory):","+                fallback_label = f\"{directory} (no GP dx)\"","+            else:","+                fallback_label = \"UNKNOWN (no GP dx)\"","+            pair_indication_cache[(upid, drug_name)] = (fallback_label, False)","+            fallback_count += 1","+","+    total_pairs = len(unique_pairs)","+    logger.info(f\"Drug-indication matching complete:\")","+    logger.info(f\"  Total UPID-Drug pairs: {total_pairs}\")","+    logger.info(f\"  Matched (GP dx + drug mapping): {match_count} ({100*match_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Matched: 0\")","+    logger.info(f\"  Tiebreaker used: {tiebreak_count}\")","+    logger.info(f\"  Fallback (no match): {fallback_count} ({100*fallback_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Fallback: 0\")","+","+    # Apply modified UPIDs to all rows","+    # Build vectorized lookup: original UPID + Drug Name -> modified UPID","+    def build_modified_upid(row: \"pd.Series\") -> str:","+        upid = row['UPID']","+        drug_name = row['Drug Name']","+        key = (upid, drug_name)","+        if key in pair_indication_cache:","+            indication, _ = pair_indication_cache[key]","+            return f\"{upid}|{indication}\"","+        # Shouldn't happen, but fallback","+        return f\"{upid}|UNKNOWN (no GP dx)\"","+","+    modified_df['UPID'] = modified_df.apply(build_modified_upid, axis=1)","+","+    # Build indication_df: modified UPID -> Search_Term/fallback label","+    # This maps each unique modified UPID to its indication for the pathway hierarchy","+    indication_records: list[dict[str, str]] = []","+    seen_upids: set[str] = set()","+","+    for (original_upid, drug_name), (indication, is_diagnosis) in pair_indication_cache.items():","+        modified_upid = f\"{original_upid}|{indication}\"","+        if modified_upid not in seen_upids:","+            indication_records.append({","+                'UPID': modified_upid,","+                'Directory': indication,","+            })","+            seen_upids.add(modified_upid)","+","+    indication_df = pd.DataFrame(indication_records)","+    if not indication_df.empty:","+        indication_df = indication_df.set_index('UPID')","+","+    logger.info(f\"  Unique modified UPIDs: {len(indication_df)}\")","+","+    # Log top indications","+    if not indication_df.empty:","+        top_terms = indication_df['Directory'].value_counts().head(5)","+        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")","+","+    return modified_df, indication_df","+","+"," # === NEW APPROACH: Query Snowflake directly using cluster CTE ==="," "," # The cluster query mapping (embedded from snomed_indication_mapping_query.sql)"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01LynK621bSTpdnJ3nPMdTMj","type":"message","role":"assistant","content":[{"type":"text","text":"Now I need to update the `__all__` exports to include `assign_drug_indications`."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2489,"cache_read_input_tokens":118570,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2489},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"c4326d49-e7db-4967-be5d-79a796eea31c"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01LynK621bSTpdnJ3nPMdTMj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01P3HnsMPAd3tspRPjJarwiD","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","offset":1680,"limit":50}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":2489,"cache_read_input_tokens":118570,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2489},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"43792b26-9624-408b-8f73-6126610238ac"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01P3HnsMPAd3tspRPjJarwiD","type":"tool_result","content":"  1680ÔåÆ\n  1681ÔåÆ        except Exception as e:\n  1682ÔåÆ            logger.error(f\"Error querying GP records for batch {batch_num}: {e}\")\n  1683ÔåÆ            # Continue with other batches - partial results are better than none\n  1684ÔåÆ\n  1685ÔåÆ    # Build result DataFrame\n  1686ÔåÆ    result_df = pd.DataFrame(all_results)\n  1687ÔåÆ\n  1688ÔåÆ    # Log summary statistics\n  1689ÔåÆ    if len(result_df) > 0:\n  1690ÔåÆ        unique_patients = result_df['PatientPseudonym'].nunique()\n  1691ÔåÆ        total_rows = len(result_df)\n  1692ÔåÆ        match_rate = 100 * unique_patients / total_patients\n  1693ÔåÆ        unique_terms = result_df['Search_Term'].nunique()\n  1694ÔåÆ        avg_indications = total_rows / unique_patients if unique_patients > 0 else 0\n  1695ÔåÆ        logger.info(f\"Indication lookup complete:\")\n  1696ÔåÆ        logger.info(f\"  Total patients queried: {total_patients}\")\n  1697ÔåÆ        logger.info(f\"  Patients with GP match: {unique_patients} ({match_rate:.1f}%)\")\n  1698ÔåÆ        logger.info(f\"  Total patient-indication rows: {total_rows} (avg {avg_indications:.1f} per patient)\")\n  1699ÔåÆ        logger.info(f\"  Unique Search_Terms found: {unique_terms}\")\n  1700ÔåÆ\n  1701ÔåÆ        # Log top Search_Terms\n  1702ÔåÆ        top_terms = result_df['Search_Term'].value_counts().head(5)\n  1703ÔåÆ        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n  1704ÔåÆ    else:\n  1705ÔåÆ        logger.info(f\"Indication lookup complete: 0 matches from {total_patients} patients\")\n  1706ÔåÆ\n  1707ÔåÆ    return result_df\n  1708ÔåÆ\n  1709ÔåÆ\n  1710ÔåÆ# Export public API\n  1711ÔåÆ__all__ = [\n  1712ÔåÆ    # Dataclasses\n  1713ÔåÆ    \"ClusterSnomedCodes\",\n  1714ÔåÆ    \"IndicationValidationResult\",\n  1715ÔåÆ    \"DrugIndicationMatchRate\",\n  1716ÔåÆ    \"DrugSnomedMapping\",\n  1717ÔåÆ    \"DirectSnomedMatchResult\",\n  1718ÔåÆ    \"DirectorateAssignment\",\n  1719ÔåÆ    # Cluster-based lookup functions (existing)\n  1720ÔåÆ    \"get_drug_clusters\",\n  1721ÔåÆ    \"get_drug_cluster_ids\",\n  1722ÔåÆ    \"get_cluster_snomed_codes\",\n  1723ÔåÆ    \"patient_has_indication\",\n  1724ÔåÆ    \"validate_indication\",\n  1725ÔåÆ    \"get_indication_match_rate\",\n  1726ÔåÆ    \"batch_validate_indications\",\n  1727ÔåÆ    \"get_available_clusters\",\n  1728ÔåÆ    # Direct SNOMED lookup functions (new)\n  1729ÔåÆ    \"get_drug_snomed_codes\",\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"81cd2c30-66ca-4ea6-b1a6-7e75d001e380","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","content":"\n        except Exception as e:\n            logger.error(f\"Error querying GP records for batch {batch_num}: {e}\")\n            # Continue with other batches - partial results are better than none\n\n    # Build result DataFrame\n    result_df = pd.DataFrame(all_results)\n\n    # Log summary statistics\n    if len(result_df) > 0:\n        unique_patients = result_df['PatientPseudonym'].nunique()\n        total_rows = len(result_df)\n        match_rate = 100 * unique_patients / total_patients\n        unique_terms = result_df['Search_Term'].nunique()\n        avg_indications = total_rows / unique_patients if unique_patients > 0 else 0\n        logger.info(f\"Indication lookup complete:\")\n        logger.info(f\"  Total patients queried: {total_patients}\")\n        logger.info(f\"  Patients with GP match: {unique_patients} ({match_rate:.1f}%)\")\n        logger.info(f\"  Total patient-indication rows: {total_rows} (avg {avg_indications:.1f} per patient)\")\n        logger.info(f\"  Unique Search_Terms found: {unique_terms}\")\n\n        # Log top Search_Terms\n        top_terms = result_df['Search_Term'].value_counts().head(5)\n        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n    else:\n        logger.info(f\"Indication lookup complete: 0 matches from {total_patients} patients\")\n\n    return result_df\n\n\n# Export public API\n__all__ = [\n    # Dataclasses\n    \"ClusterSnomedCodes\",\n    \"IndicationValidationResult\",\n    \"DrugIndicationMatchRate\",\n    \"DrugSnomedMapping\",\n    \"DirectSnomedMatchResult\",\n    \"DirectorateAssignment\",\n    # Cluster-based lookup functions (existing)\n    \"get_drug_clusters\",\n    \"get_drug_cluster_ids\",\n    \"get_cluster_snomed_codes\",\n    \"patient_has_indication\",\n    \"validate_indication\",\n    \"get_indication_match_rate\",\n    \"batch_validate_indications\",\n    \"get_available_clusters\",\n    # Direct SNOMED lookup functions (new)\n    \"get_drug_snomed_codes\",","numLines":50,"startLine":1680,"totalLines":1743}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_012fgNyzz6xvNiaAmHb9c3ns","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NWLps6SFKBgw3hMjy9K8Wu","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","offset":1730,"limit":20}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1009,"cache_read_input_tokens":121059,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1009},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f2cbf03e-c42d-4652-bcc5-4ff80f531953"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NWLps6SFKBgw3hMjy9K8Wu","type":"tool_result","content":"  1730ÔåÆ    \"patient_has_indication_direct\",\n  1731ÔåÆ    # Diagnosis-based directorate assignment\n  1732ÔåÆ    \"get_directorate_from_diagnosis\",\n  1733ÔåÆ    # Batch lookup for indication groups\n  1734ÔåÆ    \"batch_lookup_indication_groups\",\n  1735ÔåÆ    # Drug-indication mapping from DimSearchTerm.csv\n  1736ÔåÆ    \"SEARCH_TERM_MERGE_MAP\",\n  1737ÔåÆ    \"load_drug_indication_mapping\",\n  1738ÔåÆ    \"get_search_terms_for_drug\",\n  1739ÔåÆ    # Snowflake-direct indication lookup (new approach)\n  1740ÔåÆ    \"get_patient_indication_groups\",\n  1741ÔåÆ    \"CLUSTER_MAPPING_SQL\",\n  1742ÔåÆ]\n  1743ÔåÆ\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"6112129f-299e-478e-916b-54eebc3f7572","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","content":"    \"patient_has_indication_direct\",\n    # Diagnosis-based directorate assignment\n    \"get_directorate_from_diagnosis\",\n    # Batch lookup for indication groups\n    \"batch_lookup_indication_groups\",\n    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Snowflake-direct indication lookup (new approach)\n    \"get_patient_indication_groups\",\n    \"CLUSTER_MAPPING_SQL\",\n]\n","numLines":14,"startLine":1730,"totalLines":1743}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01HpJf369FPd2nn1MWFnhkNS","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BpUHu7FzeRR8mbqm26MGow","name":"Edit","input":{"replace_all":false,"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","old_string":"    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Snowflake-direct indication lookup (new approach)","new_string":"    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Drug-aware indication assignment\n    \"assign_drug_indications\",\n    # Snowflake-direct indication lookup (new approach)"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":408,"cache_read_input_tokens":122068,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":408},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"66a7faca-f0af-46a5-9ef7-d11a9ab83a16"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BpUHu7FzeRR8mbqm26MGow","type":"tool_result","content":"The file C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"9bb0f813-c1fb-484f-9dc0-48b3f4757036","tool_use_result":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\data_processing\\diagnosis_lookup.py","oldString":"    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Snowflake-direct indication lookup (new approach)","newString":"    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Drug-aware indication assignment\n    \"assign_drug_indications\",\n    # Snowflake-direct indication lookup (new approach)","originalFile":"\"\"\"\nDiagnosis lookup module for NHS Patient Pathway Analysis.\n\nProvides functions to validate patient indications by checking GP diagnosis records\nagainst SNOMED cluster codes. Uses the drug-to-cluster mapping from\ndrug_indication_clusters.csv and queries Snowflake for SNOMED codes and GP records.\n\nKey workflow:\n1. Get drug's valid indication clusters from local mapping\n2. Get all SNOMED codes for those clusters from Snowflake\n3. Check if patient has any of those SNOMED codes in GP records\n4. Report indication validation status\n\nIMPORTANT: HCD activity data indication codes are UNRELIABLE. This module uses\nGP/Primary Care data (PrimaryCareClinicalCoding) as the authoritative source.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom datetime import date, datetime\nfrom pathlib import Path\nfrom typing import Optional, Callable, Any, cast, TYPE_CHECKING\nimport csv\n\nif TYPE_CHECKING:\n    import pandas as pd\n\nfrom core.logging_config import get_logger\nfrom data_processing.database import DatabaseManager, default_db_manager\nfrom data_processing.snowflake_connector import (\n    SnowflakeConnector,\n    get_connector,\n    is_snowflake_available,\n    is_snowflake_configured,\n    SNOWFLAKE_AVAILABLE,\n)\nfrom data_processing.cache import get_cache, is_cache_enabled\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass ClusterSnomedCodes:\n    \"\"\"SNOMED codes for a clinical coding cluster.\"\"\"\n    cluster_id: str\n    cluster_description: str\n    snomed_codes: list[str] = field(default_factory=list)\n    snomed_descriptions: dict[str, str] = field(default_factory=dict)\n\n    @property\n    def code_count(self) -> int:\n        return len(self.snomed_codes)\n\n\n@dataclass\nclass IndicationValidationResult:\n    \"\"\"Result of validating a patient's indication for a drug.\"\"\"\n    patient_pseudonym: str\n    drug_name: str\n    has_valid_indication: bool\n    matched_cluster_id: Optional[str] = None\n    matched_snomed_code: Optional[str] = None\n    matched_snomed_description: Optional[str] = None\n    checked_clusters: list[str] = field(default_factory=list)\n    total_codes_checked: int = 0\n    source: str = \"GP_SNOMED\"  # GP_SNOMED | NONE\n    error_message: Optional[str] = None\n\n\n@dataclass\nclass DrugIndicationMatchRate:\n    \"\"\"Match rate statistics for a drug's indication validation.\"\"\"\n    drug_name: str\n    total_patients: int\n    patients_with_indication: int\n    patients_without_indication: int\n    match_rate: float  # 0.0 to 1.0\n    clusters_checked: list[str] = field(default_factory=list)\n    sample_unmatched: list[str] = field(default_factory=list)  # Sample patient IDs\n\n\n@dataclass\nclass DrugSnomedMapping:\n    \"\"\"SNOMED code mapping for a drug from ref_drug_snomed_mapping.\"\"\"\n    snomed_code: str\n    snomed_description: str\n    search_term: str\n    primary_directorate: str\n    indication: str = \"\"\n    ta_id: str = \"\"\n\n\n@dataclass\nclass DirectSnomedMatchResult:\n    \"\"\"Result of direct SNOMED code lookup in GP records.\"\"\"\n    patient_pseudonym: str\n    matched: bool\n    snomed_code: Optional[str] = None\n    snomed_description: Optional[str] = None\n    search_term: Optional[str] = None\n    primary_directorate: Optional[str] = None\n    event_date: Optional[datetime] = None\n    source: str = \"DIRECT_SNOMED\"  # DIRECT_SNOMED | NONE\n\n\n@dataclass\nclass DirectorateAssignment:\n    \"\"\"Result of directorate assignment for a patient-drug combination.\"\"\"\n    upid: str\n    drug_name: str\n    directorate: Optional[str]\n    search_term: Optional[str] = None\n    source: str = \"FALLBACK\"  # DIAGNOSIS | FALLBACK\n    snomed_code: Optional[str] = None\n    event_date: Optional[datetime] = None\n\n\ndef get_drug_clusters(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[dict]:\n    \"\"\"\n    Get all SNOMED cluster mappings for a drug from local SQLite.\n\n    Args:\n        drug_name: Drug name to look up (case-insensitive)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n\n    Returns:\n        List of dicts with keys: drug_name, indication, cluster_id,\n        cluster_description, nice_ta_reference\n    \"\"\"\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    query = \"\"\"\n        SELECT drug_name, indication, cluster_id, cluster_description, nice_ta_reference\n        FROM ref_drug_indication_clusters\n        WHERE UPPER(drug_name) = UPPER(?)\n        ORDER BY indication, cluster_id\n    \"\"\"\n\n    try:\n        with db_manager.get_connection() as conn:\n            cursor = conn.execute(query, (drug_name,))\n            rows = cursor.fetchall()\n\n            results = []\n            for row in rows:\n                results.append({\n                    \"drug_name\": row[\"drug_name\"],\n                    \"indication\": row[\"indication\"],\n                    \"cluster_id\": row[\"cluster_id\"],\n                    \"cluster_description\": row[\"cluster_description\"],\n                    \"nice_ta_reference\": row[\"nice_ta_reference\"],\n                })\n\n            logger.debug(f\"Found {len(results)} cluster mappings for drug '{drug_name}'\")\n            return results\n\n    except Exception as e:\n        logger.error(f\"Error getting clusters for drug '{drug_name}': {e}\")\n        return []\n\n\ndef get_drug_cluster_ids(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[str]:\n    \"\"\"\n    Get unique cluster IDs for a drug.\n\n    Args:\n        drug_name: Drug name to look up\n        db_manager: Optional DatabaseManager\n\n    Returns:\n        List of unique cluster IDs\n    \"\"\"\n    clusters = get_drug_clusters(drug_name, db_manager)\n    return list(set(c[\"cluster_id\"] for c in clusters))\n\n\ndef get_drug_snomed_codes(\n    drug_name: str,\n    db_manager: Optional[DatabaseManager] = None\n) -> list[DrugSnomedMapping]:\n    \"\"\"\n    Get all SNOMED codes for a drug from local ref_drug_snomed_mapping table.\n\n    This uses the enriched mapping CSV data loaded into SQLite, which provides\n    direct SNOMED-to-drug mappings with Search_Term and PrimaryDirectorate.\n\n    Args:\n        drug_name: Drug name to look up (case-insensitive, matches cleaned_drug_name)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n\n    Returns:\n        List of DrugSnomedMapping with snomed_code, snomed_description,\n        search_term, primary_directorate, indication, ta_id\n    \"\"\"\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    query = \"\"\"\n        SELECT DISTINCT\n            snomed_code,\n            snomed_description,\n            search_term,\n            primary_directorate,\n            indication,\n            ta_id\n        FROM ref_drug_snomed_mapping\n        WHERE UPPER(cleaned_drug_name) = UPPER(?)\n           OR UPPER(drug_name) = UPPER(?)\n        ORDER BY search_term, snomed_code\n    \"\"\"\n\n    try:\n        with db_manager.get_connection() as conn:\n            cursor = conn.execute(query, (drug_name, drug_name))\n            rows = cursor.fetchall()\n\n            results = []\n            for row in rows:\n                results.append(DrugSnomedMapping(\n                    snomed_code=row[\"snomed_code\"],\n                    snomed_description=row[\"snomed_description\"] or \"\",\n                    search_term=row[\"search_term\"] or \"\",\n                    primary_directorate=row[\"primary_directorate\"] or \"\",\n                    indication=row[\"indication\"] or \"\",\n                    ta_id=row[\"ta_id\"] or \"\",\n                ))\n\n            logger.debug(f\"Found {len(results)} SNOMED mappings for drug '{drug_name}'\")\n            return results\n\n    except Exception as e:\n        logger.error(f\"Error getting SNOMED codes for drug '{drug_name}': {e}\")\n        return []\n\n\ndef patient_has_indication_direct(\n    patient_pseudonym: str,\n    drug_snomed_mappings: list[DrugSnomedMapping],\n    connector: Optional[SnowflakeConnector] = None,\n    before_date: Optional[date] = None,\n) -> DirectSnomedMatchResult:\n    \"\"\"\n    Check if patient has any of the SNOMED codes in their GP records.\n\n    This is the direct SNOMED lookup - it queries PrimaryCareClinicalCoding\n    for exact SNOMED code matches (not via cluster). Returns the most recent\n    match by EventDateTime if multiple matches exist.\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        drug_snomed_mappings: List of DrugSnomedMapping from get_drug_snomed_codes()\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        DirectSnomedMatchResult with match details (most recent by EventDateTime)\n    \"\"\"\n    result = DirectSnomedMatchResult(\n        patient_pseudonym=patient_pseudonym,\n        matched=False,\n        source=\"NONE\",\n    )\n\n    if not drug_snomed_mappings:\n        return result\n\n    if not SNOWFLAKE_AVAILABLE:\n        logger.warning(\"Snowflake connector not available\")\n        return result\n\n    if not is_snowflake_configured():\n        logger.warning(\"Snowflake not configured - cannot check GP records\")\n        return result\n\n    if connector is None:\n        connector = get_connector()\n\n    # Build lookup dict for mapping snomed_code -> (search_term, primary_directorate, snomed_description)\n    snomed_lookup = {\n        m.snomed_code: (m.search_term, m.primary_directorate, m.snomed_description)\n        for m in drug_snomed_mappings\n    }\n\n    # Get unique SNOMED codes\n    snomed_codes = list(snomed_lookup.keys())\n\n    # Build placeholders for SNOMED codes\n    placeholders = \", \".join([\"%s\"] * len(snomed_codes))\n\n    # Query to find most recent matching SNOMED code in GP records\n    query = f'''\n        SELECT\n            \"SNOMEDCode\",\n            \"EventDateTime\"\n        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n        WHERE \"PatientPseudonym\" = %s\n            AND \"SNOMEDCode\" IN ({placeholders})\n    '''\n\n    params: list = [patient_pseudonym] + snomed_codes\n\n    if before_date:\n        query += ' AND \"EventDateTime\" < %s'\n        params.append(before_date.isoformat())\n\n    query += ' ORDER BY \"EventDateTime\" DESC LIMIT 1'\n\n    try:\n        results = connector.execute_dict(query, tuple(params))\n\n        if results:\n            row = results[0]\n            matched_code = row.get(\"SNOMEDCode\")\n            event_dt = row.get(\"EventDateTime\")\n\n            if matched_code and matched_code in snomed_lookup:\n                search_term, primary_dir, snomed_desc = snomed_lookup[matched_code]\n\n                return DirectSnomedMatchResult(\n                    patient_pseudonym=patient_pseudonym,\n                    matched=True,\n                    snomed_code=matched_code,\n                    snomed_description=snomed_desc,\n                    search_term=search_term,\n                    primary_directorate=primary_dir,\n                    event_date=event_dt,\n                    source=\"DIRECT_SNOMED\",\n                )\n\n        return result\n\n    except Exception as e:\n        logger.error(f\"Error checking direct SNOMED for patient '{patient_pseudonym}': {e}\")\n        return result\n\n\ndef get_directorate_from_diagnosis(\n    upid: str,\n    drug_name: str,\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    before_date: Optional[date] = None,\n) -> DirectorateAssignment:\n    \"\"\"\n    Get directorate assignment for a patient-drug combination using diagnosis-based lookup.\n\n    This function attempts to assign a directorate based on the patient's GP records\n    (direct SNOMED code matching). If no match is found, it returns a FALLBACK result\n    indicating that the caller should use alternative assignment methods (e.g.,\n    department_identification() from tools/data.py).\n\n    Workflow:\n    1. Get all SNOMED codes for the drug from ref_drug_snomed_mapping\n    2. Query patient's GP records for matching SNOMED codes\n    3. If match found ÔåÆ return diagnosis-based directorate and search_term\n    4. If no match ÔåÆ return FALLBACK result (caller handles fallback logic)\n\n    Args:\n        upid: Patient's unique patient ID (Provider Code[:3] + PersonKey)\n        drug_name: Drug name to look up\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        DirectorateAssignment with directorate, search_term, and source\n    \"\"\"\n    result = DirectorateAssignment(\n        upid=upid,\n        drug_name=drug_name,\n        directorate=None,\n        source=\"FALLBACK\",\n    )\n\n    # Step 1: Get SNOMED codes for the drug\n    drug_snomed_mappings = get_drug_snomed_codes(drug_name, db_manager)\n\n    if not drug_snomed_mappings:\n        logger.debug(f\"No SNOMED mappings found for drug '{drug_name}' - using fallback\")\n        return result\n\n    # Step 2: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        logger.debug(\"Snowflake not available - using fallback\")\n        return result\n\n    if not is_snowflake_configured():\n        logger.debug(\"Snowflake not configured - using fallback\")\n        return result\n\n    # Step 3: Get patient pseudonym from UPID\n    # UPID format is Provider Code (3 chars) + PersonKey\n    # We need to query Snowflake to get the PatientPseudonym for this PersonKey\n    # However, patient_has_indication_direct expects PatientPseudonym, not UPID\n    # For now, we'll use UPID as the identifier - the actual integration\n    # will need to happen at the DataFrame level where we have PersonKey\n    #\n    # NOTE: This function will be called from the pipeline where we have\n    # access to PatientPseudonym. The UPID is passed for logging/tracking.\n\n    # Actually, looking at the pipeline, we need PatientPseudonym, not UPID.\n    # The caller should pass the PatientPseudonym or we need to look it up.\n    # For now, let's assume the caller will use this in a batch context\n    # where they can map UPID -> PatientPseudonym.\n\n    # Let me reconsider: the function signature takes UPID but we need\n    # PatientPseudonym for Snowflake. In the pipeline context (fetch_and_transform_data),\n    # we'll have the PersonKey column which IS the PatientPseudonym.\n    # So UPID = ProviderCode[:3] + PersonKey, and PersonKey = PatientPseudonym.\n    #\n    # We can extract PatientPseudonym from UPID by removing the first 3 chars.\n    patient_pseudonym = upid[3:] if len(upid) > 3 else upid\n\n    # Step 4: Check patient's GP records for matching SNOMED codes\n    match_result = patient_has_indication_direct(\n        patient_pseudonym=patient_pseudonym,\n        drug_snomed_mappings=drug_snomed_mappings,\n        connector=connector,\n        before_date=before_date,\n    )\n\n    if match_result.matched and match_result.primary_directorate:\n        return DirectorateAssignment(\n            upid=upid,\n            drug_name=drug_name,\n            directorate=match_result.primary_directorate,\n            search_term=match_result.search_term,\n            source=\"DIAGNOSIS\",\n            snomed_code=match_result.snomed_code,\n            event_date=match_result.event_date,\n        )\n\n    # No match found - return fallback result\n    return result\n\n\ndef get_cluster_snomed_codes(\n    cluster_id: str,\n    connector: Optional[SnowflakeConnector] = None,\n    use_cache: bool = True,\n) -> ClusterSnomedCodes:\n    \"\"\"\n    Get all SNOMED codes for a cluster from Snowflake.\n\n    Queries the ClinicalCodingClusterSnomedCodes table to get all SNOMED codes\n    that belong to the specified cluster.\n\n    Args:\n        cluster_id: Cluster ID to look up (e.g., 'RARTH_COD', 'PSORIASIS_COD')\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        use_cache: Whether to use cached results (default True)\n\n    Returns:\n        ClusterSnomedCodes with list of SNOMED codes and descriptions\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE:\n        logger.warning(\"Snowflake connector not available\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n    if not is_snowflake_configured():\n        logger.warning(\"Snowflake not configured - cannot get cluster codes\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n    # Check cache first\n    cache_key = f\"cluster_snomed_{cluster_id}\"\n    if use_cache and is_cache_enabled():\n        cache = get_cache()\n        cached = cache.get(cache_key)\n        if cached is not None and len(cached) > 0:\n            logger.debug(f\"Using cached SNOMED codes for cluster '{cluster_id}'\")\n            cached_dict = cached[0]  # First element is our data dict\n            return ClusterSnomedCodes(\n                cluster_id=cluster_id,\n                cluster_description=str(cached_dict.get(\"description\", \"\")),\n                snomed_codes=list(cached_dict.get(\"codes\", [])),\n                snomed_descriptions=dict(cached_dict.get(\"descriptions\", {})),\n            )\n\n    if connector is None:\n        connector = get_connector()\n\n    query = '''\n        SELECT DISTINCT\n            \"Cluster_ID\",\n            \"Cluster_Description\",\n            \"SNOMEDCode\",\n            \"SNOMEDDescription\"\n        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n        WHERE \"Cluster_ID\" = %s\n        ORDER BY \"SNOMEDCode\"\n    '''\n\n    try:\n        results = connector.execute_dict(query, (cluster_id,))\n\n        if not results:\n            logger.warning(f\"No SNOMED codes found for cluster '{cluster_id}'\")\n            return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n        codes = []\n        descriptions = {}\n        description = results[0].get(\"Cluster_Description\", \"\") if results else \"\"\n\n        for row in results:\n            code = row.get(\"SNOMEDCode\")\n            if code:\n                codes.append(code)\n                descriptions[code] = row.get(\"SNOMEDDescription\", \"\")\n\n        logger.info(f\"Found {len(codes)} SNOMED codes for cluster '{cluster_id}'\")\n\n        # Cache the results (using query-based cache with fake params)\n        if use_cache and is_cache_enabled():\n            cache = get_cache()\n            cache_data = [{\n                \"description\": description,\n                \"codes\": codes,\n                \"descriptions\": descriptions,\n            }]\n            cache.set(cache_key, None, cache_data)  # type: ignore[arg-type]\n\n        return ClusterSnomedCodes(\n            cluster_id=cluster_id,\n            cluster_description=description,\n            snomed_codes=codes,\n            snomed_descriptions=descriptions,\n        )\n\n    except Exception as e:\n        logger.error(f\"Error getting SNOMED codes for cluster '{cluster_id}': {e}\")\n        return ClusterSnomedCodes(cluster_id=cluster_id, cluster_description=\"\")\n\n\ndef patient_has_indication(\n    patient_pseudonym: str,\n    cluster_ids: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    before_date: Optional[date] = None,\n) -> tuple[bool, Optional[str], Optional[str], Optional[str]]:\n    \"\"\"\n    Check if a patient has any SNOMED codes from the specified clusters in GP records.\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        cluster_ids: List of cluster IDs to check against\n        connector: Optional SnowflakeConnector\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        Tuple of (has_indication, matched_cluster_id, matched_snomed_code, matched_description)\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        return False, None, None, None\n\n    if not cluster_ids:\n        return False, None, None, None\n\n    if connector is None:\n        connector = get_connector()\n\n    # Build placeholders for cluster IDs\n    placeholders = \", \".join([\"%s\"] * len(cluster_ids))\n\n    # Query to check if patient has any matching SNOMED code\n    query = f'''\n        SELECT\n            pc.\"SNOMEDCode\",\n            cc.\"Cluster_ID\",\n            cc.\"SNOMEDDescription\"\n        FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\n        INNER JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" cc\n            ON pc.\"SNOMEDCode\" = cc.\"SNOMEDCode\"\n        WHERE pc.\"PatientPseudonym\" = %s\n            AND cc.\"Cluster_ID\" IN ({placeholders})\n    '''\n\n    params = [patient_pseudonym] + cluster_ids\n\n    if before_date:\n        query += ' AND pc.\"EventDateTime\" < %s'\n        params.append(before_date.isoformat())\n\n    query += ' LIMIT 1'\n\n    try:\n        results = connector.execute_dict(query, tuple(params))\n\n        if results:\n            row = results[0]\n            return (\n                True,\n                row.get(\"Cluster_ID\"),\n                row.get(\"SNOMEDCode\"),\n                row.get(\"SNOMEDDescription\"),\n            )\n\n        return False, None, None, None\n\n    except Exception as e:\n        logger.error(f\"Error checking indication for patient '{patient_pseudonym}': {e}\")\n        return False, None, None, None\n\n\ndef validate_indication(\n    patient_pseudonym: str,\n    drug_name: str,\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    before_date: Optional[date] = None,\n) -> IndicationValidationResult:\n    \"\"\"\n    Validate that a patient has an appropriate indication for a drug.\n\n    Full validation workflow:\n    1. Get drug's valid indication clusters from local mapping\n    2. Check if patient has any matching SNOMED codes in GP records\n    3. Return detailed validation result\n\n    Args:\n        patient_pseudonym: Patient's pseudonymised NHS number\n        drug_name: Drug name to validate indication for\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        before_date: Optional date - only check diagnoses before this date\n\n    Returns:\n        IndicationValidationResult with validation details\n    \"\"\"\n    result = IndicationValidationResult(\n        patient_pseudonym=patient_pseudonym,\n        drug_name=drug_name,\n        has_valid_indication=False,\n    )\n\n    # Step 1: Get drug's cluster mappings\n    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n\n    if not cluster_ids:\n        result.error_message = f\"No cluster mappings found for drug '{drug_name}'\"\n        result.source = \"NONE\"\n        return result\n\n    result.checked_clusters = cluster_ids\n\n    # Step 2: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        result.error_message = \"Snowflake connector not installed\"\n        result.source = \"NONE\"\n        return result\n\n    if not is_snowflake_configured():\n        result.error_message = \"Snowflake not configured\"\n        result.source = \"NONE\"\n        return result\n\n    # Step 3: Check patient GP records\n    has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n        patient_pseudonym=patient_pseudonym,\n        cluster_ids=cluster_ids,\n        connector=connector,\n        before_date=before_date,\n    )\n\n    result.has_valid_indication = has_indication\n    result.matched_cluster_id = matched_cluster\n    result.matched_snomed_code = matched_code\n    result.matched_snomed_description = matched_desc\n    result.source = \"GP_SNOMED\" if has_indication else \"NONE\"\n\n    return result\n\n\ndef get_indication_match_rate(\n    drug_name: str,\n    patient_pseudonyms: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    sample_unmatched_count: int = 10,\n) -> DrugIndicationMatchRate:\n    \"\"\"\n    Calculate indication match rate for a drug across a list of patients.\n\n    Args:\n        drug_name: Drug name to check\n        patient_pseudonyms: List of patient pseudonymised NHS numbers\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        sample_unmatched_count: Number of unmatched patient IDs to include in sample\n\n    Returns:\n        DrugIndicationMatchRate with match statistics\n    \"\"\"\n    if connector is None and SNOWFLAKE_AVAILABLE and is_snowflake_configured():\n        connector = get_connector()\n\n    cluster_ids = get_drug_cluster_ids(drug_name, db_manager)\n\n    total = len(patient_pseudonyms)\n    matched = 0\n    unmatched = 0\n    sample_unmatched: list[str] = []\n\n    if not cluster_ids:\n        logger.warning(f\"No cluster mappings for drug '{drug_name}' - all patients will be unmatched\")\n        return DrugIndicationMatchRate(\n            drug_name=drug_name,\n            total_patients=total,\n            patients_with_indication=0,\n            patients_without_indication=total,\n            match_rate=0.0,\n            clusters_checked=[],\n            sample_unmatched=patient_pseudonyms[:sample_unmatched_count],\n        )\n\n    for i, pseudonym in enumerate(patient_pseudonyms):\n        if i > 0 and i % 100 == 0:\n            logger.info(f\"Validating indications: {i}/{total} ({100*i/total:.1f}%)\")\n\n        has_indication, _, _, _ = patient_has_indication(\n            patient_pseudonym=pseudonym,\n            cluster_ids=cluster_ids,\n            connector=connector,\n        )\n\n        if has_indication:\n            matched += 1\n        else:\n            unmatched += 1\n            if len(sample_unmatched) < sample_unmatched_count:\n                sample_unmatched.append(pseudonym)\n\n    match_rate = matched / total if total > 0 else 0.0\n\n    logger.info(f\"Indication match rate for '{drug_name}': {100*match_rate:.1f}% ({matched}/{total})\")\n\n    return DrugIndicationMatchRate(\n        drug_name=drug_name,\n        total_patients=total,\n        patients_with_indication=matched,\n        patients_without_indication=unmatched,\n        match_rate=match_rate,\n        clusters_checked=cluster_ids,\n        sample_unmatched=sample_unmatched,\n    )\n\n\ndef batch_validate_indications(\n    patient_drug_pairs: list[tuple[str, str]],\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -> list[IndicationValidationResult]:\n    \"\"\"\n    Validate indications for multiple patient-drug pairs efficiently.\n\n    Args:\n        patient_drug_pairs: List of (patient_pseudonym, drug_name) tuples\n        connector: Optional SnowflakeConnector\n        db_manager: Optional DatabaseManager\n        progress_callback: Optional callback(current, total) for progress updates\n\n    Returns:\n        List of IndicationValidationResult for each pair\n    \"\"\"\n    results = []\n    total = len(patient_drug_pairs)\n\n    # Cache cluster lookups by drug\n    drug_clusters_cache = {}\n\n    for i, (pseudonym, drug_name) in enumerate(patient_drug_pairs):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # Get clusters from cache or lookup\n        drug_upper = drug_name.upper()\n        if drug_upper not in drug_clusters_cache:\n            drug_clusters_cache[drug_upper] = get_drug_cluster_ids(drug_name, db_manager)\n\n        cluster_ids = drug_clusters_cache[drug_upper]\n\n        if not cluster_ids:\n            results.append(IndicationValidationResult(\n                patient_pseudonym=pseudonym,\n                drug_name=drug_name,\n                has_valid_indication=False,\n                source=\"NONE\",\n                error_message=f\"No cluster mappings for drug '{drug_name}'\",\n            ))\n            continue\n\n        # Check patient indication\n        has_indication, matched_cluster, matched_code, matched_desc = patient_has_indication(\n            patient_pseudonym=pseudonym,\n            cluster_ids=cluster_ids,\n            connector=connector,\n        )\n\n        results.append(IndicationValidationResult(\n            patient_pseudonym=pseudonym,\n            drug_name=drug_name,\n            has_valid_indication=has_indication,\n            matched_cluster_id=matched_cluster,\n            matched_snomed_code=matched_code,\n            matched_snomed_description=matched_desc,\n            checked_clusters=cluster_ids,\n            source=\"GP_SNOMED\" if has_indication else \"NONE\",\n        ))\n\n    matched_count = sum(1 for r in results if r.has_valid_indication)\n    logger.info(f\"Batch validation complete: {matched_count}/{total} ({100*matched_count/total:.1f}%) with valid indications\")\n\n    return results\n\n\ndef get_available_clusters(\n    connector: Optional[SnowflakeConnector] = None,\n) -> list[dict]:\n    \"\"\"\n    Get list of all available SNOMED clusters from Snowflake.\n\n    Returns:\n        List of dicts with cluster_id, cluster_description, code_count\n    \"\"\"\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        logger.warning(\"Snowflake not available - cannot list clusters\")\n        return []\n\n    if connector is None:\n        connector = get_connector()\n\n    query = '''\n        SELECT\n            \"Cluster_ID\",\n            \"Cluster_Description\",\n            COUNT(DISTINCT \"SNOMEDCode\") as code_count\n        FROM DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\"\n        GROUP BY \"Cluster_ID\", \"Cluster_Description\"\n        ORDER BY \"Cluster_ID\"\n    '''\n\n    try:\n        results = connector.execute_dict(query)\n\n        clusters = []\n        for row in results:\n            clusters.append({\n                \"cluster_id\": row.get(\"Cluster_ID\"),\n                \"cluster_description\": row.get(\"Cluster_Description\"),\n                \"code_count\": row.get(\"code_count\", 0),\n            })\n\n        logger.info(f\"Found {len(clusters)} available SNOMED clusters\")\n        return clusters\n\n    except Exception as e:\n        logger.error(f\"Error getting available clusters: {e}\")\n        return []\n\n\ndef batch_lookup_indication_groups(\n    df: \"pd.DataFrame\",\n    connector: Optional[SnowflakeConnector] = None,\n    db_manager: Optional[DatabaseManager] = None,\n    batch_size: int = 500,\n) -> \"pd.DataFrame\":\n    \"\"\"\n    Batch lookup GP diagnosis-based indication groups for a DataFrame of patients.\n\n    This is the efficient batch version of get_directorate_from_diagnosis().\n    Instead of querying Snowflake per patient, it batches the lookups for performance.\n\n    Strategy:\n    1. Get all unique (PersonKey, Drug Name) pairs from DataFrame\n    2. For each unique drug, get all SNOMED codes from local SQLite\n    3. Build batched Snowflake queries to check GP records\n    4. Return indication_df mapping UPID ÔåÆ Indication_Group\n\n    For unmatched patients, Indication_Group will be their Directory (with suffix).\n\n    Args:\n        df: DataFrame with columns: UPID, Drug Name, Directory, PersonKey\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        db_manager: Optional DatabaseManager (defaults to default_db_manager)\n        batch_size: Number of patients per Snowflake query batch\n\n    Returns:\n        DataFrame with columns: UPID, Indication_Group, Source\n        - Indication_Group: Search_Term (if matched) or \"Directory (no GP dx)\" (if not)\n        - Source: \"DIAGNOSIS\" or \"FALLBACK\"\n    \"\"\"\n    import pandas as pd\n\n    if db_manager is None:\n        db_manager = default_db_manager\n\n    logger.info(f\"Starting batch indication lookup for {len(df)} records...\")\n\n    # Step 1: Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combinations\n    # We need PseudoNHSNoLinked to query Snowflake - this matches PatientPseudonym in GP records\n    # Note: PersonKey is LocalPatientID which is provider-specific and does NOT match GP records\n    if 'PseudoNHSNoLinked' not in df.columns:\n        logger.error(\"DataFrame missing 'PseudoNHSNoLinked' column - cannot lookup GP records\")\n        # Return fallback for all patients\n        result_df = df[['UPID', 'Directory']].drop_duplicates().copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']]\n\n    # Get unique patient-drug combinations (we need one lookup per patient-drug pair)\n    unique_pairs = df[['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']].drop_duplicates()\n    logger.info(f\"Found {len(unique_pairs)} unique patient-drug combinations\")\n\n    # Step 2: Get all unique drugs and their SNOMED codes\n    unique_drugs = unique_pairs['Drug Name'].unique()\n    logger.info(f\"Building SNOMED lookup for {len(unique_drugs)} unique drugs...\")\n\n    # Build drug -> list of DrugSnomedMapping dict\n    drug_snomed_map: dict[str, list[DrugSnomedMapping]] = {}\n    all_snomed_codes: set[str] = set()\n    snomed_to_drug_searchterm: dict[str, list[tuple[str, str, str]]] = {}  # snomed -> [(drug, search_term, primary_dir), ...]\n\n    for drug_name in unique_drugs:\n        mappings = get_drug_snomed_codes(drug_name, db_manager)\n        drug_snomed_map[drug_name] = mappings\n\n        for m in mappings:\n            all_snomed_codes.add(m.snomed_code)\n            if m.snomed_code not in snomed_to_drug_searchterm:\n                snomed_to_drug_searchterm[m.snomed_code] = []\n            snomed_to_drug_searchterm[m.snomed_code].append(\n                (drug_name, m.search_term, m.primary_directorate)\n            )\n\n    logger.info(f\"Total SNOMED codes to check: {len(all_snomed_codes)}\")\n\n    # Step 3: Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE or not is_snowflake_configured():\n        logger.warning(\"Snowflake not available - returning fallback for all patients\")\n        result_df = unique_pairs[['UPID', 'Directory']].copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n\n    if connector is None:\n        connector = get_connector()\n\n    # Step 4: Query GP records for all patients in batches\n    # The query finds the most recent matching SNOMED code for each patient\n\n    # Get unique PseudoNHSNoLinked values (each = one patient in GP records)\n    unique_patients = unique_pairs[['PseudoNHSNoLinked', 'UPID', 'Directory']].drop_duplicates(subset=['PseudoNHSNoLinked'])\n    patient_pseudonyms = unique_patients['PseudoNHSNoLinked'].tolist()\n\n    logger.info(f\"Querying GP records for {len(patient_pseudonyms)} unique patients in batches of {batch_size}...\")\n\n    # Results dict: PersonKey -> (snomed_code, event_date)\n    gp_matches: dict[str, tuple[str, Any]] = {}\n\n    # Convert SNOMED codes to list for query\n    snomed_list = list(all_snomed_codes)\n\n    if not snomed_list:\n        logger.warning(\"No SNOMED codes to check - returning fallback for all patients\")\n        result_df = unique_pairs[['UPID', 'Directory']].copy()\n        result_df['Indication_Group'] = result_df['Directory'] + \" (no GP dx)\"\n        result_df['Source'] = \"FALLBACK\"\n        return result_df[['UPID', 'Indication_Group', 'Source']].drop_duplicates(subset=['UPID'])\n\n    # Build SNOMED IN clause (reused across batches)\n    snomed_placeholders = \", \".join([\"%s\"] * len(snomed_list))\n\n    # Process patients in batches\n    for batch_start in range(0, len(patient_pseudonyms), batch_size):\n        batch_end = min(batch_start + batch_size, len(patient_pseudonyms))\n        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n\n        logger.info(f\"Batch {batch_start//batch_size + 1}: patients {batch_start} to {batch_end}\")\n\n        # Build patient IN clause\n        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n\n        # Query to find all matching SNOMED codes for these patients\n        # We'll get all matches and pick the most recent per patient in Python\n        query = f'''\n            SELECT\n                \"PatientPseudonym\",\n                \"SNOMEDCode\",\n                \"EventDateTime\"\n            FROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\"\n            WHERE \"PatientPseudonym\" IN ({patient_placeholders})\n              AND \"SNOMEDCode\" IN ({snomed_placeholders})\n            ORDER BY \"PatientPseudonym\", \"EventDateTime\" DESC\n        '''\n\n        params = tuple(batch_pseudonyms) + tuple(snomed_list)\n\n        try:\n            results = connector.execute_dict(query, params)\n\n            # Process results - pick most recent per patient\n            for row in results:\n                person_key = row.get(\"PatientPseudonym\")\n                snomed_code = row.get(\"SNOMEDCode\")\n                event_date = row.get(\"EventDateTime\")\n\n                if person_key and snomed_code:\n                    # Keep only if we haven't seen this patient yet (first = most recent due to ORDER BY)\n                    if person_key not in gp_matches:\n                        gp_matches[person_key] = (snomed_code, event_date)\n\n        except Exception as e:\n            logger.error(f\"Error querying GP records for batch: {e}\")\n            # Continue with other batches\n\n    logger.info(f\"Found GP matches for {len(gp_matches)} patients\")\n\n    # Step 5: Build result DataFrame\n    # For each unique_pair, determine Indication_Group based on match status\n    results_list = []\n\n    # We need to dedupe by UPID - a patient might be on multiple drugs\n    # Strategy: For each UPID, use the most recent match (if any)\n    upid_to_match: dict[str, tuple[str, str]] = {}  # UPID -> (Indication_Group, Source)\n\n    for _, row in unique_pairs.iterrows():\n        upid = row['UPID']\n        drug_name = row['Drug Name']\n        patient_pseudonym = row['PseudoNHSNoLinked']\n        directory = row['Directory']\n\n        # Check if patient has GP match (using PseudoNHSNoLinked which matches PatientPseudonym in GP)\n        if patient_pseudonym in gp_matches:\n            matched_snomed, event_date = gp_matches[patient_pseudonym]\n\n            # Find the search_term for this SNOMED code and drug\n            # (A SNOMED code might map to multiple drugs with different search_terms)\n            if matched_snomed in snomed_to_drug_searchterm:\n                # Look for match with current drug first\n                search_term = None\n                for drug, st, pd in snomed_to_drug_searchterm[matched_snomed]:\n                    if drug.upper() == drug_name.upper():\n                        search_term = st\n                        break\n                # If no drug-specific match, use any match\n                if search_term is None:\n                    search_term = snomed_to_drug_searchterm[matched_snomed][0][1]\n\n                # Only update if we don't have a match for this UPID yet\n                if upid not in upid_to_match:\n                    upid_to_match[upid] = (search_term, \"DIAGNOSIS\")\n            else:\n                # Shouldn't happen but fallback just in case\n                if upid not in upid_to_match:\n                    upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n        else:\n            # No GP match - use fallback\n            if upid not in upid_to_match:\n                upid_to_match[upid] = (directory + \" (no GP dx)\", \"FALLBACK\")\n\n    # Build result DataFrame\n    for upid, (indication_group, source) in upid_to_match.items():\n        results_list.append({\n            'UPID': upid,\n            'Indication_Group': indication_group,\n            'Source': source,\n        })\n\n    result_df = pd.DataFrame(results_list)\n\n    # Log statistics\n    diagnosis_count = len([s for s in result_df['Source'] if s == \"DIAGNOSIS\"])\n    fallback_count = len([s for s in result_df['Source'] if s == \"FALLBACK\"])\n    total = len(result_df)\n\n    logger.info(f\"Indication lookup complete:\")\n    logger.info(f\"  Total unique patients: {total}\")\n    logger.info(f\"  DIAGNOSIS matches: {diagnosis_count} ({100*diagnosis_count/total:.1f}%)\")\n    logger.info(f\"  FALLBACK (no GP match): {fallback_count} ({100*fallback_count/total:.1f}%)\")\n\n    return result_df\n\n\n# === Drug-to-indication mapping from DimSearchTerm.csv ===\n\n\n# Merge related Search_Terms into canonical names.\n# Asthma variants are clinically the same condition at different severity levels.\n# Urticaria is a separate condition ÔÇö do NOT merge with asthma.\nSEARCH_TERM_MERGE_MAP: dict[str, str] = {\n    \"allergic asthma\": \"asthma\",\n    \"severe persistent allergic asthma\": \"asthma\",\n}\n\n\ndef load_drug_indication_mapping(\n    csv_path: Optional[str] = None,\n) -> tuple[dict[str, list[str]], dict[str, list[str]]]:\n    \"\"\"\n    Load the drug-to-Search_Term mapping from DimSearchTerm.csv.\n\n    Builds two lookup dicts:\n    - fragment_to_search_terms: drug fragment (UPPERCASE) -> list of Search_Terms containing it\n    - search_term_to_fragments: search_term -> list of drug fragments (UPPERCASE)\n\n    DimSearchTerm.csv has columns: Search_Term, CleanedDrugName, PrimaryDirectorate\n    CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\").\n\n    Note: A Search_Term can appear multiple times with different PrimaryDirectorates\n    (e.g., \"diabetes\" appears under both DIABETIC MEDICINE and OPHTHALMOLOGY).\n    Drug fragments from all rows for the same Search_Term are combined.\n\n    Asthma-related Search_Terms (\"allergic asthma\", \"severe persistent allergic asthma\")\n    are merged into \"asthma\" to match the CLUSTER_MAPPING_SQL normalization.\n    \"urticaria\" stays separate.\n\n    Args:\n        csv_path: Path to DimSearchTerm.csv. Defaults to data/DimSearchTerm.csv.\n\n    Returns:\n        Tuple of (fragment_to_search_terms, search_term_to_fragments)\n    \"\"\"\n    if csv_path is None:\n        csv_path = str(Path(__file__).parent.parent / \"data\" / \"DimSearchTerm.csv\")\n\n    fragment_to_search_terms: dict[str, list[str]] = {}\n    search_term_to_fragments: dict[str, list[str]] = {}\n\n    try:\n        with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                search_term = row.get(\"Search_Term\", \"\").strip()\n                drug_names_raw = row.get(\"CleanedDrugName\", \"\").strip()\n\n                # Normalize asthma variants to canonical \"asthma\"\n                search_term = SEARCH_TERM_MERGE_MAP.get(search_term, search_term)\n\n                if not search_term or not drug_names_raw:\n                    continue\n\n                fragments = [frag.strip().upper() for frag in drug_names_raw.split(\"|\") if frag.strip()]\n\n                # Build search_term -> fragments (accumulate for duplicate Search_Terms)\n                if search_term not in search_term_to_fragments:\n                    search_term_to_fragments[search_term] = []\n                for frag in fragments:\n                    if frag not in search_term_to_fragments[search_term]:\n                        search_term_to_fragments[search_term].append(frag)\n\n                # Build fragment -> search_terms\n                for frag in fragments:\n                    if frag not in fragment_to_search_terms:\n                        fragment_to_search_terms[frag] = []\n                    if search_term not in fragment_to_search_terms[frag]:\n                        fragment_to_search_terms[frag].append(search_term)\n\n        logger.info(\n            f\"Loaded drug-indication mapping: {len(search_term_to_fragments)} Search_Terms, \"\n            f\"{len(fragment_to_search_terms)} drug fragments\"\n        )\n\n    except FileNotFoundError:\n        logger.error(f\"DimSearchTerm.csv not found at {csv_path}\")\n    except Exception as e:\n        logger.error(f\"Error loading DimSearchTerm.csv: {e}\")\n\n    return fragment_to_search_terms, search_term_to_fragments\n\n\ndef get_search_terms_for_drug(\n    drug_name: str,\n    search_term_to_fragments: dict[str, list[str]],\n) -> list[str]:\n    \"\"\"\n    Get all Search_Terms that list a given drug using substring matching.\n\n    Checks if any drug fragment from DimSearchTerm is a SUBSTRING of the given\n    drug name (case-insensitive). This handles both exact matches (ADALIMUMAB)\n    and partial fragments (PEGYLATED, INHALED).\n\n    Args:\n        drug_name: HCD drug name (e.g., \"ADALIMUMAB 40MG\", \"PEGYLATED LIPOSOMAL DOXORUBICIN\")\n        search_term_to_fragments: Mapping of search_term -> list of drug fragments\n\n    Returns:\n        List of Search_Terms whose drug fragments match the drug name\n    \"\"\"\n    drug_name_upper = drug_name.upper()\n    matched_terms: list[str] = []\n\n    for search_term, fragments in search_term_to_fragments.items():\n        for frag in fragments:\n            if frag in drug_name_upper:\n                matched_terms.append(search_term)\n                break  # One matching fragment is enough for this Search_Term\n\n    return matched_terms\n\n\ndef assign_drug_indications(\n    df: \"pd.DataFrame\",\n    gp_matches_df: \"pd.DataFrame\",\n    search_term_to_fragments: dict[str, list[str]],\n) -> \"tuple[pd.DataFrame, pd.DataFrame]\":\n    \"\"\"\n    Assign drug-aware indications by cross-referencing GP diagnoses with drug mappings.\n\n    For each UPID + Drug Name pair in the HCD data:\n    1. Look up patient's GP-matched Search_Terms (via PseudoNHSNoLinked ÔåÆ gp_matches_df)\n    2. Look up which Search_Terms list this drug (via search_term_to_fragments)\n    3. Intersect = valid indications for this drug-patient pair\n    4. If 1 match: use it\n    5. If multiple: pick highest code_frequency (most GP coding = most likely indication)\n    6. If tied frequency: alphabetical Search_Term for determinism\n    7. If 0 matches: fallback to \"{Directory} (no GP dx)\"\n\n    Modifies UPID to include indication: \"{UPID}|{search_term}\" or \"{UPID}|{Directory} (no GP dx)\".\n    This makes drugs under different indications appear as separate pathways.\n\n    Args:\n        df: HCD DataFrame with columns: UPID, Drug Name, PseudoNHSNoLinked, Directory\n        gp_matches_df: GP matches from get_patient_indication_groups() with columns:\n            PatientPseudonym, Search_Term, code_frequency\n            (multiple rows per patient, one per matched Search_Term)\n        search_term_to_fragments: From load_drug_indication_mapping()[1].\n            Maps search_term -> list of drug fragments (UPPERCASE).\n\n    Returns:\n        Tuple of (modified_df, indication_df):\n        - modified_df: Copy of df with UPID replaced by \"{UPID}|{indication}\"\n        - indication_df: DataFrame indexed by modified UPID with 'Directory' column\n            containing the Search_Term (or fallback label) for pathway hierarchy\n    \"\"\"\n    import pandas as pd\n\n    modified_df = df.copy()\n\n    # Build GP match lookup: PseudoNHSNoLinked -> {Search_Term: code_frequency}\n    gp_lookup: dict[str, dict[str, int]] = {}\n    if not gp_matches_df.empty:\n        for _, row in gp_matches_df.iterrows():\n            pseudo = row['PatientPseudonym']\n            term = row['Search_Term']\n            freq = int(row.get('code_frequency', 0))\n            if pseudo not in gp_lookup:\n                gp_lookup[pseudo] = {}\n            gp_lookup[pseudo][term] = freq\n\n    logger.info(f\"GP lookup built: {len(gp_lookup)} patients with GP matches\")\n\n    # Cache drug -> Search_Terms lookups to avoid recomputing per row\n    drug_search_terms_cache: dict[str, list[str]] = {}\n\n    def get_drug_terms(drug_name: str) -> list[str]:\n        if drug_name not in drug_search_terms_cache:\n            drug_search_terms_cache[drug_name] = get_search_terms_for_drug(\n                drug_name, search_term_to_fragments\n            )\n        return drug_search_terms_cache[drug_name]\n\n    # Process each row: determine indication for each UPID + Drug Name\n    # We work at the (UPID, Drug Name) pair level, then apply to all rows\n    # Key: (UPID, Drug Name) -> (matched_search_term_or_fallback, is_diagnosis)\n    pair_indication_cache: dict[tuple[str, str], tuple[str, bool]] = {}\n\n    # Get unique (UPID, Drug Name, PseudoNHSNoLinked, Directory) combos\n    required_cols = ['UPID', 'Drug Name', 'PseudoNHSNoLinked', 'Directory']\n    unique_pairs = modified_df[required_cols].drop_duplicates(\n        subset=['UPID', 'Drug Name']\n    )\n\n    match_count = 0\n    fallback_count = 0\n    tiebreak_count = 0\n\n    for _, pair_row in unique_pairs.iterrows():\n        upid = pair_row['UPID']\n        drug_name = pair_row['Drug Name']\n        pseudo = pair_row['PseudoNHSNoLinked']\n        directory = pair_row['Directory']\n\n        # Get Search_Terms this drug maps to\n        drug_terms = get_drug_terms(drug_name)\n\n        # Get patient's GP-matched Search_Terms\n        patient_gp_terms = gp_lookup.get(pseudo, {}) if pd.notna(pseudo) else {}\n\n        # Intersect: Search_Terms that list this drug AND patient has GP dx for\n        valid_indications = {\n            term: patient_gp_terms[term]\n            for term in drug_terms\n            if term in patient_gp_terms\n        }\n\n        if len(valid_indications) == 1:\n            matched_term = next(iter(valid_indications))\n            pair_indication_cache[(upid, drug_name)] = (matched_term, True)\n            match_count += 1\n\n        elif len(valid_indications) > 1:\n            # Tiebreaker: highest code_frequency, then alphabetical\n            sorted_terms = sorted(\n                valid_indications.items(),\n                key=lambda x: (-x[1], x[0]),\n            )\n            matched_term = sorted_terms[0][0]\n            pair_indication_cache[(upid, drug_name)] = (matched_term, True)\n            match_count += 1\n            tiebreak_count += 1\n\n        else:\n            # No intersection: fallback to directory\n            if pd.notna(directory):\n                fallback_label = f\"{directory} (no GP dx)\"\n            else:\n                fallback_label = \"UNKNOWN (no GP dx)\"\n            pair_indication_cache[(upid, drug_name)] = (fallback_label, False)\n            fallback_count += 1\n\n    total_pairs = len(unique_pairs)\n    logger.info(f\"Drug-indication matching complete:\")\n    logger.info(f\"  Total UPID-Drug pairs: {total_pairs}\")\n    logger.info(f\"  Matched (GP dx + drug mapping): {match_count} ({100*match_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Matched: 0\")\n    logger.info(f\"  Tiebreaker used: {tiebreak_count}\")\n    logger.info(f\"  Fallback (no match): {fallback_count} ({100*fallback_count/total_pairs:.1f}%)\" if total_pairs > 0 else f\"  Fallback: 0\")\n\n    # Apply modified UPIDs to all rows\n    # Build vectorized lookup: original UPID + Drug Name -> modified UPID\n    def build_modified_upid(row: \"pd.Series\") -> str:\n        upid = row['UPID']\n        drug_name = row['Drug Name']\n        key = (upid, drug_name)\n        if key in pair_indication_cache:\n            indication, _ = pair_indication_cache[key]\n            return f\"{upid}|{indication}\"\n        # Shouldn't happen, but fallback\n        return f\"{upid}|UNKNOWN (no GP dx)\"\n\n    modified_df['UPID'] = modified_df.apply(build_modified_upid, axis=1)\n\n    # Build indication_df: modified UPID -> Search_Term/fallback label\n    # This maps each unique modified UPID to its indication for the pathway hierarchy\n    indication_records: list[dict[str, str]] = []\n    seen_upids: set[str] = set()\n\n    for (original_upid, drug_name), (indication, is_diagnosis) in pair_indication_cache.items():\n        modified_upid = f\"{original_upid}|{indication}\"\n        if modified_upid not in seen_upids:\n            indication_records.append({\n                'UPID': modified_upid,\n                'Directory': indication,\n            })\n            seen_upids.add(modified_upid)\n\n    indication_df = pd.DataFrame(indication_records)\n    if not indication_df.empty:\n        indication_df = indication_df.set_index('UPID')\n\n    logger.info(f\"  Unique modified UPIDs: {len(indication_df)}\")\n\n    # Log top indications\n    if not indication_df.empty:\n        top_terms = indication_df['Directory'].value_counts().head(5)\n        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n\n    return modified_df, indication_df\n\n\n# === NEW APPROACH: Query Snowflake directly using cluster CTE ===\n\n# The cluster query mapping (embedded from snomed_indication_mapping_query.sql)\n# This maps Search_Term -> Cluster_ID for ~148 clinical conditions\nCLUSTER_MAPPING_SQL = \"\"\"\nWITH SearchTermClusters AS (\n    SELECT Search_Term, Cluster_ID FROM (VALUES\n        ('acute lymphoblastic leukaemia', 'HAEMCANMORPH_COD'),\n        ('acute myeloid leukaemia', 'C19HAEMCAN_COD'),\n        ('acute promyelocytic leukaemia', 'HAEMCANMORPH_COD'),\n        ('asthma', 'AST_COD'),\n        ('allergic rhinitis', 'MILDINTAST_COD'),\n        ('alzheimer''s disease', 'DEMALZ_COD'),\n        ('amyloidosis', 'AMYLOID_COD'),\n        ('anaemia', 'eFI2_AnaemiaTimeSensitive'),\n        ('anaplastic large cell lymphoma', 'C19HAEMCAN_COD'),\n        ('apixaban', 'DOACCON_COD'),\n        ('aplastic anaemia', 'eFI2_AnaemiaEver'),\n        ('arthritis', 'eFI2_InflammatoryArthritis'),\n        ('asthma', 'eFI2_Asthma'),\n        ('atopic dermatitis', 'ATOPDERM_COD'),\n        ('atrial fibrillation', 'eFI2_AtrialFibrillation'),\n        ('attention deficit hyperactivity disorder', 'ADHD_COD'),\n        ('bipolar disorder', 'MH_COD'),\n        ('bladder', 'eFI2_UrinaryIncontinence'),\n        ('breast cancer', 'BRCANSCR_COD'),\n        ('cardiomyopathy', 'eFI2_HarmfulDrinking'),\n        ('cardiovascular disease', 'CVDRISKASS_COD'),\n        ('cervical cancer', 'CSDEC_COD'),\n        ('cholangiocarcinoma', 'eFI2_Cancer'),\n        ('chronic kidney disease', 'CKD_COD'),\n        ('chronic liver disease', 'eFI2_LiverProblems'),\n        ('chronic lymphocytic leukaemia', 'EPPHAEMCAN_COD'),\n        ('chronic myeloid leukaemia', 'EPPHAEMCAN_COD'),\n        ('chronic obstructive pulmonary disease', 'eFI2_COPD'),\n        ('colon cancer', 'eFI2_Cancer'),\n        ('colorectal cancer', 'GICANREF_COD'),\n        ('constipation', 'CHRONCONSTIP_COD'),\n        ('covid-19', 'POSSPOSTCOVID_COD'),\n        ('crohn''s disease', 'eFI2_InflammatoryBowelDisease'),\n        ('cutaneous t-cell lymphoma', 'C19HAEMCAN_COD'),\n        ('cystic fibrosis', 'CUST_ICB_CYSTIC_FIBROSIS'),\n        ('deep vein thrombosis', 'VTE_COD'),\n        ('depression', 'eFI2_Depression'),\n        ('diabetes', 'eFI2_DiabetesEver'),\n        ('diabetic retinopathy', 'DRSELIGIBILITY_COD'),\n        ('diffuse large b-cell lymphoma', 'C19HAEMCAN_COD'),\n        ('dravet syndrome', 'EPIL_COD'),\n        ('drug misuse', 'ILLSUBINT_COD'),\n        ('dyspepsia', 'eFI2_AbdominalPain'),\n        ('epilepsy', 'eFI2_Seizures'),\n        ('fallopian tube', 'STERIL_COD'),\n        ('follicular lymphoma', 'C19HAEMCAN_COD'),\n        ('gastric cancer', 'eFI2_Cancer'),\n        ('giant cell arteritis', 'GCA_COD'),\n        ('glioma', 'NHAEMCANMORPH_COD'),\n        ('gout', 'eFI2_InflammatoryArthritis'),\n        ('graft versus host disease', 'GVHD_COD'),\n        ('granulomatosis with polyangiitis', 'WEGENERVASC_COD'),\n        ('growth hormone deficiency', 'HYPOPITUITARY_COD'),\n        ('hand eczema', 'ECZEMA_COD'),\n        ('heart failure', 'eFI2_HeartFailure'),\n        ('hepatitis b', 'HEPBCVAC_COD'),\n        ('hepatocellular carcinoma', 'eFI2_Cancer'),\n        ('hiv', 'PREFLANG_COD'),\n        ('hodgkin lymphoma', 'HAEMCANMORPH_COD'),\n        ('hormone receptor', 'eFI2_ThyroidProblems'),\n        ('hypercholesterolaemia', 'CLASSFH_COD'),\n        ('immune thrombocytopenia', 'ITP_COD'),\n        ('influenza', 'FLUINVITE_COD'),\n        ('insomnia', 'eFI2_SleepProblems'),\n        ('irritable bowel syndrome', 'IBS_COD'),\n        ('ischaemic stroke', 'OSTR_COD'),\n        ('juvenile idiopathic arthritis', 'RARTHAD_COD'),\n        ('kidney transplant', 'RENALTRANSP_COD'),\n        ('leukaemia', 'eFI2_Cancer'),\n        ('lung cancer', 'FTCANREF_COD'),\n        ('lymphoma', 'C19HAEMCAN_COD'),\n        ('macular degeneration', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('macular oedema', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('major depressive episodes', 'eFI2_Depression'),\n        ('malignant melanoma', 'eFI2_Cancer'),\n        ('malignant pleural mesothelioma', 'LUNGCAN_COD'),\n        ('manic episode', 'MH_COD'),\n        ('mantle cell lymphoma', 'HAEMCANMORPH_COD'),\n        ('melanoma', 'eFI2_Cancer'),\n        ('merkel cell carcinoma', 'C19CAN_COD'),\n        ('migraine', 'eFI2_Headache'),\n        ('motor neurone disease', 'MND_COD'),\n        ('multiple myeloma', 'C19HAEMCAN_COD'),\n        ('multiple sclerosis', 'MS_COD'),\n        ('myelodysplastic', 'eFI2_AnaemiaEver'),\n        ('myelofibrosis', 'MDS_COD'),\n        ('myocardial infarction', 'eFI2_IschaemicHeartDisease'),\n        ('myotonia', 'CNDATRISK2_COD'),\n        ('narcolepsy', 'LD_COD'),\n        ('neuroendocrine tumour', 'LUNGCAN_COD'),\n        ('non-small cell lung cancer', 'LUNGCAN_COD'),\n        ('non-small-cell lung cancer', 'FTCANREF_COD'),\n        ('obesity', 'BMI30_COD'),\n        ('osteoarthritis', 'CUST_ICB_OSTEOARTHRITIS'),\n        ('osteoporosis', 'eFI2_Osteoporosis'),\n        ('osteosarcoma', 'NHAEMCANMORPH_COD'),\n        ('ovarian cancer', 'C19CAN_COD'),\n        ('peripheral arterial disease', 'PADEXC_COD'),\n        ('plaque psoriasis', 'PSORIASIS_COD'),\n        ('polycystic kidney disease', 'EPPCONGMALF_COD'),\n        ('polycythaemia vera', 'C19HAEMCAN_COD'),\n        ('pregnancy', 'C19PREG_COD'),\n        ('primary biliary cholangitis', 'eFI2_LiverProblems'),\n        ('primary hypercholesterolaemia', 'FNFHYP_COD'),\n        ('prostate cancer', 'EPPSOLIDCAN_COD'),\n        ('psoriasis', 'PSORIASIS_COD'),\n        ('psoriatic arthritis', 'RARTHAD_COD'),\n        ('pulmonary embolism', 'eFI2_RespiratoryDiseaseTimeSensitive'),\n        ('pulmonary fibrosis', 'ILD_COD'),\n        ('relapsing multiple sclerosis', 'MS_COD'),\n        ('renal cell carcinoma', 'C19CAN_COD'),\n        ('renal transplantation', 'RENALTRANSP_COD'),\n        ('retinal vein occlusion', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('rheumatoid arthritis', 'eFI2_InflammatoryArthritis'),\n        ('rivaroxaban', 'DOACCON_COD'),\n        ('schizophrenia', 'MH_COD'),\n        ('seizures', 'LSZFREQ_COD'),\n        ('sepsis', 'C19ACTIVITY_COD'),\n        ('asthma', 'SEVAST_COD'),\n        ('sickle cell disease', 'SICKLE_COD'),\n        ('sleep apnoea', 'CUST_ICB_NON_SEVERE_LDA'),\n        ('smoking cessation', 'SMOKINGINT_COD'),\n        ('soft tissue sarcoma', 'NHAEMCANMORPH_COD'),\n        ('spinal muscular atrophy', 'MND_COD'),\n        ('squamous cell', 'C19CAN_COD'),\n        ('squamous cell carcinoma', 'C19CAN_COD'),\n        ('stem cell transplant', 'ALLOTRANSP_COD'),\n        ('stroke', 'eFI2_Stroke'),\n        ('systemic lupus erythematosus', 'SLUPUS_COD'),\n        ('systemic mastocytosis', 'HAEMCANMORPH_COD'),\n        ('thrombocytopenic purpura', 'TTP_COD'),\n        ('thrombotic thrombocytopenic purpura', 'TTP_COD'),\n        ('thyroid cancer', 'C19CAN_COD'),\n        ('tophaceous gout', 'CUST_ICB_OSTEOARTHRITIS'),\n        ('transitional cell carcinoma', 'C19CAN_COD'),\n        ('type 1 diabetes', 'DMTYPE1_COD'),\n        ('type 2 diabetes', 'DMTYPE2_COD'),\n        ('ulcerative colitis', 'eFI2_InflammatoryBowelDisease'),\n        ('urothelial carcinoma', 'NHAEMCANMORPH_COD'),\n        ('urticaria', 'XSAL_COD'),\n        ('uveitis', 'CUST_ICB_VISUAL_IMPAIRMENT'),\n        ('vascular disease', 'CVDINVITE_COD'),\n        ('vasculitis', 'CRYOGLOBVASC_COD')\n    ) AS t(Search_Term, Cluster_ID)\n),\n\nClusterCodes AS (\n    SELECT\n        stc.Search_Term,\n        c.\"SNOMEDCode\",\n        c.\"SNOMEDDescription\"\n    FROM SearchTermClusters stc\n    JOIN DATA_HUB.PHM.\"ClinicalCodingClusterSnomedCodes\" c\n        ON stc.Cluster_ID = c.\"Cluster_ID\"\n    WHERE c.\"SNOMEDCode\" IS NOT NULL\n),\n\nExplicitCodes AS (\n    SELECT Search_Term, SNOMEDCode, SNOMEDDescription FROM (VALUES\n        ('acute coronary syndrome', '837091000000100', 'Manual mapping'),\n        ('ankylosing spondylitis', '162930007', 'Manual mapping'),\n        ('ankylosing spondylitis', '239805001', 'Manual mapping'),\n        ('ankylosing spondylitis', '239810002', 'Manual mapping'),\n        ('ankylosing spondylitis', '239811003', 'Manual mapping'),\n        ('ankylosing spondylitis', '394990003', 'Manual mapping'),\n        ('ankylosing spondylitis', '429712009', 'Manual mapping'),\n        ('ankylosing spondylitis', '441562009', 'Manual mapping'),\n        ('ankylosing spondylitis', '441680005', 'Manual mapping'),\n        ('ankylosing spondylitis', '441930001', 'Manual mapping'),\n        ('axial spondyloarthritis', '723116002', 'Manual mapping'),\n        ('choroidal neovascularisation', '380621000000102', 'Manual mapping'),\n        ('choroidal neovascularisation', '733124000', 'Manual mapping')\n    ) AS t(Search_Term, SNOMEDCode, SNOMEDDescription)\n),\n\nAllIndicationCodes AS (\n    SELECT Search_Term, \"SNOMEDCode\" AS SNOMEDCode, \"SNOMEDDescription\" AS SNOMEDDescription\n    FROM ClusterCodes\n    UNION ALL\n    SELECT Search_Term, SNOMEDCode, SNOMEDDescription\n    FROM ExplicitCodes\n)\n\"\"\"\n\n\ndef get_patient_indication_groups(\n    patient_pseudonyms: list[str],\n    connector: Optional[SnowflakeConnector] = None,\n    batch_size: int = 500,\n    earliest_hcd_date: Optional[str] = None,\n) -> \"pd.DataFrame\":\n    \"\"\"\n    Batch lookup GP diagnosis-based indication groups using Snowflake cluster query.\n\n    Returns ALL matching Search_Terms per patient with code_frequency (count of\n    matching SNOMED codes). This enables drug-aware indication matching where\n    each drug is cross-referenced against the patient's GP diagnoses.\n\n    The query:\n    1. Uses the cluster mapping CTE to get all Search_Term -> SNOMED code mappings\n    2. Joins with PrimaryCareClinicalCoding to find patients with matching codes\n    3. Groups by patient + Search_Term and counts matching codes (code_frequency)\n    4. Optionally restricts to GP codes from earliest_hcd_date onwards\n\n    Args:\n        patient_pseudonyms: List of PseudoNHSNoLinked values (matches PatientPseudonym in GP records)\n        connector: Optional SnowflakeConnector (defaults to singleton)\n        batch_size: Number of patients per Snowflake query batch (default 500)\n        earliest_hcd_date: Optional ISO date string (YYYY-MM-DD). If provided, only\n            counts GP codes from this date onwards. Should be MIN(Intervention Date)\n            from the HCD DataFrame to restrict to the HCD data window.\n\n    Returns:\n        DataFrame with columns:\n        - PatientPseudonym: The patient identifier (PseudoNHSNoLinked value)\n        - Search_Term: The matched indication (e.g., \"rheumatoid arthritis\")\n        - code_frequency: Count of matching SNOMED codes for this Search_Term\n\n        Multiple rows per patient (one per matched Search_Term).\n        Patients not found in results have no matching GP diagnosis.\n    \"\"\"\n    import pandas as pd\n\n    logger.info(f\"Starting Snowflake-direct indication lookup for {len(patient_pseudonyms)} patients...\")\n    if earliest_hcd_date:\n        logger.info(f\"  Restricting GP codes to >= {earliest_hcd_date}\")\n\n    # Handle edge case: empty patient list\n    if not patient_pseudonyms:\n        logger.warning(\"Empty patient list provided\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    # Check Snowflake availability\n    if not SNOWFLAKE_AVAILABLE:\n        logger.error(\"Snowflake connector not available - cannot lookup GP records\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    if not is_snowflake_configured():\n        logger.error(\"Snowflake not configured - cannot lookup GP records\")\n        return pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\n\n    if connector is None:\n        connector = get_connector()\n\n    # Results list to collect all matches\n    all_results: list[dict] = []\n\n    # Process patients in batches\n    total_patients = len(patient_pseudonyms)\n    for batch_start in range(0, total_patients, batch_size):\n        batch_end = min(batch_start + batch_size, total_patients)\n        batch_pseudonyms = patient_pseudonyms[batch_start:batch_end]\n        batch_num = batch_start // batch_size + 1\n        total_batches = (total_patients + batch_size - 1) // batch_size\n\n        logger.info(f\"Batch {batch_num}/{total_batches}: patients {batch_start + 1} to {batch_end}\")\n\n        # Build patient IN clause placeholders\n        patient_placeholders = \", \".join([\"%s\"] * len(batch_pseudonyms))\n\n        # Build WHERE clause with optional date filter\n        date_filter = \"\"\n        if earliest_hcd_date:\n            date_filter = f\"\\n    AND pc.\\\"EventDateTime\\\" >= %s\"\n\n        # Build the full query with cluster CTE\n        # Returns ALL matching Search_Terms per patient with code_frequency\n        # code_frequency = COUNT of matching SNOMED codes per Search_Term per patient\n        query = f\"\"\"\n{CLUSTER_MAPPING_SQL}\nSELECT\n    pc.\"PatientPseudonym\" AS \"PatientPseudonym\",\n    aic.Search_Term AS \"Search_Term\",\n    COUNT(*) AS \"code_frequency\"\nFROM DATA_HUB.PHM.\"PrimaryCareClinicalCoding\" pc\nINNER JOIN AllIndicationCodes aic\n    ON pc.\"SNOMEDCode\" = aic.SNOMEDCode\nWHERE pc.\"PatientPseudonym\" IN ({patient_placeholders}){date_filter}\nGROUP BY pc.\"PatientPseudonym\", aic.Search_Term\n\"\"\"\n\n        # Build params: patient pseudonyms + optional date\n        params = list(batch_pseudonyms)\n        if earliest_hcd_date:\n            params.append(earliest_hcd_date)\n\n        try:\n            results = connector.execute_dict(query, tuple(params))\n\n            for row in results:\n                all_results.append({\n                    'PatientPseudonym': row.get('PatientPseudonym'),\n                    'Search_Term': row.get('Search_Term'),\n                    'code_frequency': row.get('code_frequency', 0),\n                })\n\n            logger.debug(f\"Batch {batch_num}: found {len(results)} patient-indication matches\")\n\n        except Exception as e:\n            logger.error(f\"Error querying GP records for batch {batch_num}: {e}\")\n            # Continue with other batches - partial results are better than none\n\n    # Build result DataFrame\n    result_df = pd.DataFrame(all_results)\n\n    # Log summary statistics\n    if len(result_df) > 0:\n        unique_patients = result_df['PatientPseudonym'].nunique()\n        total_rows = len(result_df)\n        match_rate = 100 * unique_patients / total_patients\n        unique_terms = result_df['Search_Term'].nunique()\n        avg_indications = total_rows / unique_patients if unique_patients > 0 else 0\n        logger.info(f\"Indication lookup complete:\")\n        logger.info(f\"  Total patients queried: {total_patients}\")\n        logger.info(f\"  Patients with GP match: {unique_patients} ({match_rate:.1f}%)\")\n        logger.info(f\"  Total patient-indication rows: {total_rows} (avg {avg_indications:.1f} per patient)\")\n        logger.info(f\"  Unique Search_Terms found: {unique_terms}\")\n\n        # Log top Search_Terms\n        top_terms = result_df['Search_Term'].value_counts().head(5)\n        logger.info(f\"  Top 5 indications: {dict(top_terms)}\")\n    else:\n        logger.info(f\"Indication lookup complete: 0 matches from {total_patients} patients\")\n\n    return result_df\n\n\n# Export public API\n__all__ = [\n    # Dataclasses\n    \"ClusterSnomedCodes\",\n    \"IndicationValidationResult\",\n    \"DrugIndicationMatchRate\",\n    \"DrugSnomedMapping\",\n    \"DirectSnomedMatchResult\",\n    \"DirectorateAssignment\",\n    # Cluster-based lookup functions (existing)\n    \"get_drug_clusters\",\n    \"get_drug_cluster_ids\",\n    \"get_cluster_snomed_codes\",\n    \"patient_has_indication\",\n    \"validate_indication\",\n    \"get_indication_match_rate\",\n    \"batch_validate_indications\",\n    \"get_available_clusters\",\n    # Direct SNOMED lookup functions (new)\n    \"get_drug_snomed_codes\",\n    \"patient_has_indication_direct\",\n    # Diagnosis-based directorate assignment\n    \"get_directorate_from_diagnosis\",\n    # Batch lookup for indication groups\n    \"batch_lookup_indication_groups\",\n    # Drug-indication mapping from DimSearchTerm.csv\n    \"SEARCH_TERM_MERGE_MAP\",\n    \"load_drug_indication_mapping\",\n    \"get_search_terms_for_drug\",\n    # Snowflake-direct indication lookup (new approach)\n    \"get_patient_indication_groups\",\n    \"CLUSTER_MAPPING_SQL\",\n]\n","structuredPatch":[{"oldStart":1736,"oldLines":6,"newStart":1736,"newLines":8,"lines":["     \"SEARCH_TERM_MERGE_MAP\",","     \"load_drug_indication_mapping\",","     \"get_search_terms_for_drug\",","+    # Drug-aware indication assignment","+    \"assign_drug_indications\",","     # Snowflake-direct indication lookup (new approach)","     \"get_patient_indication_groups\",","     \"CLUSTER_MAPPING_SQL\","]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01NoAiBAvc87vnLf4cK5sRRR","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me validate the code. Running py_compile and import checks."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":331,"cache_read_input_tokens":122476,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":331},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"1a766b22-a455-4184-8799-076401f905be"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01NoAiBAvc87vnLf4cK5sRRR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VGRE5FNmB4KU4uVApo3Fw8","name":"TodoWrite","input":{"todos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"in_progress","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"pending","activeForm":"Committing and updating progress"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":331,"cache_read_input_tokens":122476,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":331},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"250e0d6a-df8d-4a12-a633-99431915b14d"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VGRE5FNmB4KU4uVApo3Fw8","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"038a97a4-7513-4aee-8224-c15356c9f4f6","tool_use_result":{"oldTodos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"in_progress","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"pending","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"pending","activeForm":"Committing and updating progress"}],"newTodos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"in_progress","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"pending","activeForm":"Committing and updating progress"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01JUhtLepXTmi7sRQtuCQwPw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012bDUoKJyAKgfJ9Hwx4Lzbv","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && python -m py_compile data_processing/diagnosis_lookup.py","description":"Compile check diagnosis_lookup.py"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":269,"cache_read_input_tokens":122807,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":269},"output_tokens":26,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"dab072ac-82e3-41fa-8460-b742a0289145"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012bDUoKJyAKgfJ9Hwx4Lzbv","type":"tool_result","content":"","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"47090336-99be-4ddc-aed3-1eff74614322","tool_use_result":{"stdout":"","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01PhwWmUUeXKRh33rupxgqyX","type":"message","role":"assistant","content":[{"type":"text","text":"Syntax check passed. Now import check."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":138,"cache_read_input_tokens":123076,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":138},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"6b427c6b-439f-495e-835e-c1b39505ecaa"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01PhwWmUUeXKRh33rupxgqyX","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013usYF9Rcw6FWBnRYg1NFWY","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && python -c \"from data_processing.diagnosis_lookup import assign_drug_indications, load_drug_indication_mapping, get_search_terms_for_drug; print('Import OK')\"","description":"Import check for new and existing functions"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":138,"cache_read_input_tokens":123076,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":138},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"9072947e-2c6e-464f-be79-206925117516"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013usYF9Rcw6FWBnRYg1NFWY","type":"tool_result","content":"Import OK","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"4e2d21ff-ff52-4e57-90b6-72beb13da1a6","tool_use_result":{"stdout":"Import OK\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_019QtsYDMQqoCF4tmWQZ8h93","type":"message","role":"assistant","content":[{"type":"text","text":"Import check passed. Now let me test with mock data to verify the function works correctly ÔÇö this covers Task 2.2 (tiebreaker logic) as well."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":164,"cache_read_input_tokens":123214,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":164},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"0fbc5583-866f-4179-b7ff-31fa2dba022a"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_019QtsYDMQqoCF4tmWQZ8h93","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0134izEQP46fRzp1zbL5Vi1q","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && python -c \"\nimport pandas as pd\nfrom data_processing.diagnosis_lookup import assign_drug_indications, load_drug_indication_mapping, get_search_terms_for_drug\n\n# Load real drug mapping\nfrag_to_st, st_to_frags = load_drug_indication_mapping()\nprint(f'Loaded: {len(st_to_frags)} Search_Terms, {len(frag_to_st)} fragments')\n\n# Create mock HCD data\ndf = pd.DataFrame({\n    'UPID': ['RMV111', 'RMV111', 'RMV222', 'RMV333', 'RMV333'],\n    'Drug Name': ['ADALIMUMAB 40MG', 'OMALIZUMAB', 'ADALIMUMAB', 'TOCILIZUMAB', 'INFLIXIMAB'],\n    'PseudoNHSNoLinked': ['NHS111', 'NHS111', 'NHS222', 'NHS333', 'NHS333'],\n    'Directory': ['RHEUMATOLOGY', 'THORACIC MEDICINE', 'RHEUMATOLOGY', 'RHEUMATOLOGY', 'GASTRO'],\n})\n\n# Create mock GP matches (multiple rows per patient with code_frequency)\ngp_matches_df = pd.DataFrame({\n    'PatientPseudonym': ['NHS111', 'NHS111', 'NHS222', 'NHS222'],\n    'Search_Term': ['rheumatoid arthritis', 'asthma', 'rheumatoid arthritis', 'crohn\\\\'s disease'],\n    'code_frequency': [47, 12, 3, 2],\n})\n\nprint()\nprint('--- Test 1: Basic drug-indication matching ---')\nmodified_df, indication_df = assign_drug_indications(df, gp_matches_df, st_to_frags)\n\nprint()\nprint('Modified UPIDs:')\nfor _, row in modified_df.iterrows():\n    print(f'  {row[\\\"Drug Name\\\"]:25s} -> {row[\\\"UPID\\\"]}')\n\nprint()\nprint('Indication DataFrame:')\nprint(indication_df)\n\n# Verify expected results\nprint()\nprint('--- Verification ---')\n# Patient RMV111 on ADALIMUMAB + GP dx for RA + asthma: ADALIMUMAB should match RA\nada_upid = modified_df[modified_df['Drug Name'] == 'ADALIMUMAB 40MG']['UPID'].values[0]\nassert 'rheumatoid arthritis' in ada_upid, f'ADALIMUMAB should match RA, got: {ada_upid}'\nprint(f'OK: ADALIMUMAB matched to RA: {ada_upid}')\n\n# Patient RMV111 on OMALIZUMAB + GP dx for asthma: OMALIZUMAB should match asthma  \noma_upid = modified_df[modified_df['Drug Name'] == 'OMALIZUMAB']['UPID'].values[0]\nassert 'asthma' in oma_upid, f'OMALIZUMAB should match asthma, got: {oma_upid}'\nprint(f'OK: OMALIZUMAB matched to asthma: {oma_upid}')\n\n# Patient RMV222 on ADALIMUMAB + GP dx for RA (47) + crohn's (2): tiebreaker picks RA\nada2_upid = modified_df[(modified_df['UPID'].str.startswith('RMV222')) & (modified_df['Drug Name'] == 'ADALIMUMAB')]['UPID'].values[0]\nassert 'rheumatoid arthritis' in ada2_upid, f'Tiebreaker should pick RA (code_freq 3 vs 2), got: {ada2_upid}'\nprint(f'OK: Tiebreaker picked RA over crohns for ADALIMUMAB: {ada2_upid}')\n\n# Patient RMV333 has no GP match -> fallback\ntoci_upid = modified_df[modified_df['Drug Name'] == 'TOCILIZUMAB']['UPID'].values[0]\nassert '(no GP dx)' in toci_upid, f'No GP match should fallback, got: {toci_upid}'\nprint(f'OK: No GP match falls back: {toci_upid}')\n\n# Same patient different indications = different modified UPIDs\nada1_upid = modified_df[modified_df['Drug Name'] == 'ADALIMUMAB 40MG']['UPID'].values[0]\noma1_upid = modified_df[modified_df['Drug Name'] == 'OMALIZUMAB']['UPID'].values[0]\nassert ada1_upid != oma1_upid, 'Different drugs with different indications should have different UPIDs'\nprint(f'OK: Different indications = different UPIDs')\n\nprint()\nprint('ALL TESTS PASSED')\n\"\n","description":"Test assign_drug_indications with mock data","timeout":30000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":164,"cache_read_input_tokens":123214,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":164},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f3709871-362c-4147-a29f-4a64040e3e6a"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0134izEQP46fRzp1zbL5Vi1q","type":"tool_result","content":"Loaded: 162 Search_Terms, 346 fragments\r\n\r\n--- Test 1: Basic drug-indication matching ---\r\n\r\nModified UPIDs:\r\n  ADALIMUMAB 40MG           -> RMV111|rheumatoid arthritis\r\n  OMALIZUMAB                -> RMV111|asthma\r\n  ADALIMUMAB                -> RMV222|rheumatoid arthritis\r\n  TOCILIZUMAB               -> RMV333|RHEUMATOLOGY (no GP dx)\r\n  INFLIXIMAB                -> RMV333|GASTRO (no GP dx)\r\n\r\nIndication DataFrame:\r\n                                              Directory\r\nUPID                                                   \r\nRMV111|rheumatoid arthritis        rheumatoid arthritis\r\nRMV111|asthma                                    asthma\r\nRMV222|rheumatoid arthritis        rheumatoid arthritis\r\nRMV333|RHEUMATOLOGY (no GP dx)  RHEUMATOLOGY (no GP dx)\r\nRMV333|GASTRO (no GP dx)              GASTRO (no GP dx)\r\n\r\n--- Verification ---\r\nOK: ADALIMUMAB matched to RA: RMV111|rheumatoid arthritis\r\nOK: OMALIZUMAB matched to asthma: RMV111|asthma\r\nOK: Tiebreaker picked RA over crohns for ADALIMUMAB: RMV222|rheumatoid arthritis\r\nOK: No GP match falls back: RMV333|RHEUMATOLOGY (no GP dx)\r\nOK: Different indications = different UPIDs\r\n\r\nALL TESTS PASSED","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"af65d06e-caaa-472f-be7e-c36da21719a1","tool_use_result":{"stdout":"Loaded: 162 Search_Terms, 346 fragments\r\n\r\n--- Test 1: Basic drug-indication matching ---\r\n\r\nModified UPIDs:\r\n  ADALIMUMAB 40MG           -> RMV111|rheumatoid arthritis\r\n  OMALIZUMAB                -> RMV111|asthma\r\n  ADALIMUMAB                -> RMV222|rheumatoid arthritis\r\n  TOCILIZUMAB               -> RMV333|RHEUMATOLOGY (no GP dx)\r\n  INFLIXIMAB                -> RMV333|GASTRO (no GP dx)\r\n\r\nIndication DataFrame:\r\n                                              Directory\r\nUPID                                                   \r\nRMV111|rheumatoid arthritis        rheumatoid arthritis\r\nRMV111|asthma                                    asthma\r\nRMV222|rheumatoid arthritis        rheumatoid arthritis\r\nRMV333|RHEUMATOLOGY (no GP dx)  RHEUMATOLOGY (no GP dx)\r\nRMV333|GASTRO (no GP dx)              GASTRO (no GP dx)\r\n\r\n--- Verification ---\r\nOK: ADALIMUMAB matched to RA: RMV111|rheumatoid arthritis\r\nOK: OMALIZUMAB matched to asthma: RMV111|asthma\r\nOK: Tiebreaker picked RA over crohns for ADALIMUMAB: RMV222|rheumatoid arthritis\r\nOK: No GP match falls back: RMV333|RHEUMATOLOGY (no GP dx)\r\nOK: Different indications = different UPIDs\r\n\r\nALL TESTS PASSED\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01UPQoeu2Zjh44xNGY7QdpUS","type":"message","role":"assistant","content":[{"type":"text","text":"All tests pass. Let me also test the tiebreaker with equal frequencies to verify alphabetical fallback."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1802,"cache_read_input_tokens":123378,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1802},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"d4b1214f-4693-45d8-b07b-165d6acf44c6"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01UPQoeu2Zjh44xNGY7QdpUS","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01RtxfciffSfDqSmMdq2KCDT","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && python -c \"\nimport pandas as pd\nfrom data_processing.diagnosis_lookup import assign_drug_indications, load_drug_indication_mapping\n\n_, st_to_frags = load_drug_indication_mapping()\n\n# Test tiebreaker with equal frequencies\ndf = pd.DataFrame({\n    'UPID': ['RMV444'],\n    'Drug Name': ['ADALIMUMAB'],\n    'PseudoNHSNoLinked': ['NHS444'],\n    'Directory': ['RHEUMATOLOGY'],\n})\n\ngp_matches_df = pd.DataFrame({\n    'PatientPseudonym': ['NHS444', 'NHS444'],\n    'Search_Term': ['psoriatic arthritis', 'crohn\\\\'s disease'],\n    'code_frequency': [5, 5],  # Equal frequency\n})\n\nmodified_df, indication_df = assign_drug_indications(df, gp_matches_df, st_to_frags)\nupid = modified_df['UPID'].values[0]\n# Alphabetical: crohn's disease < psoriatic arthritis\nassert 'crohn' in upid, f'Equal freq should pick alphabetically first, got: {upid}'\nprint(f'OK: Equal frequency tiebreaker picks alphabetically: {upid}')\n\n# Test with empty gp_matches_df\ndf2 = pd.DataFrame({\n    'UPID': ['RMV555'],\n    'Drug Name': ['ADALIMUMAB'],\n    'PseudoNHSNoLinked': ['NHS555'],\n    'Directory': ['RHEUMATOLOGY'],\n})\nempty_gp = pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\nmodified_df2, indication_df2 = assign_drug_indications(df2, empty_gp, st_to_frags)\nupid2 = modified_df2['UPID'].values[0]\nassert '(no GP dx)' in upid2, f'Empty GP matches should fallback, got: {upid2}'\nprint(f'OK: Empty GP matches falls back: {upid2}')\n\nprint('ALL EDGE CASE TESTS PASSED')\n\"\n","description":"Test tiebreaker edge cases","timeout":30000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1802,"cache_read_input_tokens":123378,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1802},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"97aca897-46ea-4452-b916-2ed1a8b94a3e"}
{"type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 1\nTraceback (most recent call last):\r\n  File \"<string>\", line 24, in <module>\r\n    assert 'crohn' in upid, f'Equal freq should pick alphabetically first, got: {upid}'\r\n           ^^^^^^^^^^^^^^^\r\nAssertionError: Equal freq should pick alphabetically first, got: RMV444|psoriatic arthritis","is_error":true,"tool_use_id":"toolu_01RtxfciffSfDqSmMdq2KCDT"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"5a8912ee-eb59-4324-99ba-bf0b52397d19","tool_use_result":"Error: Exit code 1\nTraceback (most recent call last):\r\n  File \"<string>\", line 24, in <module>\r\n    assert 'crohn' in upid, f'Equal freq should pick alphabetically first, got: {upid}'\r\n           ^^^^^^^^^^^^^^^\r\nAssertionError: Equal freq should pick alphabetically first, got: RMV444|psoriatic arthritis"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Pcr6pW9yXmjSuGVpw5m3tE","type":"message","role":"assistant","content":[{"type":"text","text":"The alphabetical tiebreaker is comparing with `(-freq, term)` ÔÇö so for equal freq, \"crohn's disease\" < \"psoriatic arthritis\" alphabetically, which means crohn's should come first. But the apostrophe in \"crohn's disease\" might be causing a sorting issue. Let me check."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":784,"cache_read_input_tokens":125180,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":784},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"6647c968-af63-4ceb-b400-140a1d0ed4af"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Pcr6pW9yXmjSuGVpw5m3tE","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011s1TqAshDfxV7jJEcqSJ6p","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && python -c \"\nterms = [\\\"crohn's disease\\\", 'psoriatic arthritis']\nprint(sorted(terms))\n# crohn's has an apostrophe - let's see\nprint(repr(\\\"crohn's disease\\\" < 'psoriatic arthritis'))\n\"\n","description":"Check alphabetical sorting of crohn's vs psoriatic"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":784,"cache_read_input_tokens":125180,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":784},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"0fb301ba-acc6-4669-b17f-787127d637d4"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011s1TqAshDfxV7jJEcqSJ6p","type":"tool_result","content":"[\"crohn's disease\", 'psoriatic arthritis']\r\nTrue","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"91b17186-8974-4d83-9916-1f13cf784fbb","tool_use_result":{"stdout":"[\"crohn's disease\", 'psoriatic arthritis']\r\nTrue\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01NMPEXnVdc7mKdCd1HD9Ssj","type":"message","role":"assistant","content":[{"type":"text","text":"Hmm, \"crohn's disease\" IS alphabetically first. Let me debug what's happening in the function."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":269,"cache_read_input_tokens":125964,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":269},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"2c938ca7-69a9-4d7e-af21-a68bf6292439"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01NMPEXnVdc7mKdCd1HD9Ssj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014EHzCJRyzhqvjKDfSywfX2","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && python -c \"\nimport pandas as pd\nfrom data_processing.diagnosis_lookup import assign_drug_indications, load_drug_indication_mapping, get_search_terms_for_drug\n\n_, st_to_frags = load_drug_indication_mapping()\n\n# Check which Search_Terms ADALIMUMAB matches\nterms = get_search_terms_for_drug('ADALIMUMAB', st_to_frags)\nprint(f'ADALIMUMAB matches: {terms}')\n\n# Which of these overlap with our GP matches?\ngp_terms = {'psoriatic arthritis': 5, \\\"crohn's disease\\\": 5}\noverlap = {t: gp_terms[t] for t in terms if t in gp_terms}\nprint(f'Overlap with GP: {overlap}')\nprint(f'Sorted: {sorted(overlap.items(), key=lambda x: (-x[1], x[0]))}')\n\"\n","description":"Debug ADALIMUMAB matching overlap","timeout":15000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":269,"cache_read_input_tokens":125964,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":269},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"08df5bf1-5e2e-4b79-a29b-71eb279043c0"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014EHzCJRyzhqvjKDfSywfX2","type":"tool_result","content":"ADALIMUMAB matches: ['ankylosing spondylitis', 'axial spondyloarthritis', 'plaque psoriasis', 'psoriatic arthritis', 'rheumatoid arthritis', 'ulcerative colitis', 'uveitis']\r\nOverlap with GP: {'psoriatic arthritis': 5}\r\nSorted: [('psoriatic arthritis', 5)]","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"ff815cfb-434f-4f1a-9a6d-b55f85db87ed","tool_use_result":{"stdout":"ADALIMUMAB matches: ['ankylosing spondylitis', 'axial spondyloarthritis', 'plaque psoriasis', 'psoriatic arthritis', 'rheumatoid arthritis', 'ulcerative colitis', 'uveitis']\r\nOverlap with GP: {'psoriatic arthritis': 5}\r\nSorted: [('psoriatic arthritis', 5)]\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Y82A3ViMZ672D738Md6SX8","type":"message","role":"assistant","content":[{"type":"text","text":"I see the issue. \"crohn's disease\" from DimSearchTerm.csv is not in the ADALIMUMAB Search_Terms list ÔÇö looking at the output, ADALIMUMAB matches: ankylosing spondylitis, axial spondyloarthritis, plaque psoriasis, psoriatic arthritis, rheumatoid arthritis, ulcerative colitis, uveitis. \"crohn's disease\" is NOT in that list (ADALIMUMAB doesn't match crohn's in DimSearchTerm.csv). So only psoriatic arthritis is in the intersection, giving a single match ÔÇö not a tiebreaker.\n\nMy test was wrong about the data. Let me fix the test to use two Search_Terms that actually overlap with ADALIMUMAB."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":684,"cache_read_input_tokens":126233,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":684},"output_tokens":6,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"90ea751a-d887-487e-82db-1010611e6f58"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01Y82A3ViMZ672D738Md6SX8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UhphCkaSdqjf7hCr1jE1Gk","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && python -c \"\nimport pandas as pd\nfrom data_processing.diagnosis_lookup import assign_drug_indications, load_drug_indication_mapping, get_search_terms_for_drug\n\n_, st_to_frags = load_drug_indication_mapping()\n\n# Use two Search_Terms that ADALIMUMAB actually matches\n# Both psoriatic arthritis and rheumatoid arthritis\ndf = pd.DataFrame({\n    'UPID': ['RMV444'],\n    'Drug Name': ['ADALIMUMAB'],\n    'PseudoNHSNoLinked': ['NHS444'],\n    'Directory': ['RHEUMATOLOGY'],\n})\n\ngp_matches_df = pd.DataFrame({\n    'PatientPseudonym': ['NHS444', 'NHS444'],\n    'Search_Term': ['psoriatic arthritis', 'rheumatoid arthritis'],\n    'code_frequency': [5, 5],  # Equal frequency\n})\n\nmodified_df, indication_df = assign_drug_indications(df, gp_matches_df, st_to_frags)\nupid = modified_df['UPID'].values[0]\n# Alphabetical: psoriatic arthritis < rheumatoid arthritis\nassert 'psoriatic arthritis' in upid, f'Equal freq should pick alphabetically first (psoriatic < rheumatoid), got: {upid}'\nprint(f'OK: Equal frequency tiebreaker picks alphabetically: {upid}')\n\n# Test different frequencies ÔÇö higher wins\ngp_matches_df2 = pd.DataFrame({\n    'PatientPseudonym': ['NHS444', 'NHS444'],\n    'Search_Term': ['psoriatic arthritis', 'rheumatoid arthritis'],\n    'code_frequency': [3, 47],  # RA has higher frequency\n})\nmodified_df2, _ = assign_drug_indications(df, gp_matches_df2, st_to_frags)\nupid2 = modified_df2['UPID'].values[0]\nassert 'rheumatoid arthritis' in upid2, f'Higher freq should win, got: {upid2}'\nprint(f'OK: Higher frequency wins: {upid2}')\n\n# Test empty gp_matches\nempty_gp = pd.DataFrame(columns=['PatientPseudonym', 'Search_Term', 'code_frequency'])\nmodified_df3, indication_df3 = assign_drug_indications(df, empty_gp, st_to_frags)\nupid3 = modified_df3['UPID'].values[0]\nassert '(no GP dx)' in upid3, f'Empty GP should fallback, got: {upid3}'\nprint(f'OK: Empty GP falls back: {upid3}')\n\nprint('ALL TIEBREAKER TESTS PASSED')\n\"\n","description":"Corrected tiebreaker edge case tests","timeout":30000}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":684,"cache_read_input_tokens":126233,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":684},"output_tokens":6,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"8c471964-10af-432e-b70e-561cc43ffb52"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UhphCkaSdqjf7hCr1jE1Gk","type":"tool_result","content":"OK: Equal frequency tiebreaker picks alphabetically: RMV444|psoriatic arthritis\r\nOK: Higher frequency wins: RMV444|rheumatoid arthritis\r\nOK: Empty GP falls back: RMV444|RHEUMATOLOGY (no GP dx)\r\nALL TIEBREAKER TESTS PASSED","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"0ba58eaa-a80a-4060-9109-56a6b5ba04a0","tool_use_result":{"stdout":"OK: Equal frequency tiebreaker picks alphabetically: RMV444|psoriatic arthritis\r\nOK: Higher frequency wins: RMV444|rheumatoid arthritis\r\nOK: Empty GP falls back: RMV444|RHEUMATOLOGY (no GP dx)\r\nALL TIEBREAKER TESTS PASSED\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01XzAZrMNdnnaFtJpHkdt6rg","type":"message","role":"assistant","content":[{"type":"text","text":"All tests pass:\n- Equal frequency ÔåÆ alphabetical tiebreaker works (\"psoriatic arthritis\" < \"rheumatoid arthritis\")\n- Higher frequency wins (47 RA codes vs 3 psoriatic codes ÔåÆ RA)\n- Empty GP matches ÔåÆ fallback to directory\n\nNow let me update the IMPLEMENTATION_PLAN.md to mark Task 2.1 and 2.2 complete, then commit and update progress.txt."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1076,"cache_read_input_tokens":126917,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1076},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"b0bb082d-24d3-4b57-a07b-de399694bdcd"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01XzAZrMNdnnaFtJpHkdt6rg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GVzyhTEvB42qJTVYzBQXoU","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\IMPLEMENTATION_PLAN.md","offset":107,"limit":40}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1076,"cache_read_input_tokens":126917,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1076},"output_tokens":4,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f9622e0d-d50a-43e2-80f1-20f94464f440"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GVzyhTEvB42qJTVYzBQXoU","type":"tool_result","content":"   107ÔåÆ## Phase 2: Drug-Aware Indication Matching Logic\n   108ÔåÆ\n   109ÔåÆ### 2.1 Create `assign_drug_indications()` function\n   110ÔåÆ- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n   111ÔåÆ  ```\n   112ÔåÆ  def assign_drug_indications(\n   113ÔåÆ      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n   114ÔåÆ      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n   115ÔåÆ      drug_mapping: dict,             # From load_drug_indication_mapping()\n   116ÔåÆ  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n   117ÔåÆ      Returns: (modified_df, indication_df)\n   118ÔåÆ      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n   119ÔåÆ      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n   120ÔåÆ  ```\n   121ÔåÆ- [ ] Logic per UPID + Drug Name pair:\n   122ÔåÆ  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n   123ÔåÆ  2. Get which Search_Terms include this drug (from drug_mapping)\n   124ÔåÆ  3. Intersection = valid indications for this drug-patient pair\n   125ÔåÆ  4. If 1 match: use it\n   126ÔåÆ  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n   127ÔåÆ  6. If 0 matches: use fallback directory\n   128ÔåÆ- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n   129ÔåÆ- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n   130ÔåÆ- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n   131ÔåÆ\n   132ÔåÆ### 2.2 Handle tiebreaker for multiple indication matches\n   133ÔåÆ- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n   134ÔåÆ  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n   135ÔåÆ  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n   136ÔåÆ  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n   137ÔåÆ  - code_frequency is already returned by the updated query in Task 1.1\n   138ÔåÆ- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n   139ÔåÆ- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism\n   140ÔåÆ\n   141ÔåÆ---\n   142ÔåÆ\n   143ÔåÆ## Phase 3: Pipeline Integration\n   144ÔåÆ\n   145ÔåÆ### 3.1 Update `refresh_pathways.py` indication processing\n   146ÔåÆ- [ ] In the `elif current_chart_type == \"indication\":` block:\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"0ed84530-1d0c-4fab-9511-2e53d5ef22a0","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\IMPLEMENTATION_PLAN.md","content":"## Phase 2: Drug-Aware Indication Matching Logic\n\n### 2.1 Create `assign_drug_indications()` function\n- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n  ```\n  def assign_drug_indications(\n      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n      drug_mapping: dict,             # From load_drug_indication_mapping()\n  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n      Returns: (modified_df, indication_df)\n      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n  ```\n- [ ] Logic per UPID + Drug Name pair:\n  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n  2. Get which Search_Terms include this drug (from drug_mapping)\n  3. Intersection = valid indications for this drug-patient pair\n  4. If 1 match: use it\n  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n  6. If 0 matches: use fallback directory\n- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n\n### 2.2 Handle tiebreaker for multiple indication matches\n- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n  - code_frequency is already returned by the updated query in Task 1.1\n- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism\n\n---\n\n## Phase 3: Pipeline Integration\n\n### 3.1 Update `refresh_pathways.py` indication processing\n- [ ] In the `elif current_chart_type == \"indication\":` block:","numLines":40,"startLine":107,"totalLines":239}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01EgXtAviW8SKdvgxLsfk1Br","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DprQgdY3ovW1m5zvmxivSD","name":"Edit","input":{"replace_all":false,"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\IMPLEMENTATION_PLAN.md","old_string":"### 2.1 Create `assign_drug_indications()` function\n- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n  ```\n  def assign_drug_indications(\n      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n      drug_mapping: dict,             # From load_drug_indication_mapping()\n  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n      Returns: (modified_df, indication_df)\n      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n  ```\n- [ ] Logic per UPID + Drug Name pair:\n  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n  2. Get which Search_Terms include this drug (from drug_mapping)\n  3. Intersection = valid indications for this drug-patient pair\n  4. If 1 match: use it\n  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n  6. If 0 matches: use fallback directory\n- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n\n### 2.2 Handle tiebreaker for multiple indication matches\n- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n  - code_frequency is already returned by the updated query in Task 1.1\n- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism","new_string":"### 2.1 Create `assign_drug_indications()` function\n- [x] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n  ```\n  def assign_drug_indications(\n      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n      drug_mapping: dict,             # From load_drug_indication_mapping()\n  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n      Returns: (modified_df, indication_df)\n      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n  ```\n- [x] Logic per UPID + Drug Name pair:\n  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n  2. Get which Search_Terms include this drug (from drug_mapping)\n  3. Intersection = valid indications for this drug-patient pair\n  4. If 1 match: use it\n  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n  6. If 0 matches: use fallback directory\n- [x] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n- [x] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n- [x] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n\n### 2.2 Handle tiebreaker for multiple indication matches\n- [x] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n  - code_frequency is already returned by the updated query in Task 1.1\n- [x] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n- [x] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1087,"cache_read_input_tokens":127993,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1087},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f015b75f-3e00-46a7-be92-f01245f9d970"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DprQgdY3ovW1m5zvmxivSD","type":"tool_result","content":"The file C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\IMPLEMENTATION_PLAN.md has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"4fb0ea08-8d93-45a3-805b-00c0e317d887","tool_use_result":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\IMPLEMENTATION_PLAN.md","oldString":"### 2.1 Create `assign_drug_indications()` function\n- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n  ```\n  def assign_drug_indications(\n      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n      drug_mapping: dict,             # From load_drug_indication_mapping()\n  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n      Returns: (modified_df, indication_df)\n      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n  ```\n- [ ] Logic per UPID + Drug Name pair:\n  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n  2. Get which Search_Terms include this drug (from drug_mapping)\n  3. Intersection = valid indications for this drug-patient pair\n  4. If 1 match: use it\n  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n  6. If 0 matches: use fallback directory\n- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n\n### 2.2 Handle tiebreaker for multiple indication matches\n- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n  - code_frequency is already returned by the updated query in Task 1.1\n- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism","newString":"### 2.1 Create `assign_drug_indications()` function\n- [x] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n  ```\n  def assign_drug_indications(\n      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n      drug_mapping: dict,             # From load_drug_indication_mapping()\n  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n      Returns: (modified_df, indication_df)\n      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n  ```\n- [x] Logic per UPID + Drug Name pair:\n  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n  2. Get which Search_Terms include this drug (from drug_mapping)\n  3. Intersection = valid indications for this drug-patient pair\n  4. If 1 match: use it\n  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n  6. If 0 matches: use fallback directory\n- [x] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n- [x] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n- [x] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n\n### 2.2 Handle tiebreaker for multiple indication matches\n- [x] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n  - code_frequency is already returned by the updated query in Task 1.1\n- [x] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n- [x] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism","originalFile":"# Implementation Plan - Drug-Aware Indication Matching\n\n## Project Overview\n\nUpdate the indication-based pathway charts so that patient indications are matched **per drug**, not just per patient. Currently, each patient gets ONE indication (most recent GP diagnosis match). This ignores which drugs the patient is actually taking.\n\n### The Problem\n\nA patient on ADALIMUMAB + OMALIZUMAB currently gets assigned a single indication (e.g., \"rheumatoid arthritis\" ÔÇö the most recent GP match). But:\n- ADALIMUMAB is used for rheumatoid arthritis, axial spondyloarthritis, crohn's disease, etc.\n- OMALIZUMAB is used for asthma, allergic asthma, urticaria\n\nThese are different clinical pathways and should be treated as separate treatment journeys.\n\n### The Solution\n\nMatch each drug to an indication by cross-referencing:\n1. **GP diagnosis** ÔÇö which Search_Terms the patient has matching SNOMED codes for\n2. **Drug mapping** ÔÇö which Search_Terms list each drug (from `DimSearchTerm.csv`)\n\nOnly assign a drug to an indication if BOTH conditions are met. If a patient's drugs map to different indications, they become separate pathways (via modified UPID).\n\n### Key Design Decisions\n\n| Aspect | Decision |\n|--------|----------|\n| Drug-indication source | `data/DimSearchTerm.csv` ÔÇö Search_Term ÔåÆ CleanedDrugName mapping |\n| UPID modification | `{original_UPID}\\|{search_term}` for drugs with matched indication |\n| GP diagnosis matching | Return ALL matches per patient (not just most recent) |\n| Drug matching | Substring match: HCD drug name contains DimSearchTerm fragment |\n| Multiple indication matches per drug | Use highest GP code frequency as tiebreaker (COUNT of matching SNOMED codes per Search_Term) |\n| GP code time range | Only codes from MIN(Intervention Date) onwards ÔÇö restricts to HCD data window |\n| No indication match | Fallback to directory (same as current behavior) |\n| Same patient, different indications | Separate pathways via different modified UPIDs |\n\n### Examples\n\n**Patient on ADALIMUMAB + GOLIMUMAB, GP dx: axial spondyloarthritis + asthma**\n- axial spondyloarthritis drug list includes both ADALIMUMAB and GOLIMUMAB\n- ÔåÆ Both drugs grouped under \"axial spondyloarthritis\", single pathway\n- Modified UPID: `RMV12345|axial spondyloarthritis`\n\n**Patient on ADALIMUMAB + OMALIZUMAB, GP dx: axial spondyloarthritis + asthma**\n- axial spondyloarthritis lists ADALIMUMAB but not OMALIZUMAB\n- asthma lists OMALIZUMAB but not ADALIMUMAB\n- ÔåÆ Two separate pathways:\n  - `RMV12345|axial spondyloarthritis` with ADALIMUMAB\n  - `RMV12345|asthma` with OMALIZUMAB\n\n**Patient on ADALIMUMAB, GP dx: rheumatoid arthritis (47 codes) + crohn's disease (2 codes)**\n- Both Search_Terms list ADALIMUMAB AND patient has GP dx for both\n- ÔåÆ Tiebreaker: highest code frequency ÔÇö rheumatoid arthritis has 47 matching SNOMED codes vs 2 for crohn's\n- ÔåÆ Single pathway under rheumatoid arthritis (more clinical activity = more likely the treatment indication)\n\n---\n\n## Phase 1: Update Snowflake Query & Drug Mapping\n\n### 1.1 Update `get_patient_indication_groups()` to return ALL matches with frequency\n- [x] Modify the Snowflake query in `get_patient_indication_groups()` (diagnosis_lookup.py):\n  - Remove `QUALIFY ROW_NUMBER() OVER (PARTITION BY ... ORDER BY EventDateTime DESC) = 1`\n  - Return ALL matching Search_Terms per patient with code frequency:\n    ```sql\n    SELECT pc.\"PatientPseudonym\" AS \"PatientPseudonym\",\n           aic.Search_Term AS \"Search_Term\",\n           COUNT(*) AS \"code_frequency\"\n    FROM PrimaryCareClinicalCoding pc\n    JOIN AllIndicationCodes aic ON pc.\"SNOMEDCode\" = aic.SNOMEDCode\n    WHERE pc.\"PatientPseudonym\" IN (...)\n      AND pc.\"EventDateTime\" >= :earliest_hcd_date\n    GROUP BY pc.\"PatientPseudonym\", aic.Search_Term\n    ```\n  - `code_frequency` = number of matching SNOMED codes per Search_Term per patient\n  - Higher frequency = more clinical activity = stronger signal for tiebreaker\n  - `earliest_hcd_date` = `MIN(Intervention Date)` from the HCD DataFrame ÔÇö restricts GP codes to the HCD data window, reducing noise from old/irrelevant diagnoses\n- [x] Accept `earliest_hcd_date` parameter in `get_patient_indication_groups()` and pass to query\n- [x] Keep batch processing (500 patients per query)\n- [x] Update return type: DataFrame now has multiple rows per patient (PatientPseudonym, Search_Term, code_frequency)\n- [ ] Verify: Query returns more rows than before (patients with multiple matching diagnoses) *(requires live Snowflake ÔÇö will be verified in Phase 3/4)*\n\n### 1.2 Merge related asthma Search_Terms in CLUSTER_MAPPING_SQL\n- [x] In `CLUSTER_MAPPING_SQL` (diagnosis_lookup.py), merge these 3 Search_Terms into one `\"asthma\"` entry:\n  - `allergic asthma` (Cluster: OMALIZUMAB only)\n  - `asthma` (Cluster: BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, OMALIZUMAB, RESLIZUMAB)\n  - `severe persistent allergic asthma` (Cluster: OMALIZUMAB only)\n- [x] Map all 3 Cluster_IDs to `Search_Term = 'asthma'` in the CTE VALUES\n- [x] `urticaria` (OMALIZUMAB, DERMATOLOGY) stays SEPARATE ÔÇö do NOT merge with asthma\n- [x] Also update `load_drug_indication_mapping()` to apply the same merge when loading DimSearchTerm.csv:\n  - Combine drug lists from all 3 entries under a single `\"asthma\"` key\n  - Deduplicate drug fragments (OMALIZUMAB appears in all 3)\n- [x] Verify: GP code lookup returns `\"asthma\"` (not `\"allergic asthma\"` or `\"severe persistent allergic asthma\"`)\n- [x] Verify: Drug mapping for `\"asthma\"` includes full combined drug list: BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, OMALIZUMAB, RESLIZUMAB\n\n### 1.3 Build drug-to-Search_Term lookup from DimSearchTerm.csv\n- [x] Add function `load_drug_indication_mapping()` to `diagnosis_lookup.py`:\n  - Loads `data/DimSearchTerm.csv`\n  - Builds dict: `drug_fragment (uppercase) ÔåÆ list[Search_Term]`\n  - Also builds reverse: `search_term ÔåÆ list[drug_fragments]`\n  - CleanedDrugName is pipe-separated (e.g., \"ADALIMUMAB|GOLIMUMAB|IXEKIZUMAB\")\n- [x] Add function `get_search_terms_for_drug(drug_name, search_term_to_fragments) -> list[str]`:\n  - Returns all Search_Terms whose drug fragments are substrings of the drug name (case-insensitive)\n  - More practical than per-term boolean check ÔÇö returns all matches at once for Phase 2 use\n- [x] Verify: ADALIMUMAB matches \"axial spondyloarthritis\", OMALIZUMAB matches \"asthma\"\n\n---\n\n## Phase 2: Drug-Aware Indication Matching Logic\n\n### 2.1 Create `assign_drug_indications()` function\n- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:\n  ```\n  def assign_drug_indications(\n      df: pd.DataFrame,              # HCD data with UPID, Drug Name columns\n      gp_matches_df: pd.DataFrame,   # PatientPseudonym ÔåÆ list of matched Search_Terms\n      drug_mapping: dict,             # From load_drug_indication_mapping()\n  ) -> tuple[pd.DataFrame, pd.DataFrame]:\n      Returns: (modified_df, indication_df)\n      - modified_df: HCD data with UPID replaced by {UPID}|{indication}\n      - indication_df: mapping modified_UPID ÔåÆ Search_Term\n  ```\n- [ ] Logic per UPID + Drug Name pair:\n  1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)\n  2. Get which Search_Terms include this drug (from drug_mapping)\n  3. Intersection = valid indications for this drug-patient pair\n  4. If 1 match: use it\n  5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)\n  6. If 0 matches: use fallback directory\n- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`\n- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)\n- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)\n\n### 2.2 Handle tiebreaker for multiple indication matches\n- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:\n  - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)\n  - Higher code_frequency = more clinical activity for that condition = more likely treatment indication\n  - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA\n  - code_frequency is already returned by the updated query in Task 1.1\n- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis\n- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism\n\n---\n\n## Phase 3: Pipeline Integration\n\n### 3.1 Update `refresh_pathways.py` indication processing\n- [ ] In the `elif current_chart_type == \"indication\":` block:\n  1. Call `get_patient_indication_groups()` as before (but now returns ALL matches)\n  2. Load drug mapping: `drug_mapping = load_drug_indication_mapping()`\n  3. Call `assign_drug_indications(df, gp_matches_df, drug_mapping)`\n  4. Use modified_df (with indication-aware UPIDs) for pathway processing\n  5. Use indication_df for the indication mapping\n- [ ] Pass modified_df (not original df) to `process_indication_pathway_for_date_filter()`\n- [ ] Verify: Pipeline compiles, `python -m py_compile cli/refresh_pathways.py`\n\n### 3.2 Test with dry run\n- [ ] Run `python -m cli.refresh_pathways --chart-type indication --dry-run -v`\n- [ ] Verify:\n  - Modified UPIDs appear in pipeline log (e.g., `RMV12345|rheumatoid arthritis`)\n  - Patient counts are reasonable (will be higher than before since same patient can appear under multiple indications)\n  - Drug-indication matching is logged (match rate, fallback rate)\n  - Pathway hierarchy shows drug-specific grouping under correct indications\n\n---\n\n## Phase 4: Full Refresh & Validation\n\n### 4.1 Full refresh with both chart types\n- [ ] Run `python -m cli.refresh_pathways --chart-type all`\n- [ ] Verify:\n  - Both chart types generate data\n  - Directory charts unchanged (no modified UPIDs)\n  - Indication charts reflect drug-aware matching\n\n### 4.2 Validate indication chart correctness\n- [ ] Check that drugs under an indication all appear in that Search_Term's drug list\n- [ ] Verify that a patient on drugs for different indications creates separate pathway branches\n- [ ] Verify that drugs sharing an indication are grouped in the same pathway\n- [ ] Log: patient count comparison (old vs new approach)\n\n### 4.3 Validate Reflex UI\n- [ ] Run `python -m reflex compile` to verify app compiles\n- [ ] Verify chart type toggle still works\n- [ ] Verify indication chart shows correct hierarchy\n\n---\n\n## Completion Criteria\n\nAll tasks marked `[x]` AND:\n- [ ] App compiles without errors (`reflex compile` succeeds)\n- [ ] Both chart types generate pathway data\n- [ ] Indication charts show drug-specific indication matching\n- [ ] Drugs under the same indication for the same patient are in one pathway\n- [ ] Drugs under different indications for the same patient create separate pathways\n- [ ] Fallback works for drugs with no indication match\n- [ ] Full refresh completes successfully\n- [ ] Existing directory charts are unaffected\n\n---\n\n## Reference\n\n### DimSearchTerm.csv Structure\n```\nSearch_Term,CleanedDrugName,PrimaryDirectorate\nrheumatoid arthritis,ABATACEPT|ADALIMUMAB|ANAKINRA|BARICITINIB|...,RHEUMATOLOGY\nasthma,BENRALIZUMAB|DUPILUMAB|INHALED|MEPOLIZUMAB|OMALIZUMAB|RESLIZUMAB,THORACIC MEDICINE\n```\n\n### Modified UPID Format\n```\nOriginal:  RMV12345\nModified:  RMV12345|rheumatoid arthritis\nFallback:  RMV12345|RHEUMATOLOGY (no GP dx)\n```\n\n### Current vs New Indication Flow\n```\nCURRENT:\n  Patient ÔåÆ GP dx (most recent) ÔåÆ single Search_Term ÔåÆ one pathway\n\nNEW:\n  Patient + Drug A ÔåÆ GP dx matching Drug A ÔåÆ Search_Term X\n  Patient + Drug B ÔåÆ GP dx matching Drug B ÔåÆ Search_Term Y\n  ÔåÆ If X == Y: one pathway under X\n  ÔåÆ If X != Y: two pathways (modified UPIDs)\n```\n\n### Key Files\n\n| File | Changes |\n|------|---------|\n| `data_processing/diagnosis_lookup.py` | Update query, add drug mapping functions |\n| `data_processing/pathway_pipeline.py` | Possibly minor changes for modified UPIDs |\n| `cli/refresh_pathways.py` | Integrate drug-aware matching into pipeline |\n| `data/DimSearchTerm.csv` | Reference data (read-only) |\n| `analysis/pathway_analyzer.py` | No changes expected (UPID changes are transparent) |\n| `pathways_app/pathways_app.py` | No changes expected |\n","structuredPatch":[{"oldStart":107,"oldLines":7,"newStart":107,"newLines":7,"lines":[" ## Phase 2: Drug-Aware Indication Matching Logic"," "," ### 2.1 Create `assign_drug_indications()` function","-- [ ] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:","+- [x] Add to `diagnosis_lookup.py` or `pathway_pipeline.py`:","   ```","   def assign_drug_indications(","       df: pd.DataFrame,              # HCD data with UPID, Drug Name columns"]},{"oldStart":118,"oldLines":25,"newStart":118,"newLines":25,"lines":["       - modified_df: HCD data with UPID replaced by {UPID}|{indication}","       - indication_df: mapping modified_UPID ÔåÆ Search_Term","   ```","-- [ ] Logic per UPID + Drug Name pair:","+- [x] Logic per UPID + Drug Name pair:","   1. Get patient's GP-matched Search_Terms with code_frequency (from gp_matches_df via PseudoNHSNoLinked)","   2. Get which Search_Terms include this drug (from drug_mapping)","   3. Intersection = valid indications for this drug-patient pair","   4. If 1 match: use it","   5. If multiple matches: use highest code_frequency as tiebreaker (most GP coding activity = most likely treatment indication)","   6. If 0 matches: use fallback directory","-- [ ] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`","-- [ ] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)","-- [ ] Verify: Function compiles, handles edge cases (no GP match, no drug match)","+- [x] Modify UPID in df rows: `{original_UPID}|{matched_search_term}`","+- [x] Build indication_df: `{modified_UPID}` ÔåÆ `Search_Term` (or fallback label)","+- [x] Verify: Function compiles, handles edge cases (no GP match, no drug match)"," "," ### 2.2 Handle tiebreaker for multiple indication matches","-- [ ] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:","+- [x] When a drug matches multiple Search_Terms AND patient has GP dx for multiple:","   - Use `code_frequency` from the GP query (COUNT of matching SNOMED codes per Search_Term)","   - Higher code_frequency = more clinical activity for that condition = more likely treatment indication","   - E.g., patient with 47 RA codes and 2 crohn's codes ÔåÆ ADALIMUMAB assigned to RA","   - code_frequency is already returned by the updated query in Task 1.1","-- [ ] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis","-- [ ] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism","+- [x] Verify: Tiebreaker logic correctly picks highest-frequency diagnosis","+- [x] Verify: Tie on frequency (rare but possible) falls back to alphabetical Search_Term for determinism"," "," ---"," "]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_014cMjfz5Paspg6WPH13iNNG","type":"message","role":"assistant","content":[{"type":"text","text":"Now commit the changes."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1369,"cache_read_input_tokens":129080,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1369},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f3987f79-1638-4dbe-9d5b-cd9a9ecd99ed"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_014cMjfz5Paspg6WPH13iNNG","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014fU6TWEE47EyP9BGGefTQf","name":"TodoWrite","input":{"todos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"completed","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"in_progress","activeForm":"Committing and updating progress"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1369,"cache_read_input_tokens":129080,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1369},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f454e6fc-ed81-4ad6-be7b-f39106d4d389"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014fU6TWEE47EyP9BGGefTQf","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"4d87d538-587c-49c1-b598-fbe4bc19623f","tool_use_result":{"oldTodos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"in_progress","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"pending","activeForm":"Committing and updating progress"}],"newTodos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"completed","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"in_progress","activeForm":"Committing and updating progress"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01GpHzjMT8YAbRU546a5fNZR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01RdxBpGodKEhsHHqBuBosJq","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && git add data_processing/diagnosis_lookup.py IMPLEMENTATION_PLAN.md && git commit -m \"$(cat <<'EOF'\nfeat: add assign_drug_indications() for drug-aware indication matching (Task 2.1 + 2.2)\nEOF\n)\"","description":"Commit drug-indication matching function"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":259,"cache_read_input_tokens":130449,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":259},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"ae839dcf-d042-4d6a-a254-e4080c5e5b9b"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RdxBpGodKEhsHHqBuBosJq","type":"tool_result","content":"[feature/snomed-indication-mapping 408976e] feat: add assign_drug_indications() for drug-aware indication matching (Task 2.1 + 2.2)\n 2 files changed, 179 insertions(+), 8 deletions(-)","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"7c1b6a13-be38-44ee-bece-5288a8dbeae4","tool_use_result":{"stdout":"[feature/snomed-indication-mapping 408976e] feat: add assign_drug_indications() for drug-aware indication matching (Task 2.1 + 2.2)\n 2 files changed, 179 insertions(+), 8 deletions(-)\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01GHWoNU2Pccnq196JTFKKwR","type":"message","role":"assistant","content":[{"type":"text","text":"Now update progress.txt with the iteration log."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":235,"cache_read_input_tokens":130708,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":235},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"007baf33-ee24-4e76-88c0-e39c0601312b"}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01GHWoNU2Pccnq196JTFKKwR","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01W3QeJNk7G8AjN5bGNJCZSN","name":"Read","input":{"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\progress.txt","offset":193,"limit":5}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":235,"cache_read_input_tokens":130708,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":235},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"5fb05357-f162-4241-b2da-50ef2fa0f152"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01W3QeJNk7G8AjN5bGNJCZSN","type":"tool_result","content":"   193ÔåÆ- The final Phase 1 subtask (1.1 verify with live Snowflake) will be tested during Phase 3/4 integration\n   194ÔåÆ### Blocked items:\n   195ÔåÆ- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4\n   196ÔåÆ\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"023798ea-bbad-4ac3-9269-d7cfe8ff3d4d","tool_use_result":{"type":"text","file":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\progress.txt","content":"- The final Phase 1 subtask (1.1 verify with live Snowflake) will be tested during Phase 3/4 integration\n### Blocked items:\n- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4\n","numLines":4,"startLine":193,"totalLines":196}}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01TWYu7tHjVyBbvy5ZpZyv7s","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FMSzTY2BkumFv5N9Rj6qcS","name":"Edit","input":{"replace_all":false,"file_path":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\progress.txt","old_string":"### Blocked items:\n- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4","new_string":"### Blocked items:\n- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4\n\n## Iteration 4 ÔÇö 2026-02-05\n### Task: 2.1 + 2.2 ÔÇö Create assign_drug_indications() with tiebreaker logic\n### Why this task:\n- All Phase 1 dependencies complete (1.1 query returns ALL matches, 1.2 asthma merged, 1.3 drug mapping loaded)\n- Task 2.1 is the core matching function needed before Phase 3 pipeline integration\n- Task 2.2 (tiebreaker) is naturally part of 2.1 ÔÇö implemented together\n- Can be built and tested with mock data (no Snowflake needed)\n### Status: COMPLETE\n### What was done:\n- Added `assign_drug_indications()` to `diagnosis_lookup.py`:\n  - Input: HCD df (with UPID, Drug Name, PseudoNHSNoLinked, Directory), gp_matches_df, search_term_to_fragments\n  - Output: (modified_df with UPID|search_term, indication_df mapping modified_UPID ÔåÆ Search_Term)\n  - Builds GP lookup: PseudoNHSNoLinked ÔåÆ {Search_Term: code_frequency}\n  - Caches drugÔåÆSearch_Term lookups to avoid recomputing per row\n  - For each (UPID, Drug Name) pair:\n    - Intersects drug's Search_Terms with patient's GP matches\n    - Single match: use it\n    - Multiple matches: highest code_frequency wins, alphabetical tiebreak\n    - No match: fallback to \"{Directory} (no GP dx)\"\n  - Applies modified UPIDs via df.apply() (vectorized lookup from cache)\n  - Builds indication_df with unique modified UPID ÔåÆ Directory column\n  - Comprehensive logging: match rate, tiebreaker count, fallback count, top 5 indications\n- Updated __all__ exports\n### Validation results:\n- Tier 1 (Code): py_compile PASSED, import check PASSED\n- Tier 2 (Data): Mock data tests ALL PASSED:\n  - ADALIMUMAB + GP dx (RA + asthma) ÔåÆ matched to RA (drug mapping intersection)\n  - OMALIZUMAB + GP dx (RA + asthma) ÔåÆ matched to asthma (drug mapping intersection)\n  - ADALIMUMAB + GP dx (RA 3 freq + crohn's 2 freq) ÔåÆ tiebreaker picks RA\n  - ADALIMUMAB + GP dx (psoriatic 5 freq + RA 5 freq) ÔåÆ alphabetical tiebreak picks psoriatic arthritis\n  - Higher frequency (47 RA vs 3 psoriatic) ÔåÆ RA wins\n  - No GP match ÔåÆ fallback to directory\n  - Empty GP DataFrame ÔåÆ all fallback\n  - Different drugs with different indications ÔåÆ different modified UPIDs\n- Tier 3 (Functional): N/A (no UI changes yet)\n### Files changed:\n- data_processing/diagnosis_lookup.py (added assign_drug_indications, updated __all__)\n- IMPLEMENTATION_PLAN.md (marked 2.1 and 2.2 subtasks [x])\n### Committed: 408976e \"feat: add assign_drug_indications() for drug-aware indication matching (Task 2.1 + 2.2)\"\n### Patterns discovered:\n- Function signature takes `search_term_to_fragments` (the second element from load_drug_indication_mapping()) ÔÇö NOT the full tuple. Callers must destructure: `_, st_to_frags = load_drug_indication_mapping()`\n- The function uses df.apply() to set modified UPIDs ÔÇö for large DataFrames (656K rows), this could be slow. If performance is an issue in Phase 3, could vectorize with merge operations instead. But apply with cached lookup dict should be OK.\n- \"crohn's disease\" is NOT in ADALIMUMAB's DimSearchTerm mapping (ADALIMUMAB maps to: ankylosing spondylitis, axial spondyloarthritis, plaque psoriasis, psoriatic arthritis, rheumatoid arthritis, ulcerative colitis, uveitis). Initial test assumption was wrong.\n- indication_df has 'Directory' column (not 'Search_Term') for compatibility with generate_icicle_chart_indication() which expects indication_df.loc[upid, 'Directory']\n### Next iteration should:\n- Work on Task 3.1: Update `refresh_pathways.py` indication processing to use assign_drug_indications()\n  - The current code at lines 424-428 uses `dict(zip(...))` which only keeps LAST match per patient ÔÇö this must be replaced\n  - Key changes in the `elif current_chart_type == \"indication\":` block:\n    1. Load drug mapping: `_, st_to_frags = load_drug_indication_mapping()`\n    2. Pass `earliest_hcd_date=df['Intervention Date'].min().strftime('%Y-%m-%d')` to get_patient_indication_groups()\n    3. Call `assign_drug_indications(df, gp_matches_df, st_to_frags)` to get (modified_df, indication_df)\n    4. Use modified_df (not original df) for pathway processing\n    5. indication_df is already in the right format (indexed by modified UPID, 'Directory' column)\n    6. Remove the old match_lookup/dict(zip) code and the manual indication_df building\n  - Import assign_drug_indications and load_drug_indication_mapping at top of file\n  - This replaces ~50 lines of the old approach with ~10 lines using the new function\n  - Can verify with py_compile; full Snowflake test via --dry-run\n### Blocked items:\n- None"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":298,"cache_read_input_tokens":130943,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":298},"output_tokens":24,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"f45faff6-0237-46f0-a6f9-fcaac48268f9"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FMSzTY2BkumFv5N9Rj6qcS","type":"tool_result","content":"The file C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\progress.txt has been updated successfully."}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"b8afa581-5f62-457f-854b-8cf013d18707","tool_use_result":{"filePath":"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\\progress.txt","oldString":"### Blocked items:\n- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4","newString":"### Blocked items:\n- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4\n\n## Iteration 4 ÔÇö 2026-02-05\n### Task: 2.1 + 2.2 ÔÇö Create assign_drug_indications() with tiebreaker logic\n### Why this task:\n- All Phase 1 dependencies complete (1.1 query returns ALL matches, 1.2 asthma merged, 1.3 drug mapping loaded)\n- Task 2.1 is the core matching function needed before Phase 3 pipeline integration\n- Task 2.2 (tiebreaker) is naturally part of 2.1 ÔÇö implemented together\n- Can be built and tested with mock data (no Snowflake needed)\n### Status: COMPLETE\n### What was done:\n- Added `assign_drug_indications()` to `diagnosis_lookup.py`:\n  - Input: HCD df (with UPID, Drug Name, PseudoNHSNoLinked, Directory), gp_matches_df, search_term_to_fragments\n  - Output: (modified_df with UPID|search_term, indication_df mapping modified_UPID ÔåÆ Search_Term)\n  - Builds GP lookup: PseudoNHSNoLinked ÔåÆ {Search_Term: code_frequency}\n  - Caches drugÔåÆSearch_Term lookups to avoid recomputing per row\n  - For each (UPID, Drug Name) pair:\n    - Intersects drug's Search_Terms with patient's GP matches\n    - Single match: use it\n    - Multiple matches: highest code_frequency wins, alphabetical tiebreak\n    - No match: fallback to \"{Directory} (no GP dx)\"\n  - Applies modified UPIDs via df.apply() (vectorized lookup from cache)\n  - Builds indication_df with unique modified UPID ÔåÆ Directory column\n  - Comprehensive logging: match rate, tiebreaker count, fallback count, top 5 indications\n- Updated __all__ exports\n### Validation results:\n- Tier 1 (Code): py_compile PASSED, import check PASSED\n- Tier 2 (Data): Mock data tests ALL PASSED:\n  - ADALIMUMAB + GP dx (RA + asthma) ÔåÆ matched to RA (drug mapping intersection)\n  - OMALIZUMAB + GP dx (RA + asthma) ÔåÆ matched to asthma (drug mapping intersection)\n  - ADALIMUMAB + GP dx (RA 3 freq + crohn's 2 freq) ÔåÆ tiebreaker picks RA\n  - ADALIMUMAB + GP dx (psoriatic 5 freq + RA 5 freq) ÔåÆ alphabetical tiebreak picks psoriatic arthritis\n  - Higher frequency (47 RA vs 3 psoriatic) ÔåÆ RA wins\n  - No GP match ÔåÆ fallback to directory\n  - Empty GP DataFrame ÔåÆ all fallback\n  - Different drugs with different indications ÔåÆ different modified UPIDs\n- Tier 3 (Functional): N/A (no UI changes yet)\n### Files changed:\n- data_processing/diagnosis_lookup.py (added assign_drug_indications, updated __all__)\n- IMPLEMENTATION_PLAN.md (marked 2.1 and 2.2 subtasks [x])\n### Committed: 408976e \"feat: add assign_drug_indications() for drug-aware indication matching (Task 2.1 + 2.2)\"\n### Patterns discovered:\n- Function signature takes `search_term_to_fragments` (the second element from load_drug_indication_mapping()) ÔÇö NOT the full tuple. Callers must destructure: `_, st_to_frags = load_drug_indication_mapping()`\n- The function uses df.apply() to set modified UPIDs ÔÇö for large DataFrames (656K rows), this could be slow. If performance is an issue in Phase 3, could vectorize with merge operations instead. But apply with cached lookup dict should be OK.\n- \"crohn's disease\" is NOT in ADALIMUMAB's DimSearchTerm mapping (ADALIMUMAB maps to: ankylosing spondylitis, axial spondyloarthritis, plaque psoriasis, psoriatic arthritis, rheumatoid arthritis, ulcerative colitis, uveitis). Initial test assumption was wrong.\n- indication_df has 'Directory' column (not 'Search_Term') for compatibility with generate_icicle_chart_indication() which expects indication_df.loc[upid, 'Directory']\n### Next iteration should:\n- Work on Task 3.1: Update `refresh_pathways.py` indication processing to use assign_drug_indications()\n  - The current code at lines 424-428 uses `dict(zip(...))` which only keeps LAST match per patient ÔÇö this must be replaced\n  - Key changes in the `elif current_chart_type == \"indication\":` block:\n    1. Load drug mapping: `_, st_to_frags = load_drug_indication_mapping()`\n    2. Pass `earliest_hcd_date=df['Intervention Date'].min().strftime('%Y-%m-%d')` to get_patient_indication_groups()\n    3. Call `assign_drug_indications(df, gp_matches_df, st_to_frags)` to get (modified_df, indication_df)\n    4. Use modified_df (not original df) for pathway processing\n    5. indication_df is already in the right format (indexed by modified UPID, 'Directory' column)\n    6. Remove the old match_lookup/dict(zip) code and the manual indication_df building\n  - Import assign_drug_indications and load_drug_indication_mapping at top of file\n  - This replaces ~50 lines of the old approach with ~10 lines using the new function\n  - Can verify with py_compile; full Snowflake test via --dry-run\n### Blocked items:\n- None","originalFile":"# Progress Log - Drug-Aware Indication Matching\n\n## Project Context\n\nThis project extends the indication-based pathway charts (Phase 1-5 complete) with drug-aware matching.\n\n**Previous state**: Patients get ONE indication based on their most recent GP diagnosis match (SNOMED cluster codes). This ignores which drugs the patient is taking.\n\n**New goal**: Match each drug to an indication by cross-referencing the patient's GP diagnoses AND the drug's Search_Term mapping from DimSearchTerm.csv.\n\n## Key Data/Patterns\n\n### DimSearchTerm.csv\n- Located at `data/DimSearchTerm.csv`\n- Columns: Search_Term, CleanedDrugName (pipe-separated), PrimaryDirectorate\n- ~165 rows mapping clinical conditions to drug name fragments\n- Drug fragments are substrings that match standardized drug names from HCD data\n- Some entries have generic fragments: INHALED, CONTINUOUS, STANDARD-DOSE, PEGYLATED\n\n### Current get_patient_indication_groups() in diagnosis_lookup.py\n- Uses CLUSTER_MAPPING_SQL as CTE in Snowflake query\n- Returns ONLY the most recent match per patient (QUALIFY ROW_NUMBER() = 1)\n- Needs to return ALL matching Search_Terms per patient (remove QUALIFY)\n- Batches 500 patients per query\n\n### Modified UPID approach\n- Current: UPID = Provider Code[:3] + PersonKey (e.g., \"RMV12345\")\n- New: UPID = original + \"|\" + search_term (e.g., \"RMV12345|rheumatoid arthritis\")\n- The pipe delimiter \"|\" is safe because existing UPIDs are alphanumeric\n- generate_icicle_chart_indication() treats UPID as an opaque identifier ÔÇö modified UPIDs work transparently\n- The \" - \" delimiter in pathway ids is used for hierarchy levels, not within UPIDs\n\n### PseudoNHSNoLinked mapping\n- HCD data has PseudoNHSNoLinked column that matches PatientPseudonym in GP records\n- PersonKey is provider-specific local ID ÔÇö do NOT use for GP matching\n- One PseudoNHSNoLinked can map to multiple UPIDs (multi-provider patients)\n- GP match lookup: PseudoNHSNoLinked ÔåÆ list of matched Search_Terms\n\n### Drug matching logic\n- For each HCD row (UPID + Drug Name):\n  1. Get patient's GP-matched Search_Terms with code_frequency (via PseudoNHSNoLinked)\n  2. Get which Search_Terms list this drug (from DimSearchTerm.csv)\n  3. Intersection = valid indications\n  4. If 1: use it. If multiple: pick highest code_frequency (most GP coding = most likely indication). If 0: fallback to directory.\n- Modified UPID groups drugs under same indication together naturally\n- code_frequency = COUNT(*) of matching SNOMED codes per Search_Term per patient in GP records\n- GP code time range: only count codes from MIN(Intervention Date) onwards (the HCD data window)\n  - Reduces noise from old/irrelevant diagnoses, makes frequency more meaningful\n  - Pass earliest_hcd_date as parameter to get_patient_indication_groups()\n- Tiebreaker rationale: 47 RA codes vs 2 crohn's codes ÔåÆ RA is clearly the active condition\n\n### Known edge cases\n- Some DimSearchTerm drug fragments are generic (INHALED, ORAL, CONTINUOUS)\n  - These could match broadly but are constrained by GP diagnosis requirement\n- A patient visiting multiple providers has multiple UPIDs\n  - Each UPID gets its own drug-indication matching independently\n- Same Search_Term appears twice in DimSearchTerm.csv with different directorates\n  - e.g., \"diabetes\" ÔåÆ DIABETIC MEDICINE and OPHTHALMOLOGY\n  - For indication charts, we use Search_Term not directorate, so this is fine\n\n## Iteration Log\n\n## Iteration 1 ÔÇö 2026-02-05\n### Task: 1.3 ÔÇö Build drug-to-Search_Term lookup from DimSearchTerm.csv\n### Why this task:\n- First iteration, chose Phase 1 foundations. Task 1.2 (CSV loading) is self-contained and testable locally without Snowflake.\n- Task 1.1 (Snowflake query update) can't be verified without a live connection ÔÇö better to do 1.2 first.\n- Both 1.1 and 1.2 are independent, so order doesn't matter for dependencies.\n### Status: COMPLETE\n### What was done:\n- Added `load_drug_indication_mapping()` to `diagnosis_lookup.py`:\n  - Loads `data/DimSearchTerm.csv`, builds two dicts:\n    - `fragment_to_search_terms`: drug fragment (UPPER) ÔåÆ list of Search_Terms\n    - `search_term_to_fragments`: search_term ÔåÆ list of drug fragments (UPPER)\n  - Handles duplicate Search_Terms (e.g., \"diabetes\" rows combined)\n  - Result: 164 Search_Terms, 346 drug fragments\n- Added `get_search_terms_for_drug()` to `diagnosis_lookup.py`:\n  - Returns all Search_Terms whose drug fragments are substrings of the drug name (case-insensitive)\n  - Named differently from plan's `drug_matches_search_term()` ÔÇö returns all matches at once rather than single boolean, more practical for Phase 2\n- Updated `__all__` exports\n### Validation results:\n- Tier 1 (Code): py_compile passed, import check passed\n- Tier 2 (Data): ADALIMUMAB ÔåÆ 7 indications (including axial spondyloarthritis, rheumatoid arthritis), OMALIZUMAB ÔåÆ 4 indications (asthma, allergic asthma, etc.), PEGYLATED LIPOSOMAL DOXORUBICIN ÔåÆ 4 matches via substring, \"ADALIMUMAB 40MG\" matches correctly with dosage info, diabetes fragments combined from 2 CSV rows\n- Tier 3 (Functional): N/A (no UI changes)\n### Files changed:\n- data_processing/diagnosis_lookup.py (added load_drug_indication_mapping, get_search_terms_for_drug)\n- IMPLEMENTATION_PLAN.md (marked 1.2 subtasks [x])\n### Committed: 0779df7 \"feat: add drug-to-indication mapping from DimSearchTerm.csv (Task 1.3)\"\n### Patterns discovered:\n- DimSearchTerm.csv has 164 unique Search_Terms (not 165 as noted) because diabetes appears twice with different directorates but same Search_Term\n- Some drug fragments are very generic: INHALED, CONTINUOUS, ORAL, STANDARD-DOSE, INTRAVENOUS, PEGYLATED, ROUTINE, INDUCTION ÔÇö these will match broadly but are constrained by the GP diagnosis requirement in Phase 2\n- Function signatures for Phase 2: `get_search_terms_for_drug(drug_name, search_term_to_fragments)` returns list[str] ÔÇö use this to get candidate indications per drug\n### Next iteration should:\n- Work on Task 1.2: Merge asthma Search_Terms in CLUSTER_MAPPING_SQL and load_drug_indication_mapping()\n  - Merge \"allergic asthma\", \"asthma\", \"severe persistent allergic asthma\" ÔåÆ \"asthma\"\n  - Keep \"urticaria\" separate\n  - This is self-contained and testable locally\n- OR work on Task 1.1: Update `get_patient_indication_groups()` to return ALL matches with code_frequency\n  - The current query at line ~1352 of diagnosis_lookup.py uses `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1` ÔÇö this must be replaced with GROUP BY + COUNT(*)\n  - Add `earliest_hcd_date` parameter to restrict GP codes to HCD data window\n  - Return columns: PatientPseudonym, Search_Term, code_frequency (not EventDateTime)\n- OR if Snowflake isn't available to test 1.1, skip to Task 2.1 (assign_drug_indications function) which can be built and tested with mock data\n### Blocked items:\n- None\n\n## Iteration 2 ÔÇö 2026-02-05\n### Task: 1.2 ÔÇö Merge related asthma Search_Terms in CLUSTER_MAPPING_SQL\n### Why this task:\n- Previous iteration recommended this as the next task (self-contained, testable locally)\n- Both CLUSTER_MAPPING_SQL and load_drug_indication_mapping() need consistent Search_Term names\n- Must be done before Task 1.1 (Snowflake query) to ensure GP lookups return \"asthma\" not \"allergic asthma\"\n### Status: COMPLETE\n### What was done:\n- Updated CLUSTER_MAPPING_SQL: changed 'allergic asthma' ÔåÆ 'asthma' (AST_COD) and 'severe persistent allergic asthma' ÔåÆ 'asthma' (SEVAST_COD)\n  - Now 3 rows for 'asthma': AST_COD, eFI2_Asthma, SEVAST_COD\n  - urticaria (XSAL_COD) stays separate\n- Added SEARCH_TERM_MERGE_MAP constant: {\"allergic asthma\": \"asthma\", \"severe persistent allergic asthma\": \"asthma\"}\n- Updated load_drug_indication_mapping() to apply SEARCH_TERM_MERGE_MAP when loading CSV\n  - Normalizes Search_Term before accumulating fragments\n  - Drug fragments from all 3 original rows combined under \"asthma\" key\n- Exported SEARCH_TERM_MERGE_MAP in __all__\n### Validation results:\n- Tier 1 (Code): py_compile passed, import check passed\n- Tier 2 (Data):\n  - \"asthma\" fragments: OMALIZUMAB, BENRALIZUMAB, DUPILUMAB, INHALED, MEPOLIZUMAB, RESLIZUMAB (complete combined list)\n  - \"allergic asthma\" no longer exists as separate key\n  - \"severe persistent allergic asthma\" no longer exists as separate key\n  - \"urticaria\" ÔåÆ ['OMALIZUMAB'] ÔÇö correctly separate\n  - OMALIZUMAB maps to: ['asthma', 'urticaria'] ÔÇö correct\n  - Total Search_Terms: 162 (was 164, 3 asthma entries ÔåÆ 1)\n  - Total fragments: 346 (unchanged)\n- Tier 3 (Functional): N/A (no UI changes)\n### Files changed:\n- data_processing/diagnosis_lookup.py (CLUSTER_MAPPING_SQL, SEARCH_TERM_MERGE_MAP, load_drug_indication_mapping)\n- IMPLEMENTATION_PLAN.md (marked 1.2 subtasks [x])\n### Committed: b0a8a9d \"feat: merge asthma Search_Term variants in CLUSTER_MAPPING_SQL and drug mapping (Task 1.2)\"\n### Patterns discovered:\n- SEARCH_TERM_MERGE_MAP is reusable: any future module that receives Search_Terms from Snowflake can apply the same normalization\n- The merge approach (normalize at load time) is cleaner than post-hoc deduplication\n### Next iteration should:\n- Work on Task 1.1: Update `get_patient_indication_groups()` to return ALL matches with code_frequency\n  - The current query at ~line 1467 uses `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1`\n  - Replace with GROUP BY + COUNT(*) for code_frequency\n  - Add `earliest_hcd_date` parameter to restrict GP codes to HCD data window\n  - Return columns: PatientPseudonym, Search_Term, code_frequency\n  - Empty DataFrame columns should match new return type\n  - This requires Snowflake connectivity to fully test, but code changes can be verified with py_compile and import checks\n- OR work on Task 2.1: Create assign_drug_indications() ÔÇö can be built and tested with mock data\n  - This is independent of Task 1.1 if you mock the gp_matches_df input\n### Blocked items:\n- None\n\n## Iteration 3 ÔÇö 2026-02-05\n### Task: 1.1 ÔÇö Update get_patient_indication_groups() to return ALL matches with code_frequency\n### Why this task:\n- Previous iteration recommended this as the next task\n- This is the last remaining Phase 1 dependency ÔÇö Tasks 2.1/2.2 need the GP matches data in the right format (multiple rows per patient with code_frequency)\n- Task 1.1 changes the query fundamentals; better to do this before building assign_drug_indications() which consumes the output\n### Status: COMPLETE\n### What was done:\n- Modified `get_patient_indication_groups()` in `diagnosis_lookup.py`:\n  1. **Removed** `QUALIFY ROW_NUMBER() OVER (PARTITION BY pc.\"PatientPseudonym\" ORDER BY pc.\"EventDateTime\" DESC) = 1`\n  2. **Replaced** with `GROUP BY pc.\"PatientPseudonym\", aic.Search_Term` + `COUNT(*) AS \"code_frequency\"`\n  3. **Added** `earliest_hcd_date: Optional[str] = None` parameter\n  4. **Added** optional `AND pc.\"EventDateTime\" >= %s` when earliest_hcd_date is provided\n  5. **Updated** return columns from `(PatientPseudonym, Search_Term, EventDateTime)` to `(PatientPseudonym, Search_Term, code_frequency)`\n  6. **Updated** all empty DataFrame returns to use new column names\n  7. **Updated** logging to show multiple-rows-per-patient stats (avg indications per patient)\n  8. **Updated** docstring to describe new behavior and parameters\n- Backward compatible: `earliest_hcd_date` defaults to `None`, existing callers still work\n- Note: caller in `refresh_pathways.py` (line 424-428) does `dict(zip(...))` which will only keep last match per patient with new multi-row format ÔÇö this will be updated in Task 3.1\n### Validation results:\n- Tier 1 (Code): py_compile PASSED, import check PASSED, function signature verified\n- Tier 2 (Data): Empty DataFrame returns correct columns ['PatientPseudonym', 'Search_Term', 'code_frequency']; live Snowflake test deferred to Phase 3/4\n- Tier 3 (Functional): N/A (no UI changes)\n### Files changed:\n- data_processing/diagnosis_lookup.py (modified get_patient_indication_groups function)\n- IMPLEMENTATION_PLAN.md (marked 1.1 subtasks [x])\n### Committed: c93417f \"feat: return ALL GP matches with code_frequency in get_patient_indication_groups (Task 1.1)\"\n### Patterns discovered:\n- The `earliest_hcd_date` parameter is passed as a string in ISO format (YYYY-MM-DD) via Snowflake %s placeholder ÔÇö Snowflake handles string-to-timestamp comparison implicitly\n- The GROUP BY approach naturally deduplicates SNOMED codes within the same Search_Term ÔÇö a patient with the same SNOMED code recorded 5 times gets code_frequency=5 (reflecting clinical activity intensity)\n- params list is built dynamically: `batch_pseudonyms + [earliest_hcd_date]` only when date filter is active\n### Next iteration should:\n- Work on Task 2.1: Create `assign_drug_indications()` function\n  - This is now unblocked since 1.1 is complete (return format is known)\n  - Input: HCD df, gp_matches_df (PatientPseudonym, Search_Term, code_frequency), drug_mapping from load_drug_indication_mapping()\n  - Output: (modified_df with UPID|search_term, indication_df mapping modified_UPID ÔåÆ Search_Term)\n  - Can be built and tested with mock data (no Snowflake needed)\n  - Key logic: for each UPID+Drug pair, intersect drug's Search_Terms with patient's GP matches, pick highest code_frequency as tiebreaker\n  - The function needs PseudoNHSNoLinked to look up GP matches, so the df must have that column\n  - Task 2.2 (tiebreaker logic) can be done within 2.1 or as a follow-up\n- The final Phase 1 subtask (1.1 verify with live Snowflake) will be tested during Phase 3/4 integration\n### Blocked items:\n- Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4\n","structuredPatch":[{"oldStart":193,"oldLines":3,"newStart":193,"newLines":63,"lines":[" - The final Phase 1 subtask (1.1 verify with live Snowflake) will be tested during Phase 3/4 integration"," ### Blocked items:"," - Task 1.1 final subtask \"Verify: Query returns more rows\" requires live Snowflake ÔÇö deferred to Phase 3/4","+","+## Iteration 4 ÔÇö 2026-02-05","+### Task: 2.1 + 2.2 ÔÇö Create assign_drug_indications() with tiebreaker logic","+### Why this task:","+- All Phase 1 dependencies complete (1.1 query returns ALL matches, 1.2 asthma merged, 1.3 drug mapping loaded)","+- Task 2.1 is the core matching function needed before Phase 3 pipeline integration","+- Task 2.2 (tiebreaker) is naturally part of 2.1 ÔÇö implemented together","+- Can be built and tested with mock data (no Snowflake needed)","+### Status: COMPLETE","+### What was done:","+- Added `assign_drug_indications()` to `diagnosis_lookup.py`:","+  - Input: HCD df (with UPID, Drug Name, PseudoNHSNoLinked, Directory), gp_matches_df, search_term_to_fragments","+  - Output: (modified_df with UPID|search_term, indication_df mapping modified_UPID ÔåÆ Search_Term)","+  - Builds GP lookup: PseudoNHSNoLinked ÔåÆ {Search_Term: code_frequency}","+  - Caches drugÔåÆSearch_Term lookups to avoid recomputing per row","+  - For each (UPID, Drug Name) pair:","+    - Intersects drug's Search_Terms with patient's GP matches","+    - Single match: use it","+    - Multiple matches: highest code_frequency wins, alphabetical tiebreak","+    - No match: fallback to \"{Directory} (no GP dx)\"","+  - Applies modified UPIDs via df.apply() (vectorized lookup from cache)","+  - Builds indication_df with unique modified UPID ÔåÆ Directory column","+  - Comprehensive logging: match rate, tiebreaker count, fallback count, top 5 indications","+- Updated __all__ exports","+### Validation results:","+- Tier 1 (Code): py_compile PASSED, import check PASSED","+- Tier 2 (Data): Mock data tests ALL PASSED:","+  - ADALIMUMAB + GP dx (RA + asthma) ÔåÆ matched to RA (drug mapping intersection)","+  - OMALIZUMAB + GP dx (RA + asthma) ÔåÆ matched to asthma (drug mapping intersection)","+  - ADALIMUMAB + GP dx (RA 3 freq + crohn's 2 freq) ÔåÆ tiebreaker picks RA","+  - ADALIMUMAB + GP dx (psoriatic 5 freq + RA 5 freq) ÔåÆ alphabetical tiebreak picks psoriatic arthritis","+  - Higher frequency (47 RA vs 3 psoriatic) ÔåÆ RA wins","+  - No GP match ÔåÆ fallback to directory","+  - Empty GP DataFrame ÔåÆ all fallback","+  - Different drugs with different indications ÔåÆ different modified UPIDs","+- Tier 3 (Functional): N/A (no UI changes yet)","+### Files changed:","+- data_processing/diagnosis_lookup.py (added assign_drug_indications, updated __all__)","+- IMPLEMENTATION_PLAN.md (marked 2.1 and 2.2 subtasks [x])","+### Committed: 408976e \"feat: add assign_drug_indications() for drug-aware indication matching (Task 2.1 + 2.2)\"","+### Patterns discovered:","+- Function signature takes `search_term_to_fragments` (the second element from load_drug_indication_mapping()) ÔÇö NOT the full tuple. Callers must destructure: `_, st_to_frags = load_drug_indication_mapping()`","+- The function uses df.apply() to set modified UPIDs ÔÇö for large DataFrames (656K rows), this could be slow. If performance is an issue in Phase 3, could vectorize with merge operations instead. But apply with cached lookup dict should be OK.","+- \"crohn's disease\" is NOT in ADALIMUMAB's DimSearchTerm mapping (ADALIMUMAB maps to: ankylosing spondylitis, axial spondyloarthritis, plaque psoriasis, psoriatic arthritis, rheumatoid arthritis, ulcerative colitis, uveitis). Initial test assumption was wrong.","+- indication_df has 'Directory' column (not 'Search_Term') for compatibility with generate_icicle_chart_indication() which expects indication_df.loc[upid, 'Directory']","+### Next iteration should:","+- Work on Task 3.1: Update `refresh_pathways.py` indication processing to use assign_drug_indications()","+  - The current code at lines 424-428 uses `dict(zip(...))` which only keeps LAST match per patient ÔÇö this must be replaced","+  - Key changes in the `elif current_chart_type == \"indication\":` block:","+    1. Load drug mapping: `_, st_to_frags = load_drug_indication_mapping()`","+    2. Pass `earliest_hcd_date=df['Intervention Date'].min().strftime('%Y-%m-%d')` to get_patient_indication_groups()","+    3. Call `assign_drug_indications(df, gp_matches_df, st_to_frags)` to get (modified_df, indication_df)","+    4. Use modified_df (not original df) for pathway processing","+    5. indication_df is already in the right format (indexed by modified UPID, 'Directory' column)","+    6. Remove the old match_lookup/dict(zip) code and the manual indication_df building","+  - Import assign_drug_indications and load_drug_indication_mapping at top of file","+  - This replaces ~50 lines of the old approach with ~10 lines using the new function","+  - Can verify with py_compile; full Snowflake test via --dry-run","+### Blocked items:","+- None"]}],"userModified":false,"replaceAll":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01B3DXVnM4E8MTH91RoUuwp7","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01X6evKFj564nLekKhj9S8vL","name":"Bash","input":{"command":"cd \"C:\\Users\\charlwoodand\\Ralph local\\Tasks\\UI Redesign\" && git add progress.txt && git commit -m \"$(cat <<'EOF'\ndocs: update progress.txt with Iteration 4 results (Task 2.1 + 2.2)\nEOF\n)\"","description":"Commit progress update"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1649,"cache_read_input_tokens":131241,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1649},"output_tokens":19,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"017c05a5-867d-472b-a62f-071f2cdea00f"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01X6evKFj564nLekKhj9S8vL","type":"tool_result","content":"[feature/snomed-indication-mapping d9891c8] docs: update progress.txt with Iteration 4 results (Task 2.1 + 2.2)\n 1 file changed, 60 insertions(+)","is_error":false}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"6afdfe6f-45c7-44b8-9a4c-2c8cb7be603d","tool_use_result":{"stdout":"[feature/snomed-indication-mapping d9891c8] docs: update progress.txt with Iteration 4 results (Task 2.1 + 2.2)\n 1 file changed, 60 insertions(+)\r","stderr":"","interrupted":false,"isImage":false}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_016m8YVnwDcBoowsimt6AdT6","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01HbYqma7HJNWDpKq84KpbjY","name":"TodoWrite","input":{"todos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"completed","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"completed","activeForm":"Committing and updating progress"}]}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":207,"cache_read_input_tokens":132890,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":207},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"031ab433-30a2-4ebe-a4f8-9533bd62a087"}
{"type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01HbYqma7HJNWDpKq84KpbjY","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"bd370e46-6ce7-4eef-8e58-b8736910a0a1","tool_use_result":{"oldTodos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"completed","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"in_progress","activeForm":"Committing and updating progress"}],"newTodos":[{"content":"Read project files (progress.txt, IMPLEMENTATION_PLAN.md, guardrails.md)","status":"completed","activeForm":"Reading project files"},{"content":"Build assign_drug_indications() function (Task 2.1 + 2.2)","status":"completed","activeForm":"Building assign_drug_indications() function"},{"content":"Validate code (py_compile, import check, mock data test)","status":"completed","activeForm":"Validating code"},{"content":"Commit and update progress.txt","status":"completed","activeForm":"Committing and updating progress"}]}}
{"type":"assistant","message":{"model":"claude-opus-4-6","id":"msg_01BMc3oBVZdps8nZoF4kE5Hh","type":"message","role":"assistant","content":[{"type":"text","text":"**Iteration 4 complete.** Here's the summary:\n\n**Task 2.1 + 2.2: `assign_drug_indications()` ÔÇö COMPLETE**\n\nAdded the core drug-aware indication matching function to `diagnosis_lookup.py`. For each UPID + Drug Name pair:\n1. Gets patient's GP-matched Search_Terms (via PseudoNHSNoLinked)\n2. Gets which Search_Terms list this drug (from DimSearchTerm.csv mapping)\n3. Intersects to find valid indications\n4. Tiebreaker: highest code_frequency wins, alphabetical if tied\n5. Fallback: `{Directory} (no GP dx)` for unmatched drugs\n6. Modifies UPID: `{UPID}|{search_term}`\n\nAll mock data tests pass. Commits: `408976e` (function) and `d9891c8` (progress).\n\n**Next iteration**: Task 3.1 ÔÇö integrate `assign_drug_indications()` into `refresh_pathways.py`, replacing the old `dict(zip())` approach that only kept one match per patient."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":251,"cache_read_input_tokens":133097,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":251},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"},"context_management":null},"parent_tool_use_id":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","uuid":"afa31710-de59-4917-a73d-530a4e62236d"}
{"type":"result","subtype":"success","is_error":false,"duration_ms":300431,"duration_api_ms":280922,"num_turns":29,"result":"**Iteration 4 complete.** Here's the summary:\n\n**Task 2.1 + 2.2: `assign_drug_indications()` ÔÇö COMPLETE**\n\nAdded the core drug-aware indication matching function to `diagnosis_lookup.py`. For each UPID + Drug Name pair:\n1. Gets patient's GP-matched Search_Terms (via PseudoNHSNoLinked)\n2. Gets which Search_Terms list this drug (from DimSearchTerm.csv mapping)\n3. Intersects to find valid indications\n4. Tiebreaker: highest code_frequency wins, alphabetical if tied\n5. Fallback: `{Directory} (no GP dx)` for unmatched drugs\n6. Modifies UPID: `{UPID}|{search_term}`\n\nAll mock data tests pass. Commits: `408976e` (function) and `d9891c8` (progress).\n\n**Next iteration**: Task 3.1 ÔÇö integrate `assign_drug_indications()` into `refresh_pathways.py`, replacing the old `dict(zip())` approach that only kept one match per patient.","stop_reason":null,"session_id":"d2ce1658-c25d-416e-bc13-2d1b50478cab","total_cost_usd":2.5186082499999993,"usage":{"input_tokens":27,"cache_creation_input_tokens":105963,"cache_read_input_tokens":3048617,"output_tokens":12707,"server_tool_use":{"web_search_requests":0,"web_fetch_requests":0},"service_tier":"standard","cache_creation":{"ephemeral_1h_input_tokens":105963,"ephemeral_5m_input_tokens":0}},"modelUsage":{"claude-opus-4-6":{"inputTokens":27,"outputTokens":12707,"cacheReadInputTokens":3048617,"cacheCreationInputTokens":105963,"webSearchRequests":0,"costUSD":2.5043872499999997,"contextWindow":200000,"maxOutputTokens":32000},"claude-haiku-4-5-20251001":{"inputTokens":12486,"outputTokens":347,"cacheReadInputTokens":0,"cacheCreationInputTokens":0,"webSearchRequests":0,"costUSD":0.014221,"contextWindow":200000,"maxOutputTokens":64000}},"permission_denials":[],"uuid":"d425aa52-201b-44db-aef9-f7c335873604"}
